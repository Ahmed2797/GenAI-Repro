{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2a3f6800",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ahmed\n"
     ]
    }
   ],
   "source": [
    "print('Ahmed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2b6b3858",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ahmed/miniconda3/envs/chatapp/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from git import Repo\n",
    "from langchain_classic.document_loaders.generic import GenericLoader\n",
    "from langchain_classic.document_loaders.parsers import LanguageParser\n",
    "from langchain_classic.text_splitter import Language\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87920ffe",
   "metadata": {},
   "source": [
    "## load docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Repository already exists at Input_Repo/Network-Security\n",
      "Loaded 35 documents\n"
     ]
    }
   ],
   "source": [
    "def load_repo(url: str = None, repo_dir: str = \"Input_Repo\"):\n",
    "    \"\"\"\n",
    "    Clone a git repository to a local directory.\n",
    "    \n",
    "    Args:\n",
    "        url (str): Git repository URL\n",
    "        repo_dir (str): Local directory to clone into\n",
    "        \n",
    "    Returns:\n",
    "        str: Path to the cloned repository\n",
    "    \"\"\"\n",
    "    if not url:\n",
    "        raise ValueError(\"Repository URL is required\")\n",
    "    \n",
    "    # Create directory if it doesn't exist\n",
    "    os.makedirs(repo_dir, exist_ok=True)\n",
    "    \n",
    "    # Clone the repository\n",
    "    repo_name = url.split('/')[-1].replace('.git', '')\n",
    "    repo_path = os.path.join(repo_dir, repo_name)\n",
    "    \n",
    "    # Check if repo already exists\n",
    "    if not os.path.exists(repo_path):\n",
    "        print(f\"Cloning repository from {url} to {repo_path}\")\n",
    "        Repo.clone_from(url=url, to_path=repo_path)\n",
    "    else:\n",
    "        print(f\"Repository already exists at {repo_path}\")\n",
    "    \n",
    "    return repo_path  # Return the actual path for use in loader\n",
    "\n",
    "def document_loader_repo(path: str):\n",
    "    \"\"\"\n",
    "    Load Python documents from a repository path.\n",
    "    \n",
    "    Args:\n",
    "        path (str): Path to the repository\n",
    "        \n",
    "    Returns:\n",
    "        list: Loaded document objects\n",
    "    \"\"\"\n",
    "    if not os.path.exists(path):\n",
    "        raise FileNotFoundError(f\"Path does not exist: {path}\")\n",
    "    \n",
    "    # Create loader for Python files\n",
    "    loader = GenericLoader.from_filesystem(\n",
    "        path=path,\n",
    "        glob=\"**/*\",\n",
    "        suffixes=['.py'],\n",
    "        parser=LanguageParser(\n",
    "            language=Language.PYTHON,\n",
    "            parser_threshold=500  # Increased from 100 for better chunking\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    # Load documents\n",
    "    documents = loader.load()\n",
    "    \n",
    "    return documents\n",
    "\n",
    "# Example usage:\n",
    "if __name__ == \"__main__\":\n",
    "    # Clone a repository\n",
    "    repo_url = \"https://github.com/Ahmed2797/Network-Security.git\"\n",
    "    repo_path = load_repo(url=repo_url)\n",
    "    \n",
    "    # Load documents from the cloned repo\n",
    "    docs = document_loader_repo(repo_path)\n",
    "    print(f\"Loaded {len(docs)} documents\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'Input_Repo/Network-Security/test_mongo.py', 'language': <Language.PYTHON: 'python'>}, page_content='from pymongo.mongo_client import MongoClient\\nfrom pymongo.server_api import ServerApi\\n\\nuri = \"mongodb+srv://tanvirahmed754575_db_user:<@password>@cluster0.oofrxbi.mongodb.net/?retryWrites=true&w=majority&appName=Cluster0\"\\n\\n# Create a new client and connect to the server\\nclient = MongoClient(uri, server_api=ServerApi(\\'1\\'))\\n\\n# Send a ping to confirm a successful connection\\ntry:\\n    client.admin.command(\\'ping\\')\\n    print(\"Pinged your deployment. You successfully connected to MongoDB!\")\\nexcept Exception as e:\\n    print(e)\\n\\n\\n'),\n",
       " Document(metadata={'source': 'Input_Repo/Network-Security/pushdata.py', 'language': <Language.PYTHON: 'python'>}, page_content='import os \\nimport sys \\nimport json \\nimport certifi\\nimport pandas as pd\\nimport pymongo\\nfrom Network_Security.exception.exception import NetworkSecurityException \\nfrom Network_Security.logging.logger import logging\\nfrom dotenv import load_dotenv \\nload_dotenv()\\n\\nMONGODB_URL = os.getenv(\\'MONGODB_URL\\')\\nprint(\"MongoDB URL:\", MONGODB_URL)  \\n\\nca = certifi.where()\\n\\nclass NetworkDataExtract:\\n    def __init__(self):\\n        try:\\n            pass\\n        except Exception as e:\\n            raise NetworkSecurityException(e, sys)\\n\\n    def csv_to_json_convertor(self, file_path):\\n        try:\\n            if not os.path.exists(file_path):\\n                raise FileNotFoundError(f\"File not found: {file_path}\")\\n                \\n            data = pd.read_csv(file_path)\\n            data.reset_index(drop=True, inplace=True)\\n            records = json.loads(data.to_json(orient=\"records\"))\\n            logging.info(f\"Successfully converted {len(records)} records from CSV to JSON\")\\n            return records\\n        except Exception as e:\\n            logging.error(f\"Error converting CSV to JSON: {str(e)}\")\\n            raise NetworkSecurityException(e, sys)\\n        \\n    def insert_data_mongodb(self, records, database, collection):\\n        try:\\n            if not records:\\n                logging.warning(\"No records to insert\")\\n                return 0\\n                \\n            if not database or not collection:\\n                raise ValueError(\"Database and collection names cannot be empty\")\\n\\n            self.mongo_client = pymongo.MongoClient(MONGODB_URL, tlsCAFile=ca)\\n            self.db = self.mongo_client[database]\\n            self.collection = self.db[collection]\\n          \\n            result = self.collection.insert_many(records)\\n            inserted_count = len(result.inserted_ids)\\n            \\n            logging.info(f\"Successfully inserted {inserted_count} records into {database}.{collection}\")\\n\\n            self.mongo_client.close()\\n            \\n            return inserted_count\\n            \\n        except Exception as e:\\n            logging.error(f\"Error inserting data into MongoDB: {str(e)}\")\\n            raise NetworkSecurityException(e, sys)\\n\\nif __name__ == \"__main__\":\\n    try:\\n        FILE_PATH = \"Network_Data_\\\\phishing_data.csv\"\\n        DATABASE = \"NETWORK_SECURITY\"\\n        COLLECTION = \"NETWORK_DATA\"\\n        \\n        networkobj = NetworkDataExtract()\\n        records = networkobj.csv_to_json_convertor(file_path=FILE_PATH)\\n        print(records)\\n        print(f\"Converted {len(records)} records\")\\n        \\n        no_of_records = networkobj.insert_data_mongodb(records, DATABASE, COLLECTION)\\n        print(f\"Inserted {no_of_records} records into MongoDB\")\\n        \\n    except Exception as e:\\n        print(f\"Error in main execution: {str(e)}\")\\n        sys.exit(1)'),\n",
       " Document(metadata={'source': 'Input_Repo/Network-Security/app.py', 'language': <Language.PYTHON: 'python'>}, page_content='from Network_Security.pipeline.train_pipeline import Training_Pipeline\\nfrom Network_Security.logging.logger import logging\\nfrom Network_Security.exception.exception import NetworkSecurityException\\nfrom Network_Security.utils import load_object\\nfrom Network_Security.entity.estimator import Network_model\\nfrom Network_Security.constant import TARGET_COLUMN\\nfrom Network_Security.pipeline.prediction_pipe import NetworkSecurity_Features\\n\\nimport os\\nimport sys\\nimport uvicorn\\nimport pandas as pd\\nfrom io import StringIO\\nimport io\\nfrom fastapi import FastAPI, Header, HTTPException, Request, Depends, UploadFile, File, Form\\nfrom fastapi.responses import JSONResponse, RedirectResponse, FileResponse\\nfrom fastapi.background import BackgroundTasks\\nfrom fastapi.templating import Jinja2Templates\\nfrom fastapi.staticfiles import StaticFiles\\nfrom fastapi.middleware.cors import CORSMiddleware\\n\\napp = FastAPI(title=\\'Phishing URL Detection App\\', version=\\'0.1.1\\')\\n\\n# Static & templates\\napp.mount(\"/static\", StaticFiles(directory= r\"Network_Security\\\\static\"), name=\"static\")\\ntemplates = Jinja2Templates(directory=r\\'Network_Security\\\\templates\\')\\n\\n# CORS\\norigins = [\"*\"]\\napp.add_middleware(\\n    CORSMiddleware,\\n    allow_origins=origins,\\n    allow_credentials=True,\\n    allow_methods=[\"*\"],\\n    allow_headers=[\"*\"],\\n)\\n\\n# ---------------------- HOME ROUTE ----------------------\\n@app.get(\\'/\\', tags=[\\'Start\\'])\\nasync def index():\\n    # Redirect to Swagger docs\\n    return RedirectResponse(url=\\'/docs\\')\\n\\n# ---------------------- API KEY VERIFICATION ----------------------\\nAUTH_KEY = \"Ahmed\"\\n\\ndef verify_api_key(x_api_key: str = Header(...)):\\n    if x_api_key != AUTH_KEY:\\n        raise HTTPException(status_code=401, detail=\"Invalid API Key\")\\n    return x_api_key\\n\\n# ---------------------- TRAIN ROUTE ----------------------\\n@app.get(\\'/train\\', tags=[\\'Train The Model\\'])\\nasync def train_route(\\n    request: Request,\\n    backgroundtask: BackgroundTasks,\\n    api_key: str = Depends(verify_api_key)\\n):\\n    try:\\n        logging.info(\\'Training Process Starting\\')\\n        backgroundtask.add_task(Training_Pipeline().run_pipeline)\\n        logging.info(\\'Background Training Started\\')\\n        # set status_code=202 to indicate accepted and background processing\\n        return templates.TemplateResponse(\\n            \"train.html\",\\n            {\"request\": request, \"message\": \"Training started in background!\"},\\n            status_code=202\\n        )\\n    except Exception as e:\\n        logging.error(f\"Training error: {e}\")\\n        # you can still return 500 or 400 for failure\\n        return templates.TemplateResponse(\\n            \"train.html\",\\n            {\"request\": request, \"message\": f\"Training failed: {str(e)}\"},\\n            status_code=500\\n        )\\n    \\n\\n# CSV upload route\\n@app.post(\\'/test\\', tags=[\\'Predict The Model\\'])\\nasync def predict_csv_route(request: Request, file: UploadFile = File(...)):\\n    try:\\n        # Validate file type\\n        if not file.filename.lower().endswith(\\'.csv\\'):\\n            raise HTTPException(status_code=400, detail=\\'Only CSV files are allowed\\')\\n\\n        # Read CSV content safely\\n        content = await file.read()\\n        df = pd.read_csv(io.StringIO(content.decode(\\'utf-8\\')))\\n\\n        if df.empty:\\n            raise HTTPException(status_code=400, detail=\\'Uploaded CSV is empty\\')\\n\\n        # Load preprocessor and model\\n        preprocessor = load_object(r\\'final_model\\\\preprocessor.pkl\\')\\n        model = load_object(r\\'final_model\\\\model.pkl\\')\\n\\n        # Make predictions\\n        network_model = Network_model(transform_object=preprocessor, best_model_details=model)\\n        pred_df = network_model.predict(df)\\n        df[\\'prediction\\'] = pd.Series(pred_df).replace(-1, 0)\\n\\n        # Save prediction file\\n        os.makedirs(\\'prediction\\', exist_ok=True)\\n        df.to_csv(\\'prediction/output.csv\\', index=False)\\n\\n        # Convert dataframe to HTML table\\n        table_html = df.to_html(classes=\\'table table-striped table-bordered\\', index=False)\\n\\n        # Render template\\n        return templates.TemplateResponse(\\n            \\'prediction.html\\',\\n            {\\n                \\'request\\': request,\\n                \\'table\\': table_html,\\n                \\'filename\\': file.filename\\n            }\\n        )\\n\\n    except Exception as e:\\n        logging.error(f\"Prediction failed: {e}\")\\n        raise HTTPException(status_code=500, detail=f\"Prediction process failed: {e}\")\\n    \\n# Form prediction route\\n@app.post(\"/predict\", tags=[\"Predict Form\"])\\ndef predict_form(\\n    request: Request,\\n    having_ip_address: int = Form(...),\\n    url_length: int = Form(...),\\n    shortining_service: int = Form(...),\\n    having_at_symbol: int = Form(...),\\n    double_slash_redirecting: int = Form(...),\\n    prefix_suffix: int = Form(...),\\n    having_sub_domain: int = Form(...),\\n    sslfinal_state: int = Form(...),\\n    domain_registration_length: int = Form(...),\\n    favicon: int = Form(...),\\n    port: int = Form(...),\\n    https_token: int = Form(...),\\n    request_url: int = Form(...),\\n    url_of_anchor: int = Form(...),\\n    links_in_tags: int = Form(...),\\n    sfh: int = Form(...),\\n    submitting_to_email: int = Form(...),\\n    abnormal_url: int = Form(...),\\n    redirect: int = Form(...),\\n    on_mouseover: int = Form(...),\\n    rightclick: int = Form(...),\\n    popupwindow: int = Form(...),\\n    iframe: int = Form(...),\\n    age_of_domain: int = Form(...),\\n    dnsrecord: int = Form(...),\\n    web_traffic: int = Form(...),\\n    page_rank: int = Form(...),\\n    google_index: int = Form(...),\\n    links_pointing_to_page: int = Form(...),\\n    statistical_report: int = Form(...)\\n):\\n    try:\\n        # Create feature object\\n        features = NetworkSecurity_Features(\\n            having_ip_address,\\n            url_length,\\n            shortining_service,\\n            having_at_symbol,\\n            double_slash_redirecting,\\n            prefix_suffix,\\n            having_sub_domain,\\n            sslfinal_state,\\n            domain_registration_length,\\n            favicon,\\n            port,\\n            https_token,\\n            request_url,\\n            url_of_anchor,\\n            links_in_tags,\\n            sfh,\\n            submitting_to_email,\\n            abnormal_url,\\n            redirect,\\n            on_mouseover,\\n            rightclick,\\n            popupwindow,\\n            iframe,\\n            age_of_domain,\\n            dnsrecord,\\n            web_traffic,\\n            page_rank,\\n            google_index,\\n            links_pointing_to_page,\\n            statistical_report\\n        )\\n\\n        df = pd.DataFrame([features.dict_data()])\\n\\n        preprocessor = load_object(r\"final_model\\\\preprocessor.pkl\")\\n        model = load_object(r\"final_model\\\\model.pkl\")\\n\\n        network_model = Network_model(transform_object=preprocessor, best_model_details=model)\\n        prediction = network_model.predict(df)\\n\\n        # Add styled output\\n        if prediction == 1:\\n            result_html = \\'<h3 style=\"color:red;\">⚠️ Phishing Website Detected!</h3>\\'\\n        else:\\n            result_html = \\'<h3 style=\"color:green;\">✅ Legitimate Website</h3>\\'\\n\\n        return templates.TemplateResponse(\\'predict.html\\', {\\'request\\': request, \\'table\\': result_html})\\n\\n    except Exception as e:\\n        logging.error(f\"Prediction failed: {e}\")\\n        raise HTTPException(status_code=500, detail=f\"Prediction process failed: {e}\")\\n\\n\\n# Download CSV route\\n@app.get(\"/download-predictions\", tags=[\"Download Prediction\"])\\nasync def download_predictions():\\n    file_path = \"prediction/output.csv\"\\n    if not os.path.exists(file_path):\\n        raise HTTPException(status_code=404, detail=\"Prediction file not found\")\\n    return FileResponse(file_path, media_type=\"text/csv\", filename=\"predictions.csv\")\\n\\n\\n\\nif __name__ == \"__main__\":\\n    uvicorn.run(\"app:app\", host=\"127.0.0.1\", port=8000, reload=True)\\n\\n\\n\\n\\n\\n\\n\\n# uvicorn main:app --reload\\n\\n# pipeline = Training_Pipeline()\\n# pipeline.run_pipeline()\\n\\n#\\'C:\\\\\\\\Users\\\\\\\\tanvi\\\\\\\\Network_Security_\\'\\n# http://127.0.0.1:8000\\n\\n'),\n",
       " Document(metadata={'source': 'Input_Repo/Network-Security/setup.py', 'language': <Language.PYTHON: 'python'>}, page_content='from setuptools import find_packages,setup \\nfrom typing import List \\n\\n\\ndef get_requirements(file_path:str)->List[str]:\\n\\n    requirement_list:List[str] = []\\n    try:\\n        with open(\\'requirements.txt\\',\\'r\\') as file:\\n            lines = file.readlines()\\n            for line in lines:\\n                requirement = line.strip()\\n                if requirement and requirement!= \"-e .\":\\n                    requirement_list.append(requirement)\\n    except FileNotFoundError:\\n        print(\"File is not found\")\\n\\n    return requirement_list \\n\\nsetup(\\n    name= \\'Network_Security\\',\\n    version= \\'2.0\\',\\n    author= \\'Ahmed\\',\\n    author_email= \\'tanvirahmed754575@gmail.com\\',\\n    packages= find_packages(),\\n    install_requires= get_requirements(\\'requirements.txt\\')\\n)'),\n",
       " Document(metadata={'source': 'Input_Repo/Network-Security/main.py', 'language': <Language.PYTHON: 'python'>}, page_content=\"from Network_Security.pipeline.train_pipeline import Training_Pipeline\\nfrom Network_Security.logging.logger import logging\\nfrom Network_Security.exception.exception import NetworkSecurityException\\nimport sys \\n\\n\\n\\nif __name__ == '__main__':\\n    try:\\n        logging.info('Starting Training Pipeline...')\\n        pipeline = Training_Pipeline()\\n\\n        # Data Ingestion\\n        logging.info('>>> Starting Data Ingestion')\\n        data_ingestion_artifact = pipeline.start_data_ingestion()\\n        logging.info(f'>>> Data Ingestion Completed: {data_ingestion_artifact}')\\n\\n        # Data Validation\\n        logging.info('---------->>> Starting Data Validation-------------->>>')\\n        data_validation_artifact = pipeline.start_data_validation(data_ingestion_artifact)\\n        logging.info(f'>>> Data Validation Completed: {data_validation_artifact}')\\n\\n        # Data Transformation\\n        logging.info('---------->>> Starting Data Transformation---------->>>')\\n        data_transformation_artifact = pipeline.start_data_transformation(data_ingestion_artifact,data_validation_artifact)\\n        logging.info(f'>>>Data Transformation Completed: {data_transformation_artifact}')\\n\\n        #Model Trainer\\n        logging.info('---------->>> Starting Model Trainer -------------->>>')\\n        model_trainer_artifact = pipeline.strat_model_trainer(data_transformation_artifact)\\n        logging.info(f'>>> Model Trainer Completed: {model_trainer_artifact}')\\n\\n        # #Model Evalution\\n        # logging.info('---------->>> Starting Model evalution -------------->>>')\\n        # model_evalution_artifact = pipeline.start_model_evalution(data_ingestion_artifact,model_trainer_artifact)\\n        # logging.info(f'>>> Model Evalution Completed: {model_evalution_artifact}')\\n\\n\\n        logging.info('Pipeline finished successfully')\\n        \\n    except Exception as e:\\n        raise NetworkSecurityException(e, sys)\\n\\n\"),\n",
       " Document(metadata={'source': 'Input_Repo/Network-Security/templates.py', 'language': <Language.PYTHON: 'python'>}, page_content='import os \\nfrom pathlib import Path \\nimport logging \\n\\nlogging.basicConfig(\\n    filename=\\'setup\\',\\n    level=logging.INFO,\\n    format=\"[%(asctime)s] %(lineno)d %(name)s - %(levelname)s - %(message)s\"\\n)\\n\\nproject_name = \"Network_Security\"   \\n\\nlist_of_file = [\\n    \"Network_Data/\",\\n    f\"{project_name}/__init__.py\",\\n    f\"{project_name}/cloud/__init__.py\",\\n    f\"{project_name}/components/__init__.py\",\\n    f\"{project_name}/constant/__init__.py\",\\n    f\"{project_name}/entity/__init__.py\",\\n    f\"{project_name}/exception/__init__.py\",\\n    f\"{project_name}/logging/__init__.py\",\\n    f\"{project_name}/pipeline/__init__.py\",\\n    f\"{project_name}/utils/__init__.py\",\\n    \"notebooks/\",\\n    \\'app.py\\',\\n    \\'main.py\\',\\n    \\'setup.py\\',\\n    \\'requirements.txt\\'\\n]\\n\\nfor filepath in list_of_file:\\n    filepath = Path(filepath)\\n    file_dir,file_name = os.path.split(filepath)\\n\\n    if file_dir != \"\":\\n        os.makedirs(file_dir,exist_ok=True)\\n        logging.info(f\\'Creating Directory :{file_dir} ot the {file_name}\\')\\n    if (not os.path.exists(filepath) or os.path.getsize(filepath)==0):\\n        with open(filepath,\\'w\\') as f:\\n            pass \\n        logging.info(f\\'Creating {filepath} is empty\\')\\n    else:\\n        logging.info(f\\'{filepath} already exists\\') \\n\\n\\n\\'\\'\\'\\nNETWORKSECURITY\\n│\\n├── .github/\\n├── Network_Data/\\n├── networksecurity/\\n│   ├── cloud/\\n│   ├── components/\\n│   ├── constant/\\n│   ├── entity/\\n│   ├── exception/\\n│   ├── logging/\\n│   ├── pipeline/\\n│   ├── utils/\\n│   └── __init__.py\\n└── notebooks/\\n\\n\\'\\'\\''),\n",
       " Document(metadata={'source': 'Input_Repo/Network-Security/Network_Security/__init__.py', 'language': <Language.PYTHON: 'python'>}, page_content=''),\n",
       " Document(metadata={'source': 'Input_Repo/Network-Security/Network_Security/data_acess/networkdata_acess.py', 'language': <Language.PYTHON: 'python'>}, page_content='from Network_Security.exception.exception import NetworkSecurityException\\nfrom Network_Security.logging.logger import logging\\nfrom Network_Security.configeration.mongodb import MongoDBClient \\nfrom typing import Optional\\nimport pandas as pd\\nimport numpy as np \\nimport sys\\n\\n\\nclass NetworkData:\\n    def __init__(self):\\n        try:\\n            self.mongo_client = MongoDBClient()   \\n        except Exception as e:\\n            raise NetworkSecurityException(e, sys)\\n\\n    def get_dataframe(self, collection_name: str, database_name: Optional[str] = None)->pd.DataFrame:\\n        try:\\n            if database_name:\\n                collection = self.mongo_client.client[database_name][collection_name]\\n            else:\\n                collection = self.mongo_client.database[collection_name]\\n\\n            df = pd.DataFrame(list(collection.find()))\\n            if \"_id\" in df.columns:\\n                df.drop(columns=[\"_id\"], inplace=True)\\n            df.replace(\"na\", np.nan, inplace=True)\\n\\n            logging.info(\"DataFrame Extract Successful\")\\n            return df\\n\\n        except Exception as e:\\n            raise NetworkSecurityException(e, sys)\\n'),\n",
       " Document(metadata={'source': 'Input_Repo/Network-Security/Network_Security/data_acess/__init__.py', 'language': <Language.PYTHON: 'python'>}, page_content=''),\n",
       " Document(metadata={'source': 'Input_Repo/Network-Security/Network_Security/logging/logger.py', 'language': <Language.PYTHON: 'python'>}, page_content='import os \\nimport logging \\nfrom datetime import datetime \\n\\nlog_file = f\\'{datetime.now().strftime(\\'%m_%d_%Y_%H_%M_%S\\')}.log\\'\\nlog_path = os.path.join(os.getcwd(),\\'logs\\',log_file)\\nos.makedirs(log_path,exist_ok=True)\\n\\nlog_file_path = os.path.join(log_path,log_file)\\n\\nlogging.basicConfig(\\n    filename= log_file_path,\\n    level = logging.INFO,\\n    format = \\'[%(asctime)s] %(lineno)s %(name)s - %(levelname)s - %(message)s\\'\\n)\\n\\nlogging.info(\"Logging setup completed successfully!\")'),\n",
       " Document(metadata={'source': 'Input_Repo/Network-Security/Network_Security/logging/__init__.py', 'language': <Language.PYTHON: 'python'>}, page_content=''),\n",
       " Document(metadata={'source': 'Input_Repo/Network-Security/Network_Security/constant/__init__.py', 'language': <Language.PYTHON: 'python'>}, page_content=\"import os\\nimport numpy as np \\nfrom datetime import date\\n\\n# MongoDB\\nDATA_BASE_NAME = 'NETWORK_SECURITY'\\nCOLLECTION_NAME = 'NETWORK_DATA'\\nMONGOBD_URL = 'MONGODB_URL'\\n\\n# Artifacts\\nARTIFACTS = 'artifacts'\\nPIPELINE_DIR = 'network'\\n\\n# DATA \\nRAW_DATA = 'raw.csv'\\nTRAIN_DATA = 'train.csv'\\nTEST_DATA = 'test.csv'\\n\\n\\n# Data_ingestion\\nDATA_INGESTION_DIR: str = 'data_ingestion'\\nDATA_INGESTION_COLLECTION_NAME:str = 'NETWORK_DATA'\\nDATA_INGESTION_FEATURE_STORED_DIR:str = 'feature'\\nDATA_INGESTION_INGESTED_DIR:str = 'ingested'\\nDATA_INGESTION_SPLIT_RATIO:float = 0.2 \\n\\n# Data_validation\\nDATA_VALIDATION_DIR:str = 'data_validation'\\nDATA_VALIDATION_REPORT_DIR:str = 'drift_report'\\nDATA_VALIDATION_REPORT_YAML:str = 'report.yaml'\\n\\n# Data_transformation\\nDATA_TRANSFORMATION_DIR:str = 'data_tranasformation'\\nDATA_TRANSFORMATION_TRANSFORM_FILE:str = 'transform'\\nDATA_TRANSFORMATION_TRANSFORM_0BJECT_FILE:str = 'transform_obj'\\nPREPROCESSING_FILE:str = 'preprocessing.pkl'\\nTARGET_COLUMN = 'result'\\nCURRENT_YEAR = date.today().year\\n# KNNImputer\\nDATA_TRANSFORMATION_IMPUTER_PARAMS: dict={ \\n    'missing_values': np.nan,\\n    'n_neighbors':3,\\n    'weights':'uniform'\\n}\\nSCHEMA_FILE_PATH = os.path.join('data_schema','column.yaml')\\n\\n# model_trainer\\nMODEL_TRAINER_DIR:str = 'model_trainer'\\nMODEL_TRAINER_FILE_NAME:str = 'trained_model'\\nMODEL_TRAINER_TRAINED_MODEL_NAME:str = 'model.pkl'\\nMODEL_TRAINER_CONFIG_PARAM_PATH:str = os.path.join('data_schema','best_param.yaml')\\nMODEL_TRAINER_EXCEPTED_RATIO:float = 0.6 \\n\\n#AWS Configeration\\nREGION = 'us-east-1'\\nAWS_ACCESS_KEY = 'AWS_ACCESS_KEY_ID'\\nAWS_SECRET_KEY = 'AWS_SECRET_ACCESS_KEY' \\n\\n# model_evalution \\nMODEL_BUCKET_NAME:str = 'network_security'\\nMODEL_EVALUTION_CHANGED_THRESHOLD:float = 0.8 \\nMODEL_TRAINER_TRAINED_MODEL_NAME:str = 'model.pkl'\\n\\n\\n\\n\"),\n",
       " Document(metadata={'source': 'Input_Repo/Network-Security/Network_Security/components/data_ingestion.py', 'language': <Language.PYTHON: 'python'>}, page_content='import os\\nimport sys\\nfrom sklearn.model_selection import train_test_split\\nimport pandas as pd\\nfrom Network_Security.exception.exception import NetworkSecurityException\\nfrom Network_Security.logging.logger import logging\\nfrom Network_Security.entity.config import Data_ingestion_Config\\nfrom Network_Security.entity.artifact import Data_Ingestion_Artifact\\nfrom Network_Security.configeration.mongodb import MongoDBClient  \\nfrom Network_Security.data_acess.networkdata_acess import NetworkData \\n\\nclass Data_Ingestion:\\n    def __init__(self, ingestion_config: Data_ingestion_Config):\\n        try:\\n            self.ingestion_config = ingestion_config\\n        except Exception as e:\\n            raise NetworkSecurityException(e, sys)\\n\\n    def get_feature_extract_data(self):\\n        try:\\n            logging.info(\"Extracting data from MongoDB...\")\\n            networkdata = NetworkData()\\n            \\n            dataframe = networkdata.get_dataframe(\\n                collection_name=self.ingestion_config.data_ingestion_collection_path\\n            )\\n            # start feature_store\\n            feature_data_path = self.ingestion_config.data_ingestion_feature_path\\n            os.makedirs(os.path.dirname(feature_data_path), exist_ok=True)\\n            dataframe.to_csv(feature_data_path, index=False, header=True)\\n            logging.info(f\"Data stored at {feature_data_path}\")\\n            return dataframe\\n        except Exception as e:\\n            raise NetworkSecurityException(e, sys)\\n\\n    def split_data(self, dataframe: pd.DataFrame):\\n        try:\\n            train_data, test_data = train_test_split(\\n                dataframe, \\n                test_size=self.ingestion_config.split_ratio\\n            )\\n\\n            train_file_path = self.ingestion_config.train_data_path\\n            os.makedirs(os.path.dirname(train_file_path), exist_ok=True)\\n            train_data.to_csv(train_file_path, index=False, header=True)\\n\\n            test_file_path = self.ingestion_config.test_data_path\\n            os.makedirs(os.path.dirname(test_file_path), exist_ok=True)\\n            test_data.to_csv(test_file_path, index=False, header=True)\\n\\n            logging.info(\"Train & Test datasets saved successfully.\")\\n            return train_data, test_data\\n        except Exception as e:\\n            raise NetworkSecurityException(e, sys)\\n\\n    def init_data_ingestion(self):\\n        try:\\n            dataframe = self.get_feature_extract_data()\\n            print(dataframe.head())\\n            self.split_data(dataframe)\\n\\n            data_ingestion_artifact = Data_Ingestion_Artifact(\\n                train_file_path=self.ingestion_config.train_data_path,\\n                test_file_path=self.ingestion_config.test_data_path\\n            )\\n            logging.info(\"Data Ingestion completed successfully.\")\\n            return data_ingestion_artifact\\n        except Exception as e:\\n            raise NetworkSecurityException(e, sys) \\n        \\n\\n        \\n'),\n",
       " Document(metadata={'source': 'Input_Repo/Network-Security/Network_Security/components/model_evalution.py', 'language': <Language.PYTHON: 'python'>}, page_content='from Network_Security.entity.artifact import (Data_Ingestion_Artifact,\\n                                              Model_Trainer_Artifact,\\n                                              Model_Evalution_Artifact)\\nfrom Network_Security.entity.config import Model_Evalution_Config\\nfrom Network_Security.entity.s3_estimator import NetworkEstimator\\nfrom Network_Security.constant import TARGET_COLUMN\\nfrom sklearn.metrics import f1_score\\nfrom dataclasses import dataclass\\nimport pandas as pd\\n\\n@dataclass \\nclass ModelEvalutionResponse:\\n    is_model_accept:bool\\n    difference:float \\n    train_model_f1_score:float \\n    s3_model_f1_score:float \\n\\nclass ModelEvalution:\\n    def __init__(self,data_ingestion_artifact:Data_Ingestion_Artifact ,\\n                 model_trainer_artifact:Model_Trainer_Artifact,\\n                 model_evalution_config:Model_Evalution_Config):\\n        self.data_ingestion_artifact = data_ingestion_artifact\\n        self.model_trainer_artifact = model_trainer_artifact\\n        self.model_evalution_config = model_evalution_config\\n\\n    def get_best_model(self):\\n        bucket_name = self.model_evalution_config.bucket_name \\n        model_path = self.model_evalution_config.s3_model_path \\n        network_model = NetworkEstimator(bucket_name,model_path)\\n        if network_model.is_model_present(model_path):\\n            # return network_model.load_model()\\n            return True\\n        return None \\n    def evaluate_model(self):\\n        test_df = pd.read_csv(self.data_ingestion_artifact.test_file_path)\\n        x = test_df.drop([TARGET_COLUMN],axis=1)\\n        y = test_df[TARGET_COLUMN]\\n        y = y.replace(-1,0) \\n        best_model = self.get_best_model()\\n        best_model_score = None\\n         \\n        if best_model is not None:\\n            pred = best_model.predict(x)\\n            best_model_score = f1_score(y,pred)\\n        temp_model_score = 0 if best_model_score is None else best_model_score\\n        train_model_score = self.model_trainer_artifact.metrics.f1_score\\n\\n        model_evalution_response = ModelEvalutionResponse(\\n            is_model_accept=train_model_score > temp_model_score,\\n            difference=train_model_score - temp_model_score,\\n            train_model_f1_score=train_model_score,\\n            s3_model_f1_score=best_model_score\\n        )\\n        return model_evalution_response\\n    \\n    def init_model_evaluation(self):\\n        model_evalution_response = self.evaluate_model()\\n        s3_model = self.model_evalution_config.s3_model_path \\n        model_evalution_artifact = Model_Evalution_Artifact(\\n            is_model_accepted= model_evalution_response.is_model_accept,\\n            changed_accuracy=model_evalution_response.difference,\\n            s3_model_path=s3_model,\\n            train_model_path=self.model_trainer_artifact.model_pkl\\n        )\\n        return model_evalution_artifact\\n\\n'),\n",
       " Document(metadata={'source': 'Input_Repo/Network-Security/Network_Security/components/model_train.py', 'language': <Language.PYTHON: 'python'>}, page_content='from Network_Security.entity.artifact import (Data_Transformation_Artifact,\\n                                             Metrics_Artifact,\\n                                              Model_Trainer_Artifact)\\nfrom Network_Security.entity.config import Model_Trainer_Config\\nfrom Network_Security.utils import load_numpy_array,load_object,save_object\\nfrom Network_Security.logging.logger import logging\\nfrom Network_Security.exception.exception import NetworkSecurityException\\n\\nfrom sklearn.metrics import accuracy_score,f1_score,precision_score,recall_score\\nfrom sklearn.pipeline import Pipeline\\nfrom neuro_mf import ModelFactory\\nfrom typing import Tuple\\nimport numpy as np \\nimport pandas as pd\\nimport mlflow\\nimport sys\\nimport dagshub\\ndagshub.init(repo_owner=\\'Ahmed2797\\', repo_name=\\'Network-Security\\', mlflow=True)\\n\\n\\nclass Network_model:\\n    def __init__(self, transform_object: Pipeline, best_model_details: object):\\n        self.transform_object = transform_object\\n        self.best_model_details = best_model_details\\n\\n    def predict(self, dataframe: pd.DataFrame) -> pd.DataFrame:\\n        try:\\n            transformed_features = self.transform_object.transform(dataframe)\\n            predictions = self.best_model_details.predict(transformed_features)\\n\\n            # return pd.DataFrame(predictions, columns=[\\'prediction\\'])\\n            return predictions\\n        except Exception as e:\\n            raise NetworkSecurityException(e,sys)\\n    def __repr__(self):\\n        return f\"{type(self.best_model_details).__name__}()\"\\n\\n    def __str__(self):\\n        return f\"{type(self.best_model_details).__name__}()\"\\n\\nclass Model_Train:\\n    def __init__(self, data_transformation_artifact: Data_Transformation_Artifact,\\n                 model_trainer_config: Model_Trainer_Config):\\n        self.data_transformation_artifact = data_transformation_artifact\\n        self.model_trainer_config = model_trainer_config \\n    \\n    def track_mlflow(self,best_model,metrics_artifact):\\n        try:\\n            with mlflow.start_run():\\n                f1 = metrics_artifact.f1_score\\n                precision = metrics_artifact.precision_score\\n                accuracy = metrics_artifact.accuracy_score\\n                recall = metrics_artifact.recall_score\\n\\n                mlflow.log_metric(\\'f1_score\\', f1)\\n                mlflow.log_metric(\\'precision_score\\', precision)\\n                mlflow.log_metric(\\'accuracy_score\\', accuracy)\\n                mlflow.log_metric(\\'recall_score\\', recall)\\n                try:\\n                    mlflow.sklearn.log_model(best_model,\\'model\\')\\n                except Exception as e:\\n                    logging.info(f\"[WARNING] Failed to log model to MLflow/DagsHub: {e}\")\\n        except Exception as e:\\n            raise NetworkSecurityException(e,sys)\\n    \\n    def get_best_model_indentify(self, train_arr: np.array, test_arr: np.array):\\n        try:\\n            model_factory = ModelFactory(self.model_trainer_config.model_trained_config_param_path)\\n        \\n            xtrain, ytrain = train_arr[:, :-1], train_arr[:, -1]\\n            xtest, ytest = test_arr[:, :-1], test_arr[:, -1]\\n\\n            best_model_details = model_factory.get_best_model(\\n                                X=xtrain,y=ytrain,\\n                                base_accuracy=self.model_trainer_config.excepted_ratio)\\n            \\n            best_model = best_model_details.best_model\\n            print(best_model)\\n            pred = best_model.predict(xtest)\\n\\n            acc = accuracy_score(ytest, pred)\\n            f1 = f1_score(ytest, pred)\\n            recall = recall_score(ytest, pred)\\n            precision = precision_score(ytest, pred)\\n            \\n            metrics_artifact = Metrics_Artifact(f1_score=f1,\\n                                                accuracy_score=acc,\\n                                                recall_score=recall,\\n                                                precision_score=precision)\\n            # track_mlflow\\n            self.track_mlflow(best_model,metrics_artifact)\\n            \\n            print(metrics_artifact)\\n            print(best_model_details.best_score)\\n            print(best_model_details.best_parameters)\\n            \\n            return best_model_details, metrics_artifact\\n        except Exception as e:\\n            raise NetworkSecurityException(e,sys)\\n    \\n    def init_best_model(self):\\n        try:\\n            train_arr = load_numpy_array(self.data_transformation_artifact.transform_train_file)\\n            test_arr = load_numpy_array(self.data_transformation_artifact.transform_test_file)\\n\\n            best_model_details, metrics_artifact = self.get_best_model_indentify(train_arr, test_arr)\\n            transform_object = load_object(self.data_transformation_artifact.transform_object)\\n            #print(best_model_details)\\n            save_object(\\'final_model/model.pkl\\', best_model_details)\\n            #print(metrics_artifact)\\n            if best_model_details.best_score < self.model_trainer_config.excepted_ratio:\\n                logging.info(\"Best model not found with expected accuracy.\")\\n\\n            network_model_obj = Network_model(transform_object, best_model_details)\\n            save_object(self.model_trainer_config.model_trained_path, network_model_obj)\\n\\n\\n            model_trainer_artifact = Model_Trainer_Artifact(\\n                model_pkl=self.model_trainer_config.model_trained_path,\\n                metrics=metrics_artifact\\n            )\\n\\n            return model_trainer_artifact\\n        except Exception as e:\\n                raise NetworkSecurityException(e,sys)\\n\\n\\n'),\n",
       " Document(metadata={'source': 'Input_Repo/Network-Security/Network_Security/components/data_validation.py', 'language': <Language.PYTHON: 'python'>}, page_content='from Network_Security.logging.logger import logging\\nfrom Network_Security.constant import SCHEMA_FILE_PATH\\nfrom Network_Security.utils import read_yaml_file, write_yaml_file\\nfrom Network_Security.entity.artifact import Data_Ingestion_Artifact, Data_validation_Artifact\\nfrom Network_Security.entity.config import Data_validation_config\\nfrom Network_Security.exception.exception import NetworkSecurityException\\nfrom evidently import Report\\nfrom evidently.presets import DataDriftPreset\\nimport pandas as pd\\nimport json\\nimport sys\\n\\n\\nclass Data_validation:\\n    def __init__(self, data_ingestion_artifact: Data_Ingestion_Artifact,\\n                data_validation_config: Data_validation_config):\\n        try:\\n            self.data_ingestion_artifact = data_ingestion_artifact\\n            self.data_validation_config = data_validation_config\\n            self._schema_yaml = read_yaml_file(file_path=SCHEMA_FILE_PATH)\\n            if self._schema_yaml is None:\\n                raise ValueError(f\"Schema file not loaded or is empty: {SCHEMA_FILE_PATH}\")\\n        except Exception as e:\\n            raise NetworkSecurityException(e,sys)\\n        \\n    #if number of columns matches schema:\\n    def valid_no_columns(self, dataframe: pd.DataFrame) -> bool:\\n        try:\\n            expected_columns = self._schema_yaml[\\'columns\\']\\n            status = len(dataframe.columns) == len(expected_columns)\\n            return status\\n        except Exception as e:\\n            raise NetworkSecurityException(e,sys)\\n\\n    #if all expected columns exist:\\n    def is_column_exists(self, dataframe: pd.DataFrame) -> bool:\\n        try:\\n            missing_num_columns = [col for col in self._schema_yaml[\\'numeric_columns\\'] if col not in dataframe.columns]\\n            missing_cat_columns = [col for col in self._schema_yaml[\\'categorical_columns\\'] if col not in dataframe.columns]\\n\\n            if missing_num_columns:\\n                logging.info(f\\'Missing numeric columns: {missing_num_columns}\\')\\n            if missing_cat_columns:\\n                logging.info(f\\'Missing categorical columns: {missing_cat_columns}\\')\\n\\n            status = not (len(missing_num_columns) > 0 or len(missing_cat_columns) > 0)\\n            return status\\n        except Exception as e:\\n            raise NetworkSecurityException(e,sys)\\n\\n    def detect_dataset_drift(self, reference_df: pd.DataFrame, current_df: pd.DataFrame) -> bool:\\n        try:\\n            report = Report([DataDriftPreset()],include_tests=\"True\")\\n            report = report.run(reference_data=reference_df, current_data=current_df)\\n            report.save_html(\"data_drift_report.html\")\\n            json_report = report.json()\\n            report_dict = json.loads(json_report)\\n            write_yaml_file(\\n                file_path=self.data_validation_config.data_validation_report,\\n                content=report_dict)\\n            \\n            n_features = sum(1 for m in report_dict[\"metrics\"] if \"ValueDrift\" in m[\"metric_id\"])\\n            drift_metric = next(m for m in report_dict[\"metrics\"] if \"DriftedColumnsCount\" in m[\"metric_id\"])\\n            n_drifted_features = drift_metric[\"value\"][\"count\"]\\n            # Dataset drift status\\n            drift_status = n_drifted_features > 0\\n            print(n_features, n_drifted_features, drift_status)\\n            logging.info(f\"{n_drifted_features}/{n_features} features show drift.\")\\n            return drift_status    \\n        except Exception as e:\\n            logging.info(f\"Error in dataset drift detection: {e}\")\\n            raise NetworkSecurityException (e,sys)\\n  \\n    # Static method to read CSV\\n    @staticmethod\\n    def read_data(file_path: str) -> pd.DataFrame:\\n        return pd.read_csv(file_path)\\n    \\n    def init_data_validation(self) -> Data_validation_Artifact:\\n        try:\\n            valid_message_error = []\\n            # Read train and test data\\n            train_data = self.read_data(self.data_ingestion_artifact.train_file_path)\\n            test_data = self.read_data(self.data_ingestion_artifact.test_file_path)\\n            # train data\\n            if not self.valid_no_columns(train_data):\\n                valid_message_error.append(\\'Error: Column Mismatch in train data\\')\\n            if not self.is_column_exists(train_data):\\n                valid_message_error.append(\\'Error: Missing columns in train data\\')\\n            #test data\\n            if not self.valid_no_columns(test_data):\\n                valid_message_error.append(\\'Error: Column Mismatch in test data\\')\\n            if not self.is_column_exists(test_data):\\n                valid_message_error.append(\\'Error: Missing columns in test data\\')\\n\\n            # Drift detection\\n            validation_status = len(valid_message_error) == 0\\n            if validation_status:\\n                drift_status = self.detect_dataset_drift(train_data, test_data)\\n                if drift_status:\\n                    valid_message_error.append(\\'Drift detected\\')\\n                else:\\n                    valid_message_error.append(\\'Drift not detected\\')\\n            else:\\n                logging.info(f\\'Validation errors: {valid_message_error}\\')\\n\\n            #Create artifact\\n            data_validation_artifact = Data_validation_Artifact(\\n                validation_status=validation_status,\\n                message_error=valid_message_error,\\n                drift_report_file_path=self.data_validation_config.data_validation_report\\n            )\\n            return data_validation_artifact\\n\\n        except Exception as e:\\n            raise NetworkSecurityException(e, sys)\\n\\n'),\n",
       " Document(metadata={'source': 'Input_Repo/Network-Security/Network_Security/components/data_transformation.py', 'language': <Language.PYTHON: 'python'>}, page_content='from Network_Security.constant import * \\nfrom Network_Security.exception.exception import NetworkSecurityException\\nfrom Network_Security.logging.logger import logging\\nfrom Network_Security.utils import read_yaml_file, save_object, save_numpy_array\\nfrom Network_Security.entity.artifact import (\\n    Data_Ingestion_Artifact,\\n    Data_validation_Artifact,\\n    Data_Transformation_Artifact\\n)\\nfrom Network_Security.entity.config import Data_Transformation_Config\\n\\nfrom sklearn.pipeline import Pipeline\\nfrom sklearn.impute import KNNImputer\\nimport pandas as pd \\nimport numpy as np\\nimport sys\\n\\n\\nclass DataTransformation:\\n    def __init__(self,\\n                 data_ingestion_artifact: Data_Ingestion_Artifact,\\n                 data_validation_artifact: Data_validation_Artifact,\\n                 data_transformation_config: Data_Transformation_Config):\\n        try:\\n            self.data_ingestion_artifact = data_ingestion_artifact\\n            self.data_validation_artifact = data_validation_artifact\\n            self.data_transformation_config = data_transformation_config\\n            self._schema_config = read_yaml_file(SCHEMA_FILE_PATH)\\n        except Exception as e:\\n            raise NetworkSecurityException(e, sys)\\n    \\n    def get_data_transformation(self) -> Pipeline:\\n        try:\\n            imputer = KNNImputer(**DATA_TRANSFORMATION_IMPUTER_PARAMS)\\n            processor = Pipeline([(\\'imputer\\', imputer)])\\n            return processor\\n        except Exception as e:\\n            raise NetworkSecurityException(e, sys)\\n        \\n    @staticmethod\\n    def read_data(file_path: str) -> pd.DataFrame:\\n        try:\\n            return pd.read_csv(file_path)\\n        except Exception as e:\\n            raise NetworkSecurityException(e, sys)\\n    \\n    def init_data_transformation(self):\\n        try:\\n            logging.info(\"Data read and apply preprocessing & transformation\")\\n            train_df = DataTransformation.read_data(self.data_ingestion_artifact.train_file_path)\\n            test_df = DataTransformation.read_data(self.data_ingestion_artifact.test_file_path)\\n\\n            # Train features & target\\n            input_feature_train = train_df.drop(columns=[TARGET_COLUMN], axis=1)\\n            target_feature_train = train_df[TARGET_COLUMN].replace(-1, 0)\\n\\n            # Test features & target\\n            input_feature_test = test_df.drop(columns=[TARGET_COLUMN], axis=1)\\n            target_feature_test = test_df[TARGET_COLUMN].replace(-1, 0)\\n\\n            # Preprocessor\\n            preprocessor = self.get_data_transformation()\\n            input_feature_train_arr = preprocessor.fit_transform(input_feature_train)\\n            input_feature_test_arr = preprocessor.transform(input_feature_test)\\n\\n            # Combine arrays\\n            train_arr = np.c_[input_feature_train_arr, np.array(target_feature_train)]\\n            test_arr = np.c_[input_feature_test_arr, np.array(target_feature_test)]\\n\\n            # Save transformation pipeline and arrays\\n            save_object(\\'final_model/preprocessor.pkl\\',preprocessor) # use FastAPI\\n            save_object(self.data_transformation_config.data_transformation_object_pkl, obj=preprocessor)\\n            save_numpy_array(self.data_transformation_config.data_transformation_train_file, array=train_arr)\\n            save_numpy_array(self.data_transformation_config.data_transformation_test_file, array=test_arr)\\n            logging.info(\\'Array loaded succesfully\\')\\n\\n            data_transformation_artifact = Data_Transformation_Artifact(\\n                transform_object=self.data_transformation_config.data_transformation_object_pkl,\\n                transform_train_file=self.data_transformation_config.data_transformation_train_file,\\n                transform_test_file=self.data_transformation_config.data_transformation_test_file\\n            )\\n\\n            return data_transformation_artifact\\n\\n        except Exception as e:\\n            raise NetworkSecurityException(e, sys)\\n'),\n",
       " Document(metadata={'source': 'Input_Repo/Network-Security/Network_Security/components/__init__.py', 'language': <Language.PYTHON: 'python'>}, page_content=''),\n",
       " Document(metadata={'source': 'Input_Repo/Network-Security/Network_Security/exception/__init__.py', 'language': <Language.PYTHON: 'python'>}, page_content=''),\n",
       " Document(metadata={'source': 'Input_Repo/Network-Security/Network_Security/exception/exception.py', 'language': <Language.PYTHON: 'python'>}, page_content='import sys \\nfrom Network_Security.logging.logger import logging\\n\\nclass NetworkSecurityException(Exception):   \\n\\n    def __init__(self, error_message, error_details: sys):\\n        self.error_message = error_message \\n        _, _, exc_tb = error_details.exc_info()\\n\\n        self.lineno = exc_tb.tb_lineno \\n        self.file_name = exc_tb.tb_frame.f_code.co_filename \\n    \\n    def __str__(self):\\n        return (\\n            f\"Error occurred in Python script [{self.file_name}] \"\\n            f\"at line number [{self.lineno}] \"\\n            f\"with error: {str(self.error_message)}\"\\n        )\\n    \\n\\nif __name__ == \\'__main__\\':\\n    try:\\n        logging.info(\\'Try the logging&Exception\\')\\n        x = 1 / 0\\n    except Exception as e:\\n        raise NetworkSecurityException(e, sys)\\n\\n\\n        \\n'),\n",
       " Document(metadata={'source': 'Input_Repo/Network-Security/Network_Security/cloud/aws_service.py', 'language': <Language.PYTHON: 'python'>}, page_content=\"from Network_Security.configeration.aws_connection import S3Client\\n\\n\\nfrom typing import List,Union\\nfrom io import StringIO\\nimport pickle\\nclass SimpleStorageService:\\n    def __init__(self):\\n        s3_client = S3Client()\\n        self.s3_client = s3_client.s3_client\\n        self.resource = s3_client.resource\\n\\n    def s3_key_path_available(self,bucket_name,s3_key)->bool:\\n        bucket = self.resource.Bucket(bucket_name)\\n        file_object = [file_object for file_object in bucket.objects.filter(prefix=s3_key)]\\n        if len(file_object)>0:\\n            return True \\n        else:\\n            return False  \\n    def get_file_object(self,bucket_name,model_path)->Union[List[object],object]:\\n        bucket = self.resource.Bucket(bucket_name)\\n        file_object = [file_object for file_object in bucket.objects.filter(prefix=model_path)]\\n        func = lambda x:x[0] if len(x)==1 else x\\n        file_obj = func(file_object)\\n        return file_obj\\n\\n\\n    @staticmethod\\n    def read_object(file_object,decode: bool=True,model_readable: bool=False):\\n        func = (lambda:file_object.get()['Body'].read().decode()\\n                if decode is True \\n                else file_object.get()['Body'].read())\\n        conv_func = lambda:StringIO(func()) if model_readable is True else func()\\n        return conv_func \\n    \\n    def load_model(self,bucket_name,model_name,model_dir=None):\\n        func = (lambda: model_name\\n                if model_dir is None\\n                else model_dir + '/' + model_name)\\n        model_path = func()\\n        file_object = self.get_file_object(bucket_name=bucket_name,model_path=model_path)\\n        model_object = self.read_object(file_object,decode=False)\\n        model = pickle.load(model_object)\\n        return model\\n\\n\"),\n",
       " Document(metadata={'source': 'Input_Repo/Network-Security/Network_Security/cloud/__init__.py', 'language': <Language.PYTHON: 'python'>}, page_content='import os\\n\\n\\nclass S3Sync:\\n    def sync_folder_to_s3(self, folder, aws_bucket_url):\\n        command = f\"aws s3 sync {folder} {aws_bucket_url}\"\\n        os.system(command)\\n\\n    def sync_folder_from_s3(self, folder, aws_bucket_url):\\n        command = f\"aws s3 sync {aws_bucket_url} {folder}\"\\n        os.system(command)'),\n",
       " Document(metadata={'source': 'Input_Repo/Network-Security/Network_Security/utils/__init__.py', 'language': <Language.PYTHON: 'python'>}, page_content='import os\\nimport yaml\\nimport sys\\nimport pickle\\nimport numpy as np\\nfrom Network_Security.exception.exception import NetworkSecurityException\\n\\n# def read_yaml_file(file_path:str)->dict:\\n#     if not os.path.exists(file_path):\\n#         raise FileNotFoundError(f\"File not found: {file_path}\")\\n#     with open(file_path, \\'rb\\') as file:\\n#         yaml.safe_load(file) \\n    \\ndef read_yaml_file(file_path: str) -> dict:\\n    if not os.path.exists(file_path):\\n        raise FileNotFoundError(f\"File not found: {file_path}\")\\n    with open(file_path, \\'r\\') as file:  # text mode is fine for YAML\\n        data = yaml.safe_load(file)\\n    if data is None:\\n        raise ValueError(f\"YAML file is empty: {file_path}\")\\n    return data\\n\\n\\ndef write_yaml_file(file_path: str, content: object, replace: bool = False) -> None:\\n    \\n        if replace:\\n            if os.path.exists(file_path):\\n                os.remove(file_path)   \\n        os.makedirs(os.path.dirname(file_path), exist_ok=True)  \\n        with open(file_path, \"w\") as file:\\n            return yaml.dump(content, file)  \\n    \\ndef save_object(file_path: str, obj: object):\\n    try:\\n        os.makedirs(os.path.dirname(file_path), exist_ok=True)\\n        with open(file_path, \\'wb\\') as file_obj:\\n            pickle.dump(obj, file_obj)  \\n    except Exception as e:\\n        raise NetworkSecurityException(e, sys)\\n     \\n\\ndef save_numpy_array(file_path: str, array: np.array):\\n    try:\\n        os.makedirs(os.path.dirname(file_path), exist_ok=True)\\n        with open(file_path, \\'wb\\') as file_obj:\\n            return np.save(file_obj, array)  \\n    except Exception as e:\\n        raise NetworkSecurityException(e, sys)\\n\\ndef load_numpy_array(file_path:str)->np.array:\\n    try:\\n        os.makedirs(os.path.dirname(file_path),exist_ok=True)\\n        with open(file_path,\\'rb\\') as file_obj:\\n            return np.load(file_obj)\\n    except Exception as e:\\n          raise NetworkSecurityException(e,sys)\\n    \\n\\ndef load_object(file_path: str) -> object:\\n    try:\\n        with open(file_path, \"rb\") as file_obj:\\n            obj = pickle.load(file_obj)\\n        return obj\\n    except Exception as e:\\n        raise NetworkSecurityException(e, sys)'),\n",
       " Document(metadata={'source': 'Input_Repo/Network-Security/Network_Security/entity/estimator.py', 'language': <Language.PYTHON: 'python'>}, page_content='import sys \\nfrom sklearn.pipeline import Pipeline \\nfrom Network_Security.constant import TARGET_COLUMN\\nfrom Network_Security.exception.exception import NetworkSecurityException \\n\\nimport pandas as pd \\n\\nclass Network_model:\\n    def __init__(self, transform_object: Pipeline, best_model_details: object):\\n        self.transform_object = transform_object\\n        self.best_model_details = best_model_details\\n\\n    def predict(self, dataframe: pd.DataFrame) -> pd.DataFrame:\\n        try:\\n            # x = dataframe.drop(columns=[TARGET_COLUMN], axis=1)\\n            # y = dataframe[TARGET_COLUMN].replace(-1, 0)\\n            transformed_features = self.transform_object.transform(dataframe)\\n            model = getattr(self.best_model_details, \"best_model\", self.best_model_details)\\n            predictions = model.predict(transformed_features)\\n            return predictions\\n            #return pd.DataFrame(predictions, columns=[\\'prediction\\'])\\n        except Exception as e:\\n            raise NetworkSecurityException(e,sys)\\n    def __repr__(self):\\n        return f\"{type(self.best_model_details).__name__}()\"\\n\\n    def __str__(self):\\n        return f\"{type(self.best_model_details).__name__}()\"'),\n",
       " Document(metadata={'source': 'Input_Repo/Network-Security/Network_Security/entity/__init__.py', 'language': <Language.PYTHON: 'python'>}, page_content=''),\n",
       " Document(metadata={'source': 'Input_Repo/Network-Security/Network_Security/entity/config.py', 'language': <Language.PYTHON: 'python'>}, page_content=\"from dataclasses import dataclass \\nfrom datetime import datetime\\nfrom Network_Security.constant import *\\nfrom Network_Security.entity.artifact import Metrics_Artifact\\n\\n\\nTIMESTAMP = datetime.now().strftime('%m_%d_%Y_%H_%M_%S')\\n\\n@dataclass \\nclass NS_Train_Configeration:\\n    artifact_dir:str = os.path.join(ARTIFACTS,TIMESTAMP)\\n    pipeline_dir:str = PIPELINE_DIR\\n    TIMESTAMP:str = TIMESTAMP\\n    model_dir:str = os.path.join('final_model')\\n\\ntrain_config = NS_Train_Configeration()\\n\\n\\n@dataclass \\nclass Data_ingestion_Config:\\n    data_ingestion_path = os.path.join(train_config.artifact_dir,DATA_INGESTION_DIR)\\n    data_ingestion_collection_path = DATA_INGESTION_COLLECTION_NAME \\n    data_ingestion_feature_path = os.path.join(data_ingestion_path,DATA_INGESTION_FEATURE_STORED_DIR,RAW_DATA)\\n    train_data_path = os.path.join(data_ingestion_path,DATA_INGESTION_INGESTED_DIR,TRAIN_DATA)\\n    test_data_path = os.path.join(data_ingestion_path,DATA_INGESTION_INGESTED_DIR,TEST_DATA)\\n    split_ratio = DATA_INGESTION_SPLIT_RATIO \\n\\n@dataclass\\nclass Data_validation_config:\\n    data_validation_dir = os.path.join(train_config.artifact_dir,DATA_VALIDATION_DIR)\\n    data_validation_report = os.path.join(data_validation_dir,DATA_VALIDATION_REPORT_DIR,DATA_VALIDATION_REPORT_YAML)\\n\\n@dataclass \\nclass Data_Transformation_Config:\\n    data_transformation_dir = os.path.join(train_config.artifact_dir,DATA_TRANSFORMATION_DIR)\\n    data_transformation_train_file = os.path.join(data_transformation_dir,DATA_TRANSFORMATION_TRANSFORM_FILE,TRAIN_DATA.replace('csv','npy'))\\n    data_transformation_test_file = os.path.join(data_transformation_dir,DATA_TRANSFORMATION_TRANSFORM_FILE,TEST_DATA.replace('csv','npy'))\\n    data_transformation_object_pkl = os.path.join(data_transformation_dir,DATA_TRANSFORMATION_TRANSFORM_0BJECT_FILE,PREPROCESSING_FILE)\\n\\n@dataclass \\nclass Model_Trainer_Config:\\n    model_trainer_dir = os.path.join(train_config.artifact_dir, MODEL_TRAINER_DIR)\\n    model_trained_path = os.path.join(model_trainer_dir, MODEL_TRAINER_FILE_NAME, MODEL_TRAINER_TRAINED_MODEL_NAME)\\n    model_trained_config_param_path = MODEL_TRAINER_CONFIG_PARAM_PATH\\n    excepted_ratio = MODEL_TRAINER_EXCEPTED_RATIO\\n\\n@dataclass \\nclass Model_Evalution_Config:\\n    bucket_name:str = MODEL_BUCKET_NAME \\n    s3_model_path:str = MODEL_TRAINER_TRAINED_MODEL_NAME\\n    changed_model_score:float =  MODEL_EVALUTION_CHANGED_THRESHOLD\\n\\n\\n\"),\n",
       " Document(metadata={'source': 'Input_Repo/Network-Security/Network_Security/entity/s3_estimator.py', 'language': <Language.PYTHON: 'python'>}, page_content='from Network_Security.cloud.aws_service import SimpleStorageService\\n#from Network_Security.components.model_train import Network_model\\nfrom Network_Security.entity.estimator import Network_model\\n\\nclass NetworkEstimator:\\n    def __init__(self,bucket_name,model_path):\\n        self.model_path = model_path \\n        self.bucket_name = bucket_name\\n        self.s3 = SimpleStorageService() \\n        self.loaded_model:Network_model=None\\n\\n    def is_model_present(self,model_path):\\n     return self.s3.s3_key_path_available(bucket_name=self.bucket_name,\\n                                          model_path=model_path)\\n    def load_model(self)->Network_model:\\n        model_pkl = self.s3.load_model(bucket_name=self.bucket_name,\\n                                  model_name=self.model_path)\\n        return model_pkl'),\n",
       " Document(metadata={'source': 'Input_Repo/Network-Security/Network_Security/entity/artifact.py', 'language': <Language.PYTHON: 'python'>}, page_content='from dataclasses import dataclass \\n\\n@dataclass \\nclass Data_Ingestion_Artifact:\\n    train_file_path:str\\n    test_file_path:str \\n\\n@dataclass \\nclass Data_validation_Artifact:\\n    validation_status:bool \\n    message_error:str \\n    drift_report_file_path:str\\n\\n@dataclass \\nclass Data_Transformation_Artifact:\\n    transform_object:str\\n    transform_train_file:str \\n    transform_test_file:str \\n\\n@dataclass \\nclass Metrics_Artifact:\\n    f1_score:float \\n    accuracy_score:float\\n    recall_score:float \\n    precision_score:float\\n\\n@dataclass \\nclass Model_Trainer_Artifact:\\n    model_pkl:str \\n    metrics : Metrics_Artifact \\n\\n@dataclass \\nclass Model_Evalution_Artifact:\\n    is_model_accepted:bool \\n    changed_accuracy:float\\n    train_model_path:str \\n    s3_model_path:str'),\n",
       " Document(metadata={'source': 'Input_Repo/Network-Security/Network_Security/pipeline/prediction_pipe.py', 'language': <Language.PYTHON: 'python'>}, page_content='from dataclasses import dataclass,asdict\\nimport pandas as pd\\n\\n@dataclass\\nclass NetworkSecurity_Features:\\n    having_ip_address: int\\n    url_length: int\\n    shortining_service: int\\n    having_at_symbol: int\\n    double_slash_redirecting: int\\n    prefix_suffix: int\\n    having_sub_domain: int\\n    sslfinal_state: int\\n    domain_registration_length: int\\n    favicon: int\\n    port: int\\n    https_token: int\\n    request_url: int\\n    url_of_anchor: int\\n    links_in_tags: int\\n    sfh: int\\n    submitting_to_email: int\\n    abnormal_url: int\\n    redirect: int\\n    on_mouseover: int\\n    rightclick: int\\n    popupwindow: int\\n    iframe: int\\n    age_of_domain: int\\n    dnsrecord: int\\n    web_traffic: int\\n    page_rank: int\\n    google_index: int\\n    links_pointing_to_page: int\\n    statistical_report: int\\n\\n    # Convert to dict\\n    def dict_data(self):\\n        return asdict(self) \\n    # dict-->>--DataFrame\\n    def dict_data_to_dataframe(self):\\n        data = pd.DataFrame(self.dict_data())\\n        return data'),\n",
       " Document(metadata={'source': 'Input_Repo/Network-Security/Network_Security/pipeline/__init__.py', 'language': <Language.PYTHON: 'python'>}, page_content=''),\n",
       " Document(metadata={'source': 'Input_Repo/Network-Security/Network_Security/pipeline/train_pipeline.py', 'language': <Language.PYTHON: 'python'>}, page_content='from Network_Security.components.data_ingestion import Data_Ingestion\\nfrom Network_Security.components.data_validation import Data_validation\\nfrom Network_Security.components.data_transformation import DataTransformation\\nfrom Network_Security.components.model_train import Model_Train\\nfrom Network_Security.components.model_evalution import ModelEvalution\\nfrom Network_Security.constant import MODEL_BUCKET_NAME\\n\\nfrom Network_Security.entity.config import (Data_ingestion_Config,\\n                                            Data_validation_config,\\n                                            Data_Transformation_Config,\\n                                            Model_Trainer_Config,\\n                                            Model_Evalution_Config) \\n\\nfrom Network_Security.entity.artifact import (Data_Ingestion_Artifact,\\n                                              Data_validation_Artifact,\\n                                              Data_Transformation_Artifact,\\n                                              Model_Trainer_Artifact,\\n                                              Model_Evalution_Artifact)\\nfrom Network_Security.cloud import S3Sync\\nfrom Network_Security.exception.exception import NetworkSecurityException\\nfrom Network_Security.entity.config import NS_Train_Configeration\\nimport sys\\n\\n\\nclass Training_Pipeline:\\n    def __init__(self):\\n        self.data_ingestion_config = Data_ingestion_Config()\\n        self.data_validation_config = Data_validation_config()\\n        self.data_transformation_config = Data_Transformation_Config()\\n        self.model_trainer_config = Model_Trainer_Config()\\n        self.model_evalution_config  = Model_Evalution_Config()\\n\\n        self.s3_sync = S3Sync()\\n        self.ns_train_config = NS_Train_Configeration()\\n\\n\\n    def start_data_ingestion(self)->Data_Ingestion_Artifact:\\n        try:\\n            data_ingestion = Data_Ingestion(ingestion_config=self.data_ingestion_config)\\n            data_ingestion_artifact = data_ingestion.init_data_ingestion()\\n            return data_ingestion_artifact \\n        except Exception as e:\\n            raise NetworkSecurityException(e,sys)\\n    \\n    def start_data_validation(self, data_ingestion_artifact: Data_Ingestion_Artifact) -> Data_validation_Artifact:\\n        try:\\n            data_valid = Data_validation(data_ingestion_artifact=data_ingestion_artifact,\\n                                        data_validation_config=self.data_validation_config)\\n            data_validation_artifact = data_valid.init_data_validation()\\n            return data_validation_artifact\\n        except Exception as e:\\n            raise NetworkSecurityException(e,sys)\\n\\n\\n    def start_data_transformation(self,data_ingestion_artifact: Data_Ingestion_Artifact,\\n                                  data_validation_artifact:Data_validation_Artifact)->Data_Transformation_Artifact:\\n        try:\\n            data_transformation = DataTransformation(data_ingestion_artifact=data_ingestion_artifact,\\n                                                data_validation_artifact=data_validation_artifact,\\n                                                data_transformation_config=self.data_transformation_config)\\n            data_transformation_artifact = data_transformation.init_data_transformation()\\n            return data_transformation_artifact\\n        except Exception as e:\\n            raise NetworkSecurityException(e,sys)\\n        \\n    def strat_model_trainer(self,data_transformation_artifact:Data_Transformation_Artifact)->Model_Trainer_Artifact:\\n        try:\\n            model_train = Model_Train(data_transformation_artifact=data_transformation_artifact,\\n                                    model_trainer_config=self.model_trainer_config)\\n            model_trainer_artifact=model_train.init_best_model()\\n            return model_trainer_artifact\\n        except Exception as e:\\n            raise NetworkSecurityException(e,sys)\\n        \\n    \\n# K ---------------------------------------------------------------->    \\n    # artifact --> S3\\n    def sync_artifact_dir_to_s3(self):\\n        try:\\n            aws_bucket_url = f\"s3://{MODEL_BUCKET_NAME}/artifact/{self.ns_train_config.TIMESTAMP}\"\\n            self.s3_sync.sync_folder_to_s3(\\n                folder=self.ns_train_config.artifact_dir,\\n                aws_bucket_url=aws_bucket_url)\\n        except Exception as e:\\n            raise NetworkSecurityException(e,sys)\\n    \\n    # final_model --> S3\\n    def sync_saved_model_dir_to_s3(self):\\n        try:\\n            aws_bucket_url = f\"s3://{MODEL_BUCKET_NAME}/final_model/{self.ns_train_config.TIMESTAMP}\"\\n            self.s3_sync.sync_folder_to_s3(\\n                folder=self.ns_train_config.model_dir,\\n                aws_bucket_url=aws_bucket_url\\n            )\\n        except Exception as e:\\n            raise NetworkSecurityException(e,sys)\\n# K----------------------------------------------------------------->\\n        \\n\\n\\n\\n    # B -->\\n    def start_model_evalution(self,data_ingestion_artifact:Data_Ingestion_Artifact,\\n                              model_trainer_artifact:Model_Trainer_Artifact)->Model_Evalution_Artifact:\\n        try:\\n            model_evaluate = ModelEvalution(data_ingestion_artifact = data_ingestion_artifact, \\n                                            model_trainer_artifact = model_trainer_artifact, \\n                                            model_evalution_config = self.model_evalution_config)\\n            model_evalution_artifact = model_evaluate.init_model_evaluation()\\n            return model_evalution_artifact\\n        except Exception as e:\\n            raise NetworkSecurityException(e,sys)\\n\\n    def run_pipeline(self)->None:\\n        try:\\n            data_ingestion_artifact = self.start_data_ingestion()\\n            data_validation_artifact=self.start_data_validation(data_ingestion_artifact)\\n            data_transformation_artifact = self.start_data_transformation(data_ingestion_artifact,data_validation_artifact)\\n            model_trainer_artifact = self.strat_model_trainer(data_transformation_artifact)\\n\\n\\n            # --------------------------------->\\n            # self.sync_artifact_dir_to_s3()\\n            # self.sync_saved_model_dir_to_s3()\\n            # --------------------------------->\\n\\n\\n            # model_evalution_artifact = self.start_model_evalution(model_trainer_artifact)\\n            # return model_evalution_artifact\\n        except Exception as e:\\n            raise NetworkSecurityException(e,sys)\\n\\n        return None'),\n",
       " Document(metadata={'source': 'Input_Repo/Network-Security/Network_Security/research/__init__.py', 'language': <Language.PYTHON: 'python'>}, page_content=''),\n",
       " Document(metadata={'source': 'Input_Repo/Network-Security/Network_Security/configeration/aws_connection.py', 'language': <Language.PYTHON: 'python'>}, page_content='import os\\nimport boto3\\nfrom Network_Security.constant import (REGION,AWS_ACCESS_KEY,AWS_SECRET_KEY)\\nfrom dotenv import load_dotenv\\nload_dotenv()\\n\\nclass S3Client:\\n    s3_client = None\\n    s3_resource = None \\n\\n    def __init__(self, region_name=REGION):    \\n        _access_key = os.getenv(AWS_ACCESS_KEY)\\n        _secret_key = os.getenv(AWS_SECRET_KEY)  \\n\\n        if _access_key is None or _secret_key is None:\\n            raise ValueError(\"Missing AWS credentials in environment variables\")\\n\\n        if S3Client.s3_client is None and S3Client.s3_resource is None:\\n\\n            S3Client.s3_resource = boto3.resource(\\n                \\'s3\\',\\n                aws_access_key_id=_access_key,\\n                aws_secret_access_key=_secret_key,\\n                region_name=region_name\\n            )\\n            S3Client.s3_client = boto3.client(\\n                \\'s3\\',\\n                aws_access_key_id=_access_key,\\n                aws_secret_access_key=_secret_key,\\n                region_name=region_name\\n            )\\n        self.s3_client = S3Client.s3_client\\n        self.s3_resource = S3Client.s3_resource\\n'),\n",
       " Document(metadata={'source': 'Input_Repo/Network-Security/Network_Security/configeration/mongodb.py', 'language': <Language.PYTHON: 'python'>}, page_content='from Network_Security.constant import MONGOBD_URL, DATA_BASE_NAME\\nfrom Network_Security.exception.exception import NetworkSecurityException\\nfrom Network_Security.logging.logger import logging\\nfrom dotenv import load_dotenv \\nimport certifi \\nimport pymongo\\nimport sys\\nimport os \\n\\n\\nload_dotenv()\\n# MONGOBD_URL = os.getenv(\"MONGOBD_URL\")\\nca = certifi.where()  \\n\\nclass MongoDBClient:\\n    def __init__(self, database=DATA_BASE_NAME):\\n        try:\\n            mongo_url = os.getenv(MONGOBD_URL)\\n            if mongo_url is None:\\n                logging.info(\"MongoDB URL not found in environment variables\")\\n                raise ValueError(\"MongoDB URL is missing\")\\n\\n            MongoDBClient.client = pymongo.MongoClient(mongo_url, tlsCAFile=ca)\\n            self.client = MongoDBClient.client \\n            self.database = self.client[database]\\n            self.database_name = database  \\n\\n        except Exception as e:\\n            raise NetworkSecurityException(e, sys)\\n \\n    \\n\\n'),\n",
       " Document(metadata={'source': 'Input_Repo/Network-Security/Network_Security/configeration/__init__.py', 'language': <Language.PYTHON: 'python'>}, page_content='')]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdae536f",
   "metadata": {},
   "source": [
    "## split_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created 181 chunks from 35 documents\n",
      "Chunk 1: 323 characters\n",
      "Chunk 2: 198 characters\n",
      "Chunk 3: 357 characters\n",
      "Chunk 4: 161 characters\n",
      "Chunk 5: 471 characters\n"
     ]
    }
   ],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_core.documents import Document\n",
    "\n",
    "from langchain_classic.text_splitter import Language\n",
    "from typing import List\n",
    "\n",
    "def split_documents(documents: List[Document], \n",
    "                   chunk_size: int = 500, \n",
    "                   chunk_overlap: int = 100) -> List[Document]:\n",
    "    \"\"\"\n",
    "    Split documents into smaller chunks for processing.\n",
    "    \n",
    "    Args:\n",
    "        documents: List of documents to split\n",
    "        chunk_size: Maximum size of each chunk in characters\n",
    "        chunk_overlap: Overlap between consecutive chunks in characters\n",
    "        \n",
    "    Returns:\n",
    "        List[Document]: List of split document chunks\n",
    "    \"\"\"\n",
    "    # Create text splitter configured for Python code\n",
    "    text_splitter = RecursiveCharacterTextSplitter.from_language(\n",
    "        language=Language.PYTHON,\n",
    "        chunk_size=chunk_size,\n",
    "        chunk_overlap=chunk_overlap\n",
    "    )\n",
    "    \n",
    "    # Split documents into chunks\n",
    "    chunks = text_splitter.split_documents(documents)\n",
    "    \n",
    "    print(f\"Created {len(chunks)} chunks from {len(documents)} documents\")\n",
    "    \n",
    "    # Display size of first 5 chunks for verification\n",
    "    for i, chunk in enumerate(chunks[:5]):\n",
    "        print(f\"Chunk {i+1}: {len(chunk.page_content)} characters\")\n",
    "    \n",
    "    return chunks\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    chunks = split_documents(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be6f9657",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "00b5c883",
   "metadata": {},
   "source": [
    "## Embedding MOdel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b8a5794c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_huggingface.embeddings import HuggingFaceEmbeddings\n",
    "\n",
    "def embedding_model(model_name:str=\"sentence-transformers/all-MiniLM-l6-v2\"):\n",
    "    embedding = HuggingFaceEmbeddings(model_name=model_name)\n",
    "\n",
    "    return embedding\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "678f482d",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding = embedding_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91905e80",
   "metadata": {},
   "source": [
    "## FAISS vactor store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fb7612a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created vector store with 181 documents\n",
      "Vector store saved to: faiss_index\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.vectorstores.faiss import FAISS\n",
    "from langchain_core.documents import Document\n",
    "from typing import List, Optional\n",
    "import os\n",
    "\n",
    "class VectorStore:\n",
    "    def __init__(self, index_name: str = \"faiss_index\"):\n",
    "        \"\"\"\n",
    "        Initialize Vector Store for FAISS.\n",
    "        \n",
    "        Args:\n",
    "            index_name: Name for the index directory\n",
    "        \"\"\"\n",
    "        self.index_name = index_name\n",
    "    \n",
    "    def create_vector_store(self, document_chunks: List[Document], embedding_model):\n",
    "        \"\"\"\n",
    "        Create a FAISS vector store from document chunks.\n",
    "        \n",
    "        Args:\n",
    "            document_chunks: List of document chunks to embed\n",
    "            embedding_model: Embedding model instance\n",
    "            \n",
    "        Returns:\n",
    "            FAISS: Vector store instance\n",
    "        \"\"\"\n",
    "        # Validate input\n",
    "        if not document_chunks:\n",
    "            raise ValueError(\"Document chunks list cannot be empty\")\n",
    "        \n",
    "        if len(document_chunks) == 0:\n",
    "            raise ValueError(\"No document chunks provided\")\n",
    "        \n",
    "        # Create vector store\n",
    "        vector_store = FAISS.from_documents(\n",
    "            documents=document_chunks,\n",
    "            embedding=embedding_model\n",
    "        )\n",
    "        \n",
    "        print(f\"Created vector store with {len(document_chunks)} documents\")\n",
    "        return vector_store\n",
    "    \n",
    "    def save_vector_store(self, vector_store, custom_path: Optional[str] = None):\n",
    "        \"\"\"\n",
    "        Save vector store to disk.\n",
    "        \n",
    "        Args:\n",
    "            vector_store: FAISS vector store instance\n",
    "            custom_path: Custom path to save index (optional)\n",
    "            \n",
    "        Returns:\n",
    "            str: Path where index was saved\n",
    "        \"\"\"\n",
    "        save_path = custom_path or self.index_name\n",
    "        \n",
    "        # Ensure directory exists\n",
    "        os.makedirs(os.path.dirname(save_path) if os.path.dirname(save_path) else \".\", exist_ok=True)\n",
    "        \n",
    "        # Save the vector store\n",
    "        vector_store.save_local(save_path)\n",
    "        print(f\"Vector store saved to: {save_path}\")\n",
    "        return save_path\n",
    "    \n",
    "    def load_vector_store(self, embedding_model, custom_path: Optional[str] = None):\n",
    "        \"\"\"\n",
    "        Load vector store from disk.\n",
    "        \n",
    "        Args:\n",
    "            embedding_model: Same embedding model used during creation\n",
    "            custom_path: Custom path to load index from (optional)\n",
    "            \n",
    "        Returns:\n",
    "            FAISS: Loaded vector store instance\n",
    "        \"\"\"\n",
    "        load_path = custom_path or self.index_name\n",
    "        \n",
    "        if not os.path.exists(load_path):\n",
    "            raise FileNotFoundError(f\"Vector store not found at: {load_path}\")\n",
    "        \n",
    "        # Load the vector store\n",
    "        vector_store = FAISS.load_local(\n",
    "            folder_path=load_path,\n",
    "            embeddings=embedding_model,\n",
    "            allow_dangerous_deserialization=True  # Required for FAISS\n",
    "        )\n",
    "        print(f\"Vector store loaded from: {load_path}\")\n",
    "        return vector_store\n",
    "    \n",
    "    def search_similar(self, vector_store, query: str, k: int = 4):\n",
    "        \"\"\"\n",
    "        Search for similar documents.\n",
    "        \n",
    "        Args:\n",
    "            vector_store: FAISS vector store instance\n",
    "            query: Search query\n",
    "            k: Number of results to return\n",
    "            \n",
    "        Returns:\n",
    "            List[Document]: Similar documents\n",
    "        \"\"\"\n",
    "        results = vector_store.similarity_search(query=query, k=k)\n",
    "        return results\n",
    "    \n",
    "if __name__==\"__main__\":\n",
    "    store = VectorStore()\n",
    "    vactor_store = store.create_vector_store(chunks,embedding)\n",
    "    save_path = store.save_vector_store(vector_store=vactor_store)\n",
    "    # load_vector_store = store.load_vector_store(embedding_model=embedding)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c9cfef67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector store loaded from: faiss_index\n"
     ]
    }
   ],
   "source": [
    "load_vector_store = store.load_vector_store(embedding_model=embedding)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "017762ac",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c2801781",
   "metadata": {},
   "source": [
    "## LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0d013336",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_groq import ChatGroq\n",
    "\n",
    "def llm_load(api_key:str,model:str=\"llama-3.1-8b-instant\"):\n",
    "\n",
    "    llm = ChatGroq(\n",
    "    model=\"llama-3.1-8b-instant\",\n",
    "    api_key=api_key,\n",
    "    max_tokens=500\n",
    "    )\n",
    "    return llm\n",
    "    \n",
    "\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv()\n",
    "groq_api_key = os.getenv('GROQ_API_KEY')\n",
    "\n",
    "llm = llm_load(api_key=groq_api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "60489e61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created conversational chain with retriever\n",
      "Search type: MMR, k: 5, fetch_k: 50\n"
     ]
    }
   ],
   "source": [
    "from langchain_classic.memory import ConversationSummaryMemory\n",
    "from langchain_classic.chains import ConversationalRetrievalChain\n",
    "from langchain_core.vectorstores import VectorStore\n",
    "from langchain_classic.schema import BaseRetriever\n",
    "\n",
    "\n",
    "class GenerateRetriever:\n",
    "    def __init__(self, vector_store: VectorStore, memory_key: str = 'chat_history'):\n",
    "        \"\"\"\n",
    "        Initialize a conversational retriever with memory.\n",
    "        \n",
    "        Args:\n",
    "            vector_store: Vector store containing embedded documents\n",
    "            memory_key: Key for storing chat history in memory\n",
    "        \"\"\"\n",
    "        self.vector_store = vector_store\n",
    "        self.memory_key = memory_key  \n",
    "    \n",
    "    def create_memory(self, llm) -> ConversationSummaryMemory:\n",
    "        \"\"\"\n",
    "        Create conversation summary memory.\n",
    "        \n",
    "        Args:\n",
    "            llm: Language model instance\n",
    "            \n",
    "        Returns:\n",
    "            ConversationSummaryMemory: Configured memory object\n",
    "        \"\"\"\n",
    "        memory = ConversationSummaryMemory(\n",
    "            llm=llm,\n",
    "            memory_key=self.memory_key,\n",
    "            return_messages=True,output_key=\"answer\"\n",
    "        )\n",
    "        return memory\n",
    "    \n",
    "    def create_conversational_chain(self, llm, memory: ConversationSummaryMemory) -> ConversationalRetrievalChain:\n",
    "        \"\"\"\n",
    "        Create conversational retrieval chain.\n",
    "        \n",
    "        Args:\n",
    "            llm: Language model instance\n",
    "            memory: Conversation memory object\n",
    "            \n",
    "        Returns:\n",
    "            ConversationalRetrievalChain: Configured retrieval chain\n",
    "        \"\"\"\n",
    "        # Create retriever from vector store\n",
    "        retriever = self.vector_store.as_retriever(\n",
    "            search_type=\"mmr\",\n",
    "            search_kwargs={\n",
    "                \"k\": 3,           # Number of documents to return\n",
    "                \"fetch_k\": 50,    # Number of documents to fetch for MMR\n",
    "                \"score_threshold\": 0.5  # Optional: minimum similarity score\n",
    "            }\n",
    "        )\n",
    "        \n",
    "        # Create conversational retrieval chain\n",
    "        conversational_chain = ConversationalRetrievalChain.from_llm(\n",
    "            llm=llm,\n",
    "            retriever=retriever,\n",
    "            memory=memory,\n",
    "            #verbose=True,  # Shows what's happening\n",
    "            chain_type=\"stuff\",  # Options: \"stuff\", \"map_reduce\", \"refine\", \"map_rerank\"\n",
    "            return_source_documents=True  # Return source documents for citation\n",
    "            \n",
    "        )\n",
    "        \n",
    "        print(f\"Created conversational chain with retriever\")\n",
    "        print(f\"Search type: MMR, k: 5, fetch_k: 50\")\n",
    "        \n",
    "        return conversational_chain\n",
    "    \n",
    "    def simple_retriever(self, search_type: str = \"similarity\", k: int = 4) -> BaseRetriever:\n",
    "        \"\"\"\n",
    "        Create a simple retriever without conversation chain.\n",
    "        \n",
    "        Args:\n",
    "            search_type: Type of search (\"similarity\", \"mmr\", \"similarity_score_threshold\")\n",
    "            k: Number of documents to retrieve\n",
    "            \n",
    "        Returns:\n",
    "            BaseRetriever: Configured retriever\n",
    "        \"\"\"\n",
    "        search_kwargs = {\"k\": k}\n",
    "        \n",
    "        # Add additional parameters based on search type\n",
    "        if search_type == \"similarity_score_threshold\":\n",
    "            search_kwargs[\"score_threshold\"] = 0.7\n",
    "        \n",
    "        retriever = self.vector_store.as_retriever(\n",
    "            search_type=search_type,\n",
    "            search_kwargs=search_kwargs\n",
    "        )\n",
    "        \n",
    "        return retriever\n",
    "\n",
    "\n",
    "if __name__=='__main__':\n",
    "    gen_retriver = GenerateRetriever(load_vector_store)\n",
    "    memory = gen_retriver.create_memory(llm=llm)\n",
    "    chain = gen_retriver.create_conversational_chain(llm=llm,memory=memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b1255cc2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ConversationalRetrievalChain(memory=ConversationSummaryMemory(llm=ChatGroq(client=<groq.resources.chat.completions.Completions object at 0x744d2d9e1670>, async_client=<groq.resources.chat.completions.AsyncCompletions object at 0x744d2d9e1100>, model_name='llama-3.1-8b-instant', model_kwargs={}, groq_api_key=SecretStr('**********'), max_tokens=500), chat_memory=InMemoryChatMessageHistory(messages=[]), output_key='answer', return_messages=True, memory_key='chat_history'), verbose=False, combine_docs_chain=StuffDocumentsChain(verbose=False, llm_chain=LLMChain(verbose=False, prompt=ChatPromptTemplate(input_variables=['context', 'question'], input_types={}, partial_variables={}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context'], input_types={}, partial_variables={}, template=\"Use the following pieces of context to answer the user's question.\\nIf you don't know the answer, just say that you don't know, don't try to make up an answer.\\n----------------\\n{context}\"), additional_kwargs={}), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['question'], input_types={}, partial_variables={}, template='{question}'), additional_kwargs={})]), llm=ChatGroq(client=<groq.resources.chat.completions.Completions object at 0x744d2d9e1670>, async_client=<groq.resources.chat.completions.AsyncCompletions object at 0x744d2d9e1100>, model_name='llama-3.1-8b-instant', model_kwargs={}, groq_api_key=SecretStr('**********'), max_tokens=500), output_parser=StrOutputParser(), llm_kwargs={}), document_prompt=PromptTemplate(input_variables=['page_content'], input_types={}, partial_variables={}, template='{page_content}'), document_variable_name='context'), question_generator=LLMChain(verbose=False, prompt=PromptTemplate(input_variables=['chat_history', 'question'], input_types={}, partial_variables={}, template='Given the following conversation and a follow up question, rephrase the follow up question to be a standalone question, in its original language.\\n\\nChat History:\\n{chat_history}\\nFollow Up Input: {question}\\nStandalone question:'), llm=ChatGroq(client=<groq.resources.chat.completions.Completions object at 0x744d2d9e1670>, async_client=<groq.resources.chat.completions.AsyncCompletions object at 0x744d2d9e1100>, model_name='llama-3.1-8b-instant', model_kwargs={}, groq_api_key=SecretStr('**********'), max_tokens=500), output_parser=StrOutputParser(), llm_kwargs={}), return_source_documents=True, retriever=VectorStoreRetriever(tags=['FAISS', 'HuggingFaceEmbeddings'], vectorstore=<langchain_community.vectorstores.faiss.FAISS object at 0x744d2f218830>, search_type='mmr', search_kwargs={'k': 3, 'fetch_k': 50, 'score_threshold': 0.5}))"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a8224db",
   "metadata": {},
   "source": [
    "System: Use the following pieces of context to answer the user's question.\n",
    "If you don't know the answer, just say that you don't know, don't try to make up an answer.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "52adfbf4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'question': 'whats get_feature_extract_data',\n",
       " 'chat_history': [SystemMessage(content='', additional_kwargs={}, response_metadata={})],\n",
       " 'answer': \"`get_feature_extract_data` is a method of a class (not explicitly shown in the provided context) that appears to be responsible for extracting data from a MongoDB database.\\n\\nHere's a step-by-step breakdown of what the method does:\\n\\n1. It logs an informational message indicating that data extraction from MongoDB is starting.\\n2. It creates an instance of the `NetworkData` class, which is not shown in the provided context.\\n3. It calls the `get_dataframe` method of the `NetworkData` instance, passing the `collection_name` from `self.ingestion_config.data_ingestion_collection_path` as an argument. This method likely retrieves data from the specified MongoDB collection and returns it as a Pandas DataFrame.\\n4. The method does not immediately return the DataFrame, but instead sets up a variable named `feature_data_path` that is assigned the value of `self.ingestion_config.data_ingestion_feature_path`. This suggests that the method might be related to feature extraction or processing, but the exact purpose is not clear without more context. \\n\\nIn summary, `get_feature_extract_data` is a method that extracts data from a MongoDB database and sets up variables for potential feature extraction or processing.\",\n",
       " 'source_documents': [Document(id='7422cecd-ff9a-41bd-9b76-2b1d482ca65c', metadata={'source': 'Input_Repo/Network-Security/Network_Security/components/data_ingestion.py', 'language': <Language.PYTHON: 'python'>}, page_content='def get_feature_extract_data(self):\\n        try:\\n            logging.info(\"Extracting data from MongoDB...\")\\n            networkdata = NetworkData()\\n            \\n            dataframe = networkdata.get_dataframe(\\n                collection_name=self.ingestion_config.data_ingestion_collection_path\\n            )\\n            # start feature_store\\n            feature_data_path = self.ingestion_config.data_ingestion_feature_path'),\n",
       "  Document(id='abc94188-0150-4585-9aa8-26d0a68d99c3', metadata={'source': 'Input_Repo/Network-Security/Network_Security/entity/artifact.py', 'language': <Language.PYTHON: 'python'>}, page_content='class Metrics_Artifact:\\n    f1_score:float \\n    accuracy_score:float\\n    recall_score:float \\n    precision_score:float\\n\\n@dataclass \\nclass Model_Trainer_Artifact:\\n    model_pkl:str \\n    metrics : Metrics_Artifact \\n\\n@dataclass \\nclass Model_Evalution_Artifact:\\n    is_model_accepted:bool \\n    changed_accuracy:float\\n    train_model_path:str \\n    s3_model_path:str'),\n",
       "  Document(id='89020947-8ab3-4d67-802b-9e212e920bee', metadata={'source': 'Input_Repo/Network-Security/Network_Security/components/data_validation.py', 'language': <Language.PYTHON: 'python'>}, page_content='from evidently import Report\\nfrom evidently.presets import DataDriftPreset\\nimport pandas as pd\\nimport json\\nimport sys')]}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question = 'whats get_feature_extract_data'\n",
    "result = chain(question)\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "20f9c664",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"`get_feature_extract_data` is a method of a class (not explicitly shown in the provided context) that appears to be responsible for extracting data from a MongoDB database.\\n\\nHere's a step-by-step breakdown of what the method does:\\n\\n1. It logs an informational message indicating that data extraction from MongoDB is starting.\\n2. It creates an instance of the `NetworkData` class, which is not shown in the provided context.\\n3. It calls the `get_dataframe` method of the `NetworkData` instance, passing the `collection_name` from `self.ingestion_config.data_ingestion_collection_path` as an argument. This method likely retrieves data from the specified MongoDB collection and returns it as a Pandas DataFrame.\\n4. The method does not immediately return the DataFrame, but instead sets up a variable named `feature_data_path` that is assigned the value of `self.ingestion_config.data_ingestion_feature_path`. This suggests that the method might be related to feature extraction or processing, but the exact purpose is not clear without more context. \\n\\nIn summary, `get_feature_extract_data` is a method that extracts data from a MongoDB database and sets up variables for potential feature extraction or processing.\""
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result['answer']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "452fdf28",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Storing feature data in an S3 bucket serves several purposes:\\n\\n1. **Data Versioning**: S3 allows you to store multiple versions of your data, making it easier to track changes and maintain a record of your data\\'s evolution over time.\\n\\n2. **Data Sharing**: S3 buckets can be shared with others, facilitating collaboration and data exchange between teams or organizations.\\n\\n3. **Data Retrieval and Analysis**: With S3, you can easily access and analyze your feature data using various tools and libraries, such as those provided by AWS or third-party vendors.\\n\\n4. **Scalability and Cost-Effectiveness**: S3 offers a scalable and cost-effective solution for storing large amounts of data, as you only pay for the storage you use.\\n\\n5. **Data Backup and Disaster Recovery**: By storing feature data in S3, you can easily create backups and ensure business continuity in case of data loss or system failures.\\n\\n6. **Machine Learning and Model Training**: Feature data stored in S3 can be used to train machine learning models, enabling data-driven decision-making and predictive analytics.\\n\\nIn the context of the provided code snippet, storing feature data in an S3 bucket seems to be used for data retrieval and analysis purposes, as indicated by the `get_file_object` method and the use of the `evidently` library for data drift analysis.\\n\\nHere is an example of how you might use the `s3_key_path_available` method to verify the existence of a feature data file in an S3 bucket before retrieving it:\\n```python\\nif self.s3_key_path_available(\\'my-bucket\\', \\'features/data.csv\\'):\\n    features_data = self.get_file_object(\\'my-bucket\\', \\'features/data.csv\\')\\n    # Analyze or process the feature data\\nelse:\\n    print(\"Feature data file not found in S3 bucket.\")\\n```'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = 'NetworkSecurity_Features'\n",
    "result = chain(query)\n",
    "result['answer']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "dd68dc4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def question(question:str):\n",
    "    result = chain(question)\n",
    "    answer = result['answer']\n",
    "    return answer\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "chatapp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
