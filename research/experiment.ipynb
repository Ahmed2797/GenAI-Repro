{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4e18654c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ahmed\n"
     ]
    }
   ],
   "source": [
    "print(\"ahmed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c345befa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ahmed/miniconda3/envs/chatapp/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from git import Repo\n",
    "\n",
    "from langchain_classic.text_splitter import Language\n",
    "from langchain_classic.document_loaders.generic import GenericLoader\n",
    "from langchain_classic.document_loaders.parsers import LanguageParser\n",
    "from langchain_classic.vectorstores.faiss import FAISS\n",
    "from langchain_classic.memory import ConversationSummaryMemory\n",
    "from langchain_classic.chains import ConversationalRetrievalChain\n",
    "\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_huggingface.embeddings import HuggingFaceEmbeddings\n",
    "from langchain_groq import ChatGroq\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/ahmed/project/GenAI-Repro/research'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55010723",
   "metadata": {},
   "source": [
    "## Repo loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! mkdir input_repo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<git.repo.base.Repo '/home/ahmed/project/GenAI-Repro/research/input_repo/.git'>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "repo_path = \"input_repo/\"\n",
    "\n",
    "Repo.clone_from(url='https://github.com/Ahmed2797/Network-Security.git',to_path=repo_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11281071",
   "metadata": {},
   "source": [
    "## Document loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cd3522cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'input_repo/Network_Security'\n",
    "\n",
    "loader = GenericLoader.from_filesystem(path=path,\n",
    "                                       glob=\"**/*\",\n",
    "                                       suffixes=['.py'],\n",
    "                                       parser=LanguageParser(language=Language.PYTHON,\n",
    "                                                             parser_threshold=100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9b5f1744",
   "metadata": {},
   "outputs": [],
   "source": [
    "documnets = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'input_repo/Network_Security/__init__.py', 'language': <Language.PYTHON: 'python'>}, page_content=''),\n",
       " Document(metadata={'source': 'input_repo/Network_Security/data_acess/networkdata_acess.py', 'language': <Language.PYTHON: 'python'>}, page_content='from Network_Security.exception.exception import NetworkSecurityException\\nfrom Network_Security.logging.logger import logging\\nfrom Network_Security.configeration.mongodb import MongoDBClient \\nfrom typing import Optional\\nimport pandas as pd\\nimport numpy as np \\nimport sys\\n\\n\\nclass NetworkData:\\n    def __init__(self):\\n        try:\\n            self.mongo_client = MongoDBClient()   \\n        except Exception as e:\\n            raise NetworkSecurityException(e, sys)\\n\\n    def get_dataframe(self, collection_name: str, database_name: Optional[str] = None)->pd.DataFrame:\\n        try:\\n            if database_name:\\n                collection = self.mongo_client.client[database_name][collection_name]\\n            else:\\n                collection = self.mongo_client.database[collection_name]\\n\\n            df = pd.DataFrame(list(collection.find()))\\n            if \"_id\" in df.columns:\\n                df.drop(columns=[\"_id\"], inplace=True)\\n            df.replace(\"na\", np.nan, inplace=True)\\n\\n            logging.info(\"DataFrame Extract Successful\")\\n            return df\\n\\n        except Exception as e:\\n            raise NetworkSecurityException(e, sys)\\n'),\n",
       " Document(metadata={'source': 'input_repo/Network_Security/data_acess/__init__.py', 'language': <Language.PYTHON: 'python'>}, page_content=''),\n",
       " Document(metadata={'source': 'input_repo/Network_Security/logging/logger.py', 'language': <Language.PYTHON: 'python'>}, page_content='import os \\nimport logging \\nfrom datetime import datetime \\n\\nlog_file = f\\'{datetime.now().strftime(\\'%m_%d_%Y_%H_%M_%S\\')}.log\\'\\nlog_path = os.path.join(os.getcwd(),\\'logs\\',log_file)\\nos.makedirs(log_path,exist_ok=True)\\n\\nlog_file_path = os.path.join(log_path,log_file)\\n\\nlogging.basicConfig(\\n    filename= log_file_path,\\n    level = logging.INFO,\\n    format = \\'[%(asctime)s] %(lineno)s %(name)s - %(levelname)s - %(message)s\\'\\n)\\n\\nlogging.info(\"Logging setup completed successfully!\")'),\n",
       " Document(metadata={'source': 'input_repo/Network_Security/logging/__init__.py', 'language': <Language.PYTHON: 'python'>}, page_content=''),\n",
       " Document(metadata={'source': 'input_repo/Network_Security/constant/__init__.py', 'language': <Language.PYTHON: 'python'>}, page_content=\"import os\\nimport numpy as np \\nfrom datetime import date\\n\\n# MongoDB\\nDATA_BASE_NAME = 'NETWORK_SECURITY'\\nCOLLECTION_NAME = 'NETWORK_DATA'\\nMONGOBD_URL = 'MONGODB_URL'\\n\\n# Artifacts\\nARTIFACTS = 'artifacts'\\nPIPELINE_DIR = 'network'\\n\\n# DATA \\nRAW_DATA = 'raw.csv'\\nTRAIN_DATA = 'train.csv'\\nTEST_DATA = 'test.csv'\\n\\n\\n# Data_ingestion\\nDATA_INGESTION_DIR: str = 'data_ingestion'\\nDATA_INGESTION_COLLECTION_NAME:str = 'NETWORK_DATA'\\nDATA_INGESTION_FEATURE_STORED_DIR:str = 'feature'\\nDATA_INGESTION_INGESTED_DIR:str = 'ingested'\\nDATA_INGESTION_SPLIT_RATIO:float = 0.2 \\n\\n# Data_validation\\nDATA_VALIDATION_DIR:str = 'data_validation'\\nDATA_VALIDATION_REPORT_DIR:str = 'drift_report'\\nDATA_VALIDATION_REPORT_YAML:str = 'report.yaml'\\n\\n# Data_transformation\\nDATA_TRANSFORMATION_DIR:str = 'data_tranasformation'\\nDATA_TRANSFORMATION_TRANSFORM_FILE:str = 'transform'\\nDATA_TRANSFORMATION_TRANSFORM_0BJECT_FILE:str = 'transform_obj'\\nPREPROCESSING_FILE:str = 'preprocessing.pkl'\\nTARGET_COLUMN = 'result'\\nCURRENT_YEAR = date.today().year\\n# KNNImputer\\nDATA_TRANSFORMATION_IMPUTER_PARAMS: dict={ \\n    'missing_values': np.nan,\\n    'n_neighbors':3,\\n    'weights':'uniform'\\n}\\nSCHEMA_FILE_PATH = os.path.join('data_schema','column.yaml')\\n\\n# model_trainer\\nMODEL_TRAINER_DIR:str = 'model_trainer'\\nMODEL_TRAINER_FILE_NAME:str = 'trained_model'\\nMODEL_TRAINER_TRAINED_MODEL_NAME:str = 'model.pkl'\\nMODEL_TRAINER_CONFIG_PARAM_PATH:str = os.path.join('data_schema','best_param.yaml')\\nMODEL_TRAINER_EXCEPTED_RATIO:float = 0.6 \\n\\n#AWS Configeration\\nREGION = 'us-east-1'\\nAWS_ACCESS_KEY = 'AWS_ACCESS_KEY_ID'\\nAWS_SECRET_KEY = 'AWS_SECRET_ACCESS_KEY' \\n\\n# model_evalution \\nMODEL_BUCKET_NAME:str = 'network_security'\\nMODEL_EVALUTION_CHANGED_THRESHOLD:float = 0.8 \\nMODEL_TRAINER_TRAINED_MODEL_NAME:str = 'model.pkl'\\n\\n\\n\\n\"),\n",
       " Document(metadata={'source': 'input_repo/Network_Security/components/data_ingestion.py', 'language': <Language.PYTHON: 'python'>}, page_content='import os\\nimport sys\\nfrom sklearn.model_selection import train_test_split\\nimport pandas as pd\\nfrom Network_Security.exception.exception import NetworkSecurityException\\nfrom Network_Security.logging.logger import logging\\nfrom Network_Security.entity.config import Data_ingestion_Config\\nfrom Network_Security.entity.artifact import Data_Ingestion_Artifact\\nfrom Network_Security.configeration.mongodb import MongoDBClient  \\nfrom Network_Security.data_acess.networkdata_acess import NetworkData \\n\\nclass Data_Ingestion:\\n    def __init__(self, ingestion_config: Data_ingestion_Config):\\n        try:\\n            self.ingestion_config = ingestion_config\\n        except Exception as e:\\n            raise NetworkSecurityException(e, sys)\\n\\n    def get_feature_extract_data(self):\\n        try:\\n            logging.info(\"Extracting data from MongoDB...\")\\n            networkdata = NetworkData()\\n            \\n            dataframe = networkdata.get_dataframe(\\n                collection_name=self.ingestion_config.data_ingestion_collection_path\\n            )\\n            # start feature_store\\n            feature_data_path = self.ingestion_config.data_ingestion_feature_path\\n            os.makedirs(os.path.dirname(feature_data_path), exist_ok=True)\\n            dataframe.to_csv(feature_data_path, index=False, header=True)\\n            logging.info(f\"Data stored at {feature_data_path}\")\\n            return dataframe\\n        except Exception as e:\\n            raise NetworkSecurityException(e, sys)\\n\\n    def split_data(self, dataframe: pd.DataFrame):\\n        try:\\n            train_data, test_data = train_test_split(\\n                dataframe, \\n                test_size=self.ingestion_config.split_ratio\\n            )\\n\\n            train_file_path = self.ingestion_config.train_data_path\\n            os.makedirs(os.path.dirname(train_file_path), exist_ok=True)\\n            train_data.to_csv(train_file_path, index=False, header=True)\\n\\n            test_file_path = self.ingestion_config.test_data_path\\n            os.makedirs(os.path.dirname(test_file_path), exist_ok=True)\\n            test_data.to_csv(test_file_path, index=False, header=True)\\n\\n            logging.info(\"Train & Test datasets saved successfully.\")\\n            return train_data, test_data\\n        except Exception as e:\\n            raise NetworkSecurityException(e, sys)\\n\\n    def init_data_ingestion(self):\\n        try:\\n            dataframe = self.get_feature_extract_data()\\n            print(dataframe.head())\\n            self.split_data(dataframe)\\n\\n            data_ingestion_artifact = Data_Ingestion_Artifact(\\n                train_file_path=self.ingestion_config.train_data_path,\\n                test_file_path=self.ingestion_config.test_data_path\\n            )\\n            logging.info(\"Data Ingestion completed successfully.\")\\n            return data_ingestion_artifact\\n        except Exception as e:\\n            raise NetworkSecurityException(e, sys) \\n        \\n\\n        \\n'),\n",
       " Document(metadata={'source': 'input_repo/Network_Security/components/model_evalution.py', 'language': <Language.PYTHON: 'python'>}, page_content='from Network_Security.entity.artifact import (Data_Ingestion_Artifact,\\n                                              Model_Trainer_Artifact,\\n                                              Model_Evalution_Artifact)\\nfrom Network_Security.entity.config import Model_Evalution_Config\\nfrom Network_Security.entity.s3_estimator import NetworkEstimator\\nfrom Network_Security.constant import TARGET_COLUMN\\nfrom sklearn.metrics import f1_score\\nfrom dataclasses import dataclass\\nimport pandas as pd\\n\\n@dataclass \\nclass ModelEvalutionResponse:\\n    is_model_accept:bool\\n    difference:float \\n    train_model_f1_score:float \\n    s3_model_f1_score:float \\n\\nclass ModelEvalution:\\n    def __init__(self,data_ingestion_artifact:Data_Ingestion_Artifact ,\\n                 model_trainer_artifact:Model_Trainer_Artifact,\\n                 model_evalution_config:Model_Evalution_Config):\\n        self.data_ingestion_artifact = data_ingestion_artifact\\n        self.model_trainer_artifact = model_trainer_artifact\\n        self.model_evalution_config = model_evalution_config\\n\\n    def get_best_model(self):\\n        bucket_name = self.model_evalution_config.bucket_name \\n        model_path = self.model_evalution_config.s3_model_path \\n        network_model = NetworkEstimator(bucket_name,model_path)\\n        if network_model.is_model_present(model_path):\\n            # return network_model.load_model()\\n            return True\\n        return None \\n    def evaluate_model(self):\\n        test_df = pd.read_csv(self.data_ingestion_artifact.test_file_path)\\n        x = test_df.drop([TARGET_COLUMN],axis=1)\\n        y = test_df[TARGET_COLUMN]\\n        y = y.replace(-1,0) \\n        best_model = self.get_best_model()\\n        best_model_score = None\\n         \\n        if best_model is not None:\\n            pred = best_model.predict(x)\\n            best_model_score = f1_score(y,pred)\\n        temp_model_score = 0 if best_model_score is None else best_model_score\\n        train_model_score = self.model_trainer_artifact.metrics.f1_score\\n\\n        model_evalution_response = ModelEvalutionResponse(\\n            is_model_accept=train_model_score > temp_model_score,\\n            difference=train_model_score - temp_model_score,\\n            train_model_f1_score=train_model_score,\\n            s3_model_f1_score=best_model_score\\n        )\\n        return model_evalution_response\\n    \\n    def init_model_evaluation(self):\\n        model_evalution_response = self.evaluate_model()\\n        s3_model = self.model_evalution_config.s3_model_path \\n        model_evalution_artifact = Model_Evalution_Artifact(\\n            is_model_accepted= model_evalution_response.is_model_accept,\\n            changed_accuracy=model_evalution_response.difference,\\n            s3_model_path=s3_model,\\n            train_model_path=self.model_trainer_artifact.model_pkl\\n        )\\n        return model_evalution_artifact\\n\\n'),\n",
       " Document(metadata={'source': 'input_repo/Network_Security/components/model_train.py', 'content_type': 'functions_classes', 'language': <Language.PYTHON: 'python'>}, page_content='class Network_model:\\n    def __init__(self, transform_object: Pipeline, best_model_details: object):\\n        self.transform_object = transform_object\\n        self.best_model_details = best_model_details\\n\\n    def predict(self, dataframe: pd.DataFrame) -> pd.DataFrame:\\n        try:\\n            transformed_features = self.transform_object.transform(dataframe)\\n            predictions = self.best_model_details.predict(transformed_features)\\n\\n            # return pd.DataFrame(predictions, columns=[\\'prediction\\'])\\n            return predictions\\n        except Exception as e:\\n            raise NetworkSecurityException(e,sys)\\n    def __repr__(self):\\n        return f\"{type(self.best_model_details).__name__}()\"\\n\\n    def __str__(self):\\n        return f\"{type(self.best_model_details).__name__}()\"'),\n",
       " Document(metadata={'source': 'input_repo/Network_Security/components/model_train.py', 'content_type': 'functions_classes', 'language': <Language.PYTHON: 'python'>}, page_content='class Model_Train:\\n    def __init__(self, data_transformation_artifact: Data_Transformation_Artifact,\\n                 model_trainer_config: Model_Trainer_Config):\\n        self.data_transformation_artifact = data_transformation_artifact\\n        self.model_trainer_config = model_trainer_config \\n    \\n    def track_mlflow(self,best_model,metrics_artifact):\\n        try:\\n            with mlflow.start_run():\\n                f1 = metrics_artifact.f1_score\\n                precision = metrics_artifact.precision_score\\n                accuracy = metrics_artifact.accuracy_score\\n                recall = metrics_artifact.recall_score\\n\\n                mlflow.log_metric(\\'f1_score\\', f1)\\n                mlflow.log_metric(\\'precision_score\\', precision)\\n                mlflow.log_metric(\\'accuracy_score\\', accuracy)\\n                mlflow.log_metric(\\'recall_score\\', recall)\\n                try:\\n                    mlflow.sklearn.log_model(best_model,\\'model\\')\\n                except Exception as e:\\n                    logging.info(f\"[WARNING] Failed to log model to MLflow/DagsHub: {e}\")\\n        except Exception as e:\\n            raise NetworkSecurityException(e,sys)\\n    \\n    def get_best_model_indentify(self, train_arr: np.array, test_arr: np.array):\\n        try:\\n            model_factory = ModelFactory(self.model_trainer_config.model_trained_config_param_path)\\n        \\n            xtrain, ytrain = train_arr[:, :-1], train_arr[:, -1]\\n            xtest, ytest = test_arr[:, :-1], test_arr[:, -1]\\n\\n            best_model_details = model_factory.get_best_model(\\n                                X=xtrain,y=ytrain,\\n                                base_accuracy=self.model_trainer_config.excepted_ratio)\\n            \\n            best_model = best_model_details.best_model\\n            print(best_model)\\n            pred = best_model.predict(xtest)\\n\\n            acc = accuracy_score(ytest, pred)\\n            f1 = f1_score(ytest, pred)\\n            recall = recall_score(ytest, pred)\\n            precision = precision_score(ytest, pred)\\n            \\n            metrics_artifact = Metrics_Artifact(f1_score=f1,\\n                                                accuracy_score=acc,\\n                                                recall_score=recall,\\n                                                precision_score=precision)\\n            # track_mlflow\\n            self.track_mlflow(best_model,metrics_artifact)\\n            \\n            print(metrics_artifact)\\n            print(best_model_details.best_score)\\n            print(best_model_details.best_parameters)\\n            \\n            return best_model_details, metrics_artifact\\n        except Exception as e:\\n            raise NetworkSecurityException(e,sys)\\n    \\n    def init_best_model(self):\\n        try:\\n            train_arr = load_numpy_array(self.data_transformation_artifact.transform_train_file)\\n            test_arr = load_numpy_array(self.data_transformation_artifact.transform_test_file)\\n\\n            best_model_details, metrics_artifact = self.get_best_model_indentify(train_arr, test_arr)\\n            transform_object = load_object(self.data_transformation_artifact.transform_object)\\n            #print(best_model_details)\\n            save_object(\\'final_model/model.pkl\\', best_model_details)\\n            #print(metrics_artifact)\\n            if best_model_details.best_score < self.model_trainer_config.excepted_ratio:\\n                logging.info(\"Best model not found with expected accuracy.\")\\n\\n            network_model_obj = Network_model(transform_object, best_model_details)\\n            save_object(self.model_trainer_config.model_trained_path, network_model_obj)\\n\\n\\n            model_trainer_artifact = Model_Trainer_Artifact(\\n                model_pkl=self.model_trainer_config.model_trained_path,\\n                metrics=metrics_artifact\\n            )\\n\\n            return model_trainer_artifact\\n        except Exception as e:\\n                raise NetworkSecurityException(e,sys)'),\n",
       " Document(metadata={'source': 'input_repo/Network_Security/components/model_train.py', 'content_type': 'simplified_code', 'language': <Language.PYTHON: 'python'>}, page_content=\"from Network_Security.entity.artifact import (Data_Transformation_Artifact,\\n                                             Metrics_Artifact,\\n                                              Model_Trainer_Artifact)\\nfrom Network_Security.entity.config import Model_Trainer_Config\\nfrom Network_Security.utils import load_numpy_array,load_object,save_object\\nfrom Network_Security.logging.logger import logging\\nfrom Network_Security.exception.exception import NetworkSecurityException\\n\\nfrom sklearn.metrics import accuracy_score,f1_score,precision_score,recall_score\\nfrom sklearn.pipeline import Pipeline\\nfrom neuro_mf import ModelFactory\\nfrom typing import Tuple\\nimport numpy as np \\nimport pandas as pd\\nimport mlflow\\nimport sys\\nimport dagshub\\ndagshub.init(repo_owner='Ahmed2797', repo_name='Network-Security', mlflow=True)\\n\\n\\n# Code for: class Network_model:\\n\\n# Code for: class Model_Train:\\n\\n\"),\n",
       " Document(metadata={'source': 'input_repo/Network_Security/components/data_validation.py', 'content_type': 'functions_classes', 'language': <Language.PYTHON: 'python'>}, page_content='class Data_validation:\\n    def __init__(self, data_ingestion_artifact: Data_Ingestion_Artifact,\\n                data_validation_config: Data_validation_config):\\n        try:\\n            self.data_ingestion_artifact = data_ingestion_artifact\\n            self.data_validation_config = data_validation_config\\n            self._schema_yaml = read_yaml_file(file_path=SCHEMA_FILE_PATH)\\n            if self._schema_yaml is None:\\n                raise ValueError(f\"Schema file not loaded or is empty: {SCHEMA_FILE_PATH}\")\\n        except Exception as e:\\n            raise NetworkSecurityException(e,sys)\\n        \\n    #if number of columns matches schema:\\n    def valid_no_columns(self, dataframe: pd.DataFrame) -> bool:\\n        try:\\n            expected_columns = self._schema_yaml[\\'columns\\']\\n            status = len(dataframe.columns) == len(expected_columns)\\n            return status\\n        except Exception as e:\\n            raise NetworkSecurityException(e,sys)\\n\\n    #if all expected columns exist:\\n    def is_column_exists(self, dataframe: pd.DataFrame) -> bool:\\n        try:\\n            missing_num_columns = [col for col in self._schema_yaml[\\'numeric_columns\\'] if col not in dataframe.columns]\\n            missing_cat_columns = [col for col in self._schema_yaml[\\'categorical_columns\\'] if col not in dataframe.columns]\\n\\n            if missing_num_columns:\\n                logging.info(f\\'Missing numeric columns: {missing_num_columns}\\')\\n            if missing_cat_columns:\\n                logging.info(f\\'Missing categorical columns: {missing_cat_columns}\\')\\n\\n            status = not (len(missing_num_columns) > 0 or len(missing_cat_columns) > 0)\\n            return status\\n        except Exception as e:\\n            raise NetworkSecurityException(e,sys)\\n\\n    def detect_dataset_drift(self, reference_df: pd.DataFrame, current_df: pd.DataFrame) -> bool:\\n        try:\\n            report = Report([DataDriftPreset()],include_tests=\"True\")\\n            report = report.run(reference_data=reference_df, current_data=current_df)\\n            report.save_html(\"data_drift_report.html\")\\n            json_report = report.json()\\n            report_dict = json.loads(json_report)\\n            write_yaml_file(\\n                file_path=self.data_validation_config.data_validation_report,\\n                content=report_dict)\\n            \\n            n_features = sum(1 for m in report_dict[\"metrics\"] if \"ValueDrift\" in m[\"metric_id\"])\\n            drift_metric = next(m for m in report_dict[\"metrics\"] if \"DriftedColumnsCount\" in m[\"metric_id\"])\\n            n_drifted_features = drift_metric[\"value\"][\"count\"]\\n            # Dataset drift status\\n            drift_status = n_drifted_features > 0\\n            print(n_features, n_drifted_features, drift_status)\\n            logging.info(f\"{n_drifted_features}/{n_features} features show drift.\")\\n            return drift_status    \\n        except Exception as e:\\n            logging.info(f\"Error in dataset drift detection: {e}\")\\n            raise NetworkSecurityException (e,sys)\\n  \\n    # Static method to read CSV\\n    @staticmethod\\n    def read_data(file_path: str) -> pd.DataFrame:\\n        return pd.read_csv(file_path)\\n    \\n    def init_data_validation(self) -> Data_validation_Artifact:\\n        try:\\n            valid_message_error = []\\n            # Read train and test data\\n            train_data = self.read_data(self.data_ingestion_artifact.train_file_path)\\n            test_data = self.read_data(self.data_ingestion_artifact.test_file_path)\\n            # train data\\n            if not self.valid_no_columns(train_data):\\n                valid_message_error.append(\\'Error: Column Mismatch in train data\\')\\n            if not self.is_column_exists(train_data):\\n                valid_message_error.append(\\'Error: Missing columns in train data\\')\\n            #test data\\n            if not self.valid_no_columns(test_data):\\n                valid_message_error.append(\\'Error: Column Mismatch in test data\\')\\n            if not self.is_column_exists(test_data):\\n                valid_message_error.append(\\'Error: Missing columns in test data\\')\\n\\n            # Drift detection\\n            validation_status = len(valid_message_error) == 0\\n            if validation_status:\\n                drift_status = self.detect_dataset_drift(train_data, test_data)\\n                if drift_status:\\n                    valid_message_error.append(\\'Drift detected\\')\\n                else:\\n                    valid_message_error.append(\\'Drift not detected\\')\\n            else:\\n                logging.info(f\\'Validation errors: {valid_message_error}\\')\\n\\n            #Create artifact\\n            data_validation_artifact = Data_validation_Artifact(\\n                validation_status=validation_status,\\n                message_error=valid_message_error,\\n                drift_report_file_path=self.data_validation_config.data_validation_report\\n            )\\n            return data_validation_artifact\\n\\n        except Exception as e:\\n            raise NetworkSecurityException(e, sys)'),\n",
       " Document(metadata={'source': 'input_repo/Network_Security/components/data_validation.py', 'content_type': 'simplified_code', 'language': <Language.PYTHON: 'python'>}, page_content='from Network_Security.logging.logger import logging\\nfrom Network_Security.constant import SCHEMA_FILE_PATH\\nfrom Network_Security.utils import read_yaml_file, write_yaml_file\\nfrom Network_Security.entity.artifact import Data_Ingestion_Artifact, Data_validation_Artifact\\nfrom Network_Security.entity.config import Data_validation_config\\nfrom Network_Security.exception.exception import NetworkSecurityException\\nfrom evidently import Report\\nfrom evidently.presets import DataDriftPreset\\nimport pandas as pd\\nimport json\\nimport sys\\n\\n\\n# Code for: class Data_validation:\\n'),\n",
       " Document(metadata={'source': 'input_repo/Network_Security/components/data_transformation.py', 'language': <Language.PYTHON: 'python'>}, page_content='from Network_Security.constant import * \\nfrom Network_Security.exception.exception import NetworkSecurityException\\nfrom Network_Security.logging.logger import logging\\nfrom Network_Security.utils import read_yaml_file, save_object, save_numpy_array\\nfrom Network_Security.entity.artifact import (\\n    Data_Ingestion_Artifact,\\n    Data_validation_Artifact,\\n    Data_Transformation_Artifact\\n)\\nfrom Network_Security.entity.config import Data_Transformation_Config\\n\\nfrom sklearn.pipeline import Pipeline\\nfrom sklearn.impute import KNNImputer\\nimport pandas as pd \\nimport numpy as np\\nimport sys\\n\\n\\nclass DataTransformation:\\n    def __init__(self,\\n                 data_ingestion_artifact: Data_Ingestion_Artifact,\\n                 data_validation_artifact: Data_validation_Artifact,\\n                 data_transformation_config: Data_Transformation_Config):\\n        try:\\n            self.data_ingestion_artifact = data_ingestion_artifact\\n            self.data_validation_artifact = data_validation_artifact\\n            self.data_transformation_config = data_transformation_config\\n            self._schema_config = read_yaml_file(SCHEMA_FILE_PATH)\\n        except Exception as e:\\n            raise NetworkSecurityException(e, sys)\\n    \\n    def get_data_transformation(self) -> Pipeline:\\n        try:\\n            imputer = KNNImputer(**DATA_TRANSFORMATION_IMPUTER_PARAMS)\\n            processor = Pipeline([(\\'imputer\\', imputer)])\\n            return processor\\n        except Exception as e:\\n            raise NetworkSecurityException(e, sys)\\n        \\n    @staticmethod\\n    def read_data(file_path: str) -> pd.DataFrame:\\n        try:\\n            return pd.read_csv(file_path)\\n        except Exception as e:\\n            raise NetworkSecurityException(e, sys)\\n    \\n    def init_data_transformation(self):\\n        try:\\n            logging.info(\"Data read and apply preprocessing & transformation\")\\n            train_df = DataTransformation.read_data(self.data_ingestion_artifact.train_file_path)\\n            test_df = DataTransformation.read_data(self.data_ingestion_artifact.test_file_path)\\n\\n            # Train features & target\\n            input_feature_train = train_df.drop(columns=[TARGET_COLUMN], axis=1)\\n            target_feature_train = train_df[TARGET_COLUMN].replace(-1, 0)\\n\\n            # Test features & target\\n            input_feature_test = test_df.drop(columns=[TARGET_COLUMN], axis=1)\\n            target_feature_test = test_df[TARGET_COLUMN].replace(-1, 0)\\n\\n            # Preprocessor\\n            preprocessor = self.get_data_transformation()\\n            input_feature_train_arr = preprocessor.fit_transform(input_feature_train)\\n            input_feature_test_arr = preprocessor.transform(input_feature_test)\\n\\n            # Combine arrays\\n            train_arr = np.c_[input_feature_train_arr, np.array(target_feature_train)]\\n            test_arr = np.c_[input_feature_test_arr, np.array(target_feature_test)]\\n\\n            # Save transformation pipeline and arrays\\n            save_object(\\'final_model/preprocessor.pkl\\',preprocessor) # use FastAPI\\n            save_object(self.data_transformation_config.data_transformation_object_pkl, obj=preprocessor)\\n            save_numpy_array(self.data_transformation_config.data_transformation_train_file, array=train_arr)\\n            save_numpy_array(self.data_transformation_config.data_transformation_test_file, array=test_arr)\\n            logging.info(\\'Array loaded succesfully\\')\\n\\n            data_transformation_artifact = Data_Transformation_Artifact(\\n                transform_object=self.data_transformation_config.data_transformation_object_pkl,\\n                transform_train_file=self.data_transformation_config.data_transformation_train_file,\\n                transform_test_file=self.data_transformation_config.data_transformation_test_file\\n            )\\n\\n            return data_transformation_artifact\\n\\n        except Exception as e:\\n            raise NetworkSecurityException(e, sys)\\n'),\n",
       " Document(metadata={'source': 'input_repo/Network_Security/components/__init__.py', 'language': <Language.PYTHON: 'python'>}, page_content=''),\n",
       " Document(metadata={'source': 'input_repo/Network_Security/exception/__init__.py', 'language': <Language.PYTHON: 'python'>}, page_content=''),\n",
       " Document(metadata={'source': 'input_repo/Network_Security/exception/exception.py', 'language': <Language.PYTHON: 'python'>}, page_content='import sys \\nfrom Network_Security.logging.logger import logging\\n\\nclass NetworkSecurityException(Exception):   \\n\\n    def __init__(self, error_message, error_details: sys):\\n        self.error_message = error_message \\n        _, _, exc_tb = error_details.exc_info()\\n\\n        self.lineno = exc_tb.tb_lineno \\n        self.file_name = exc_tb.tb_frame.f_code.co_filename \\n    \\n    def __str__(self):\\n        return (\\n            f\"Error occurred in Python script [{self.file_name}] \"\\n            f\"at line number [{self.lineno}] \"\\n            f\"with error: {str(self.error_message)}\"\\n        )\\n    \\n\\nif __name__ == \\'__main__\\':\\n    try:\\n        logging.info(\\'Try the logging&Exception\\')\\n        x = 1 / 0\\n    except Exception as e:\\n        raise NetworkSecurityException(e, sys)\\n\\n\\n        \\n'),\n",
       " Document(metadata={'source': 'input_repo/Network_Security/cloud/aws_service.py', 'language': <Language.PYTHON: 'python'>}, page_content=\"from Network_Security.configeration.aws_connection import S3Client\\n\\n\\nfrom typing import List,Union\\nfrom io import StringIO\\nimport pickle\\nclass SimpleStorageService:\\n    def __init__(self):\\n        s3_client = S3Client()\\n        self.s3_client = s3_client.s3_client\\n        self.resource = s3_client.resource\\n\\n    def s3_key_path_available(self,bucket_name,s3_key)->bool:\\n        bucket = self.resource.Bucket(bucket_name)\\n        file_object = [file_object for file_object in bucket.objects.filter(prefix=s3_key)]\\n        if len(file_object)>0:\\n            return True \\n        else:\\n            return False  \\n    def get_file_object(self,bucket_name,model_path)->Union[List[object],object]:\\n        bucket = self.resource.Bucket(bucket_name)\\n        file_object = [file_object for file_object in bucket.objects.filter(prefix=model_path)]\\n        func = lambda x:x[0] if len(x)==1 else x\\n        file_obj = func(file_object)\\n        return file_obj\\n\\n\\n    @staticmethod\\n    def read_object(file_object,decode: bool=True,model_readable: bool=False):\\n        func = (lambda:file_object.get()['Body'].read().decode()\\n                if decode is True \\n                else file_object.get()['Body'].read())\\n        conv_func = lambda:StringIO(func()) if model_readable is True else func()\\n        return conv_func \\n    \\n    def load_model(self,bucket_name,model_name,model_dir=None):\\n        func = (lambda: model_name\\n                if model_dir is None\\n                else model_dir + '/' + model_name)\\n        model_path = func()\\n        file_object = self.get_file_object(bucket_name=bucket_name,model_path=model_path)\\n        model_object = self.read_object(file_object,decode=False)\\n        model = pickle.load(model_object)\\n        return model\\n\\n\"),\n",
       " Document(metadata={'source': 'input_repo/Network_Security/cloud/__init__.py', 'language': <Language.PYTHON: 'python'>}, page_content='import os\\n\\n\\nclass S3Sync:\\n    def sync_folder_to_s3(self, folder, aws_bucket_url):\\n        command = f\"aws s3 sync {folder} {aws_bucket_url}\"\\n        os.system(command)\\n\\n    def sync_folder_from_s3(self, folder, aws_bucket_url):\\n        command = f\"aws s3 sync {aws_bucket_url} {folder}\"\\n        os.system(command)'),\n",
       " Document(metadata={'source': 'input_repo/Network_Security/utils/__init__.py', 'language': <Language.PYTHON: 'python'>}, page_content='import os\\nimport yaml\\nimport sys\\nimport pickle\\nimport numpy as np\\nfrom Network_Security.exception.exception import NetworkSecurityException\\n\\n# def read_yaml_file(file_path:str)->dict:\\n#     if not os.path.exists(file_path):\\n#         raise FileNotFoundError(f\"File not found: {file_path}\")\\n#     with open(file_path, \\'rb\\') as file:\\n#         yaml.safe_load(file) \\n    \\ndef read_yaml_file(file_path: str) -> dict:\\n    if not os.path.exists(file_path):\\n        raise FileNotFoundError(f\"File not found: {file_path}\")\\n    with open(file_path, \\'r\\') as file:  # text mode is fine for YAML\\n        data = yaml.safe_load(file)\\n    if data is None:\\n        raise ValueError(f\"YAML file is empty: {file_path}\")\\n    return data\\n\\n\\ndef write_yaml_file(file_path: str, content: object, replace: bool = False) -> None:\\n    \\n        if replace:\\n            if os.path.exists(file_path):\\n                os.remove(file_path)   \\n        os.makedirs(os.path.dirname(file_path), exist_ok=True)  \\n        with open(file_path, \"w\") as file:\\n            return yaml.dump(content, file)  \\n    \\ndef save_object(file_path: str, obj: object):\\n    try:\\n        os.makedirs(os.path.dirname(file_path), exist_ok=True)\\n        with open(file_path, \\'wb\\') as file_obj:\\n            pickle.dump(obj, file_obj)  \\n    except Exception as e:\\n        raise NetworkSecurityException(e, sys)\\n     \\n\\ndef save_numpy_array(file_path: str, array: np.array):\\n    try:\\n        os.makedirs(os.path.dirname(file_path), exist_ok=True)\\n        with open(file_path, \\'wb\\') as file_obj:\\n            return np.save(file_obj, array)  \\n    except Exception as e:\\n        raise NetworkSecurityException(e, sys)\\n\\ndef load_numpy_array(file_path:str)->np.array:\\n    try:\\n        os.makedirs(os.path.dirname(file_path),exist_ok=True)\\n        with open(file_path,\\'rb\\') as file_obj:\\n            return np.load(file_obj)\\n    except Exception as e:\\n          raise NetworkSecurityException(e,sys)\\n    \\n\\ndef load_object(file_path: str) -> object:\\n    try:\\n        with open(file_path, \"rb\") as file_obj:\\n            obj = pickle.load(file_obj)\\n        return obj\\n    except Exception as e:\\n        raise NetworkSecurityException(e, sys)'),\n",
       " Document(metadata={'source': 'input_repo/Network_Security/entity/estimator.py', 'language': <Language.PYTHON: 'python'>}, page_content='import sys \\nfrom sklearn.pipeline import Pipeline \\nfrom Network_Security.constant import TARGET_COLUMN\\nfrom Network_Security.exception.exception import NetworkSecurityException \\n\\nimport pandas as pd \\n\\nclass Network_model:\\n    def __init__(self, transform_object: Pipeline, best_model_details: object):\\n        self.transform_object = transform_object\\n        self.best_model_details = best_model_details\\n\\n    def predict(self, dataframe: pd.DataFrame) -> pd.DataFrame:\\n        try:\\n            # x = dataframe.drop(columns=[TARGET_COLUMN], axis=1)\\n            # y = dataframe[TARGET_COLUMN].replace(-1, 0)\\n            transformed_features = self.transform_object.transform(dataframe)\\n            model = getattr(self.best_model_details, \"best_model\", self.best_model_details)\\n            predictions = model.predict(transformed_features)\\n            return predictions\\n            #return pd.DataFrame(predictions, columns=[\\'prediction\\'])\\n        except Exception as e:\\n            raise NetworkSecurityException(e,sys)\\n    def __repr__(self):\\n        return f\"{type(self.best_model_details).__name__}()\"\\n\\n    def __str__(self):\\n        return f\"{type(self.best_model_details).__name__}()\"'),\n",
       " Document(metadata={'source': 'input_repo/Network_Security/entity/__init__.py', 'language': <Language.PYTHON: 'python'>}, page_content=''),\n",
       " Document(metadata={'source': 'input_repo/Network_Security/entity/config.py', 'language': <Language.PYTHON: 'python'>}, page_content=\"from dataclasses import dataclass \\nfrom datetime import datetime\\nfrom Network_Security.constant import *\\nfrom Network_Security.entity.artifact import Metrics_Artifact\\n\\n\\nTIMESTAMP = datetime.now().strftime('%m_%d_%Y_%H_%M_%S')\\n\\n@dataclass \\nclass NS_Train_Configeration:\\n    artifact_dir:str = os.path.join(ARTIFACTS,TIMESTAMP)\\n    pipeline_dir:str = PIPELINE_DIR\\n    TIMESTAMP:str = TIMESTAMP\\n    model_dir:str = os.path.join('final_model')\\n\\ntrain_config = NS_Train_Configeration()\\n\\n\\n@dataclass \\nclass Data_ingestion_Config:\\n    data_ingestion_path = os.path.join(train_config.artifact_dir,DATA_INGESTION_DIR)\\n    data_ingestion_collection_path = DATA_INGESTION_COLLECTION_NAME \\n    data_ingestion_feature_path = os.path.join(data_ingestion_path,DATA_INGESTION_FEATURE_STORED_DIR,RAW_DATA)\\n    train_data_path = os.path.join(data_ingestion_path,DATA_INGESTION_INGESTED_DIR,TRAIN_DATA)\\n    test_data_path = os.path.join(data_ingestion_path,DATA_INGESTION_INGESTED_DIR,TEST_DATA)\\n    split_ratio = DATA_INGESTION_SPLIT_RATIO \\n\\n@dataclass\\nclass Data_validation_config:\\n    data_validation_dir = os.path.join(train_config.artifact_dir,DATA_VALIDATION_DIR)\\n    data_validation_report = os.path.join(data_validation_dir,DATA_VALIDATION_REPORT_DIR,DATA_VALIDATION_REPORT_YAML)\\n\\n@dataclass \\nclass Data_Transformation_Config:\\n    data_transformation_dir = os.path.join(train_config.artifact_dir,DATA_TRANSFORMATION_DIR)\\n    data_transformation_train_file = os.path.join(data_transformation_dir,DATA_TRANSFORMATION_TRANSFORM_FILE,TRAIN_DATA.replace('csv','npy'))\\n    data_transformation_test_file = os.path.join(data_transformation_dir,DATA_TRANSFORMATION_TRANSFORM_FILE,TEST_DATA.replace('csv','npy'))\\n    data_transformation_object_pkl = os.path.join(data_transformation_dir,DATA_TRANSFORMATION_TRANSFORM_0BJECT_FILE,PREPROCESSING_FILE)\\n\\n@dataclass \\nclass Model_Trainer_Config:\\n    model_trainer_dir = os.path.join(train_config.artifact_dir, MODEL_TRAINER_DIR)\\n    model_trained_path = os.path.join(model_trainer_dir, MODEL_TRAINER_FILE_NAME, MODEL_TRAINER_TRAINED_MODEL_NAME)\\n    model_trained_config_param_path = MODEL_TRAINER_CONFIG_PARAM_PATH\\n    excepted_ratio = MODEL_TRAINER_EXCEPTED_RATIO\\n\\n@dataclass \\nclass Model_Evalution_Config:\\n    bucket_name:str = MODEL_BUCKET_NAME \\n    s3_model_path:str = MODEL_TRAINER_TRAINED_MODEL_NAME\\n    changed_model_score:float =  MODEL_EVALUTION_CHANGED_THRESHOLD\\n\\n\\n\"),\n",
       " Document(metadata={'source': 'input_repo/Network_Security/entity/s3_estimator.py', 'language': <Language.PYTHON: 'python'>}, page_content='from Network_Security.cloud.aws_service import SimpleStorageService\\n#from Network_Security.components.model_train import Network_model\\nfrom Network_Security.entity.estimator import Network_model\\n\\nclass NetworkEstimator:\\n    def __init__(self,bucket_name,model_path):\\n        self.model_path = model_path \\n        self.bucket_name = bucket_name\\n        self.s3 = SimpleStorageService() \\n        self.loaded_model:Network_model=None\\n\\n    def is_model_present(self,model_path):\\n     return self.s3.s3_key_path_available(bucket_name=self.bucket_name,\\n                                          model_path=model_path)\\n    def load_model(self)->Network_model:\\n        model_pkl = self.s3.load_model(bucket_name=self.bucket_name,\\n                                  model_name=self.model_path)\\n        return model_pkl'),\n",
       " Document(metadata={'source': 'input_repo/Network_Security/entity/artifact.py', 'language': <Language.PYTHON: 'python'>}, page_content='from dataclasses import dataclass \\n\\n@dataclass \\nclass Data_Ingestion_Artifact:\\n    train_file_path:str\\n    test_file_path:str \\n\\n@dataclass \\nclass Data_validation_Artifact:\\n    validation_status:bool \\n    message_error:str \\n    drift_report_file_path:str\\n\\n@dataclass \\nclass Data_Transformation_Artifact:\\n    transform_object:str\\n    transform_train_file:str \\n    transform_test_file:str \\n\\n@dataclass \\nclass Metrics_Artifact:\\n    f1_score:float \\n    accuracy_score:float\\n    recall_score:float \\n    precision_score:float\\n\\n@dataclass \\nclass Model_Trainer_Artifact:\\n    model_pkl:str \\n    metrics : Metrics_Artifact \\n\\n@dataclass \\nclass Model_Evalution_Artifact:\\n    is_model_accepted:bool \\n    changed_accuracy:float\\n    train_model_path:str \\n    s3_model_path:str'),\n",
       " Document(metadata={'source': 'input_repo/Network_Security/pipeline/prediction_pipe.py', 'language': <Language.PYTHON: 'python'>}, page_content='from dataclasses import dataclass,asdict\\nimport pandas as pd\\n\\n@dataclass\\nclass NetworkSecurity_Features:\\n    having_ip_address: int\\n    url_length: int\\n    shortining_service: int\\n    having_at_symbol: int\\n    double_slash_redirecting: int\\n    prefix_suffix: int\\n    having_sub_domain: int\\n    sslfinal_state: int\\n    domain_registration_length: int\\n    favicon: int\\n    port: int\\n    https_token: int\\n    request_url: int\\n    url_of_anchor: int\\n    links_in_tags: int\\n    sfh: int\\n    submitting_to_email: int\\n    abnormal_url: int\\n    redirect: int\\n    on_mouseover: int\\n    rightclick: int\\n    popupwindow: int\\n    iframe: int\\n    age_of_domain: int\\n    dnsrecord: int\\n    web_traffic: int\\n    page_rank: int\\n    google_index: int\\n    links_pointing_to_page: int\\n    statistical_report: int\\n\\n    # Convert to dict\\n    def dict_data(self):\\n        return asdict(self) \\n    # dict-->>--DataFrame\\n    def dict_data_to_dataframe(self):\\n        data = pd.DataFrame(self.dict_data())\\n        return data'),\n",
       " Document(metadata={'source': 'input_repo/Network_Security/pipeline/__init__.py', 'language': <Language.PYTHON: 'python'>}, page_content=''),\n",
       " Document(metadata={'source': 'input_repo/Network_Security/pipeline/train_pipeline.py', 'content_type': 'functions_classes', 'language': <Language.PYTHON: 'python'>}, page_content='class Training_Pipeline:\\n    def __init__(self):\\n        self.data_ingestion_config = Data_ingestion_Config()\\n        self.data_validation_config = Data_validation_config()\\n        self.data_transformation_config = Data_Transformation_Config()\\n        self.model_trainer_config = Model_Trainer_Config()\\n        self.model_evalution_config  = Model_Evalution_Config()\\n\\n        self.s3_sync = S3Sync()\\n        self.ns_train_config = NS_Train_Configeration()\\n\\n\\n    def start_data_ingestion(self)->Data_Ingestion_Artifact:\\n        try:\\n            data_ingestion = Data_Ingestion(ingestion_config=self.data_ingestion_config)\\n            data_ingestion_artifact = data_ingestion.init_data_ingestion()\\n            return data_ingestion_artifact \\n        except Exception as e:\\n            raise NetworkSecurityException(e,sys)\\n    \\n    def start_data_validation(self, data_ingestion_artifact: Data_Ingestion_Artifact) -> Data_validation_Artifact:\\n        try:\\n            data_valid = Data_validation(data_ingestion_artifact=data_ingestion_artifact,\\n                                        data_validation_config=self.data_validation_config)\\n            data_validation_artifact = data_valid.init_data_validation()\\n            return data_validation_artifact\\n        except Exception as e:\\n            raise NetworkSecurityException(e,sys)\\n\\n\\n    def start_data_transformation(self,data_ingestion_artifact: Data_Ingestion_Artifact,\\n                                  data_validation_artifact:Data_validation_Artifact)->Data_Transformation_Artifact:\\n        try:\\n            data_transformation = DataTransformation(data_ingestion_artifact=data_ingestion_artifact,\\n                                                data_validation_artifact=data_validation_artifact,\\n                                                data_transformation_config=self.data_transformation_config)\\n            data_transformation_artifact = data_transformation.init_data_transformation()\\n            return data_transformation_artifact\\n        except Exception as e:\\n            raise NetworkSecurityException(e,sys)\\n        \\n    def strat_model_trainer(self,data_transformation_artifact:Data_Transformation_Artifact)->Model_Trainer_Artifact:\\n        try:\\n            model_train = Model_Train(data_transformation_artifact=data_transformation_artifact,\\n                                    model_trainer_config=self.model_trainer_config)\\n            model_trainer_artifact=model_train.init_best_model()\\n            return model_trainer_artifact\\n        except Exception as e:\\n            raise NetworkSecurityException(e,sys)\\n        \\n    \\n# K ---------------------------------------------------------------->    \\n    # artifact --> S3\\n    def sync_artifact_dir_to_s3(self):\\n        try:\\n            aws_bucket_url = f\"s3://{MODEL_BUCKET_NAME}/artifact/{self.ns_train_config.TIMESTAMP}\"\\n            self.s3_sync.sync_folder_to_s3(\\n                folder=self.ns_train_config.artifact_dir,\\n                aws_bucket_url=aws_bucket_url)\\n        except Exception as e:\\n            raise NetworkSecurityException(e,sys)\\n    \\n    # final_model --> S3\\n    def sync_saved_model_dir_to_s3(self):\\n        try:\\n            aws_bucket_url = f\"s3://{MODEL_BUCKET_NAME}/final_model/{self.ns_train_config.TIMESTAMP}\"\\n            self.s3_sync.sync_folder_to_s3(\\n                folder=self.ns_train_config.model_dir,\\n                aws_bucket_url=aws_bucket_url\\n            )\\n        except Exception as e:\\n            raise NetworkSecurityException(e,sys)\\n# K----------------------------------------------------------------->\\n        \\n\\n\\n\\n    # B -->\\n    def start_model_evalution(self,data_ingestion_artifact:Data_Ingestion_Artifact,\\n                              model_trainer_artifact:Model_Trainer_Artifact)->Model_Evalution_Artifact:\\n        try:\\n            model_evaluate = ModelEvalution(data_ingestion_artifact = data_ingestion_artifact, \\n                                            model_trainer_artifact = model_trainer_artifact, \\n                                            model_evalution_config = self.model_evalution_config)\\n            model_evalution_artifact = model_evaluate.init_model_evaluation()\\n            return model_evalution_artifact\\n        except Exception as e:\\n            raise NetworkSecurityException(e,sys)\\n\\n    def run_pipeline(self)->None:\\n        try:\\n            data_ingestion_artifact = self.start_data_ingestion()\\n            data_validation_artifact=self.start_data_validation(data_ingestion_artifact)\\n            data_transformation_artifact = self.start_data_transformation(data_ingestion_artifact,data_validation_artifact)\\n            model_trainer_artifact = self.strat_model_trainer(data_transformation_artifact)\\n\\n\\n            # --------------------------------->\\n            # self.sync_artifact_dir_to_s3()\\n            # self.sync_saved_model_dir_to_s3()\\n            # --------------------------------->\\n\\n\\n            # model_evalution_artifact = self.start_model_evalution(model_trainer_artifact)\\n            # return model_evalution_artifact\\n        except Exception as e:\\n            raise NetworkSecurityException(e,sys)\\n\\n        return None'),\n",
       " Document(metadata={'source': 'input_repo/Network_Security/pipeline/train_pipeline.py', 'content_type': 'simplified_code', 'language': <Language.PYTHON: 'python'>}, page_content='from Network_Security.components.data_ingestion import Data_Ingestion\\nfrom Network_Security.components.data_validation import Data_validation\\nfrom Network_Security.components.data_transformation import DataTransformation\\nfrom Network_Security.components.model_train import Model_Train\\nfrom Network_Security.components.model_evalution import ModelEvalution\\nfrom Network_Security.constant import MODEL_BUCKET_NAME\\n\\nfrom Network_Security.entity.config import (Data_ingestion_Config,\\n                                            Data_validation_config,\\n                                            Data_Transformation_Config,\\n                                            Model_Trainer_Config,\\n                                            Model_Evalution_Config) \\n\\nfrom Network_Security.entity.artifact import (Data_Ingestion_Artifact,\\n                                              Data_validation_Artifact,\\n                                              Data_Transformation_Artifact,\\n                                              Model_Trainer_Artifact,\\n                                              Model_Evalution_Artifact)\\nfrom Network_Security.cloud import S3Sync\\nfrom Network_Security.exception.exception import NetworkSecurityException\\nfrom Network_Security.entity.config import NS_Train_Configeration\\nimport sys\\n\\n\\n# Code for: class Training_Pipeline:'),\n",
       " Document(metadata={'source': 'input_repo/Network_Security/research/__init__.py', 'language': <Language.PYTHON: 'python'>}, page_content=''),\n",
       " Document(metadata={'source': 'input_repo/Network_Security/configeration/aws_connection.py', 'language': <Language.PYTHON: 'python'>}, page_content='import os\\nimport boto3\\nfrom Network_Security.constant import (REGION,AWS_ACCESS_KEY,AWS_SECRET_KEY)\\nfrom dotenv import load_dotenv\\nload_dotenv()\\n\\nclass S3Client:\\n    s3_client = None\\n    s3_resource = None \\n\\n    def __init__(self, region_name=REGION):    \\n        _access_key = os.getenv(AWS_ACCESS_KEY)\\n        _secret_key = os.getenv(AWS_SECRET_KEY)  \\n\\n        if _access_key is None or _secret_key is None:\\n            raise ValueError(\"Missing AWS credentials in environment variables\")\\n\\n        if S3Client.s3_client is None and S3Client.s3_resource is None:\\n\\n            S3Client.s3_resource = boto3.resource(\\n                \\'s3\\',\\n                aws_access_key_id=_access_key,\\n                aws_secret_access_key=_secret_key,\\n                region_name=region_name\\n            )\\n            S3Client.s3_client = boto3.client(\\n                \\'s3\\',\\n                aws_access_key_id=_access_key,\\n                aws_secret_access_key=_secret_key,\\n                region_name=region_name\\n            )\\n        self.s3_client = S3Client.s3_client\\n        self.s3_resource = S3Client.s3_resource\\n'),\n",
       " Document(metadata={'source': 'input_repo/Network_Security/configeration/mongodb.py', 'language': <Language.PYTHON: 'python'>}, page_content='from Network_Security.constant import MONGOBD_URL, DATA_BASE_NAME\\nfrom Network_Security.exception.exception import NetworkSecurityException\\nfrom Network_Security.logging.logger import logging\\nfrom dotenv import load_dotenv \\nimport certifi \\nimport pymongo\\nimport sys\\nimport os \\n\\n\\nload_dotenv()\\n# MONGOBD_URL = os.getenv(\"MONGOBD_URL\")\\nca = certifi.where()  \\n\\nclass MongoDBClient:\\n    def __init__(self, database=DATA_BASE_NAME):\\n        try:\\n            mongo_url = os.getenv(MONGOBD_URL)\\n            if mongo_url is None:\\n                logging.info(\"MongoDB URL not found in environment variables\")\\n                raise ValueError(\"MongoDB URL is missing\")\\n\\n            MongoDBClient.client = pymongo.MongoClient(mongo_url, tlsCAFile=ca)\\n            self.client = MongoDBClient.client \\n            self.database = self.client[database]\\n            self.database_name = database  \\n\\n        except Exception as e:\\n            raise NetworkSecurityException(e, sys)\\n \\n    \\n\\n'),\n",
       " Document(metadata={'source': 'input_repo/Network_Security/configeration/__init__.py', 'language': <Language.PYTHON: 'python'>}, page_content='')]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documnets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "document_spliter = RecursiveCharacterTextSplitter.from_language(\n",
    "    language=Language.PYTHON,chunk_size = 500,chunk_overlap = 50\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "15513e70",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langchain_text_splitters.character.RecursiveCharacterTextSplitter at 0x7cd77edaa270>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "document_spliter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_chunk = document_spliter.split_documents(documnets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text_chunk: 134\n"
     ]
    }
   ],
   "source": [
    "print(\"text_chunk:\",len(text_chunk))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78c89040",
   "metadata": {},
   "source": [
    "## Embedding Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "HuggingFaceEmbeddings(model_name='sentence-transformers/all-MiniLM-l6-v2', cache_folder=None, model_kwargs={}, encode_kwargs={}, query_encode_kwargs={}, multi_process=False, show_progress=False)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## embeddings=OpenAIEmbeddings(disallowed_special=())\n",
    "\n",
    "model_name=\"sentence-transformers/all-MiniLM-l6-v2\"\n",
    "embedding = HuggingFaceEmbeddings(model_name=model_name)\n",
    "embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_store = FAISS.from_documents(text_chunk,embedding)\n",
    "## vector_store.persist()\n",
    "vector_store.save_local(\"faiss_index\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc9d7c2f",
   "metadata": {},
   "source": [
    "## LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "809894e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv()\n",
    "groq_api_key = os.getenv('GROQ_API_KEY')\n",
    "\n",
    "llm = ChatGroq(\n",
    "    model=\"llama-3.1-8b-instant\",\n",
    "    api_key=groq_api_key,\n",
    "    max_tokens=500\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b456f76e",
   "metadata": {},
   "outputs": [],
   "source": [
    "memory = ConversationSummaryMemory(llm=llm,memory_key='chat_history',return_messages=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriver = vector_store.as_retriever(search_type=\"mmr\", search_kwargs={\"k\": 5, \"fetch_k\": 50})\n",
    "search = ConversationalRetrievalChain.from_llm(llm=llm,retriever=retriver,memory=memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VectorStoreRetriever(tags=['FAISS', 'HuggingFaceEmbeddings'], vectorstore=<langchain_community.vectorstores.faiss.FAISS object at 0x7cd77fbaf380>, search_type='mmr', search_kwargs={'k': 5, 'fetch_k': 50})"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retriver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ConversationalRetrievalChain(memory=ConversationSummaryMemory(llm=ChatGroq(client=<groq.resources.chat.completions.Completions object at 0x7cd77e9e2de0>, async_client=<groq.resources.chat.completions.AsyncCompletions object at 0x7cd7b58025d0>, model_name='llama-3.1-8b-instant', model_kwargs={}, groq_api_key=SecretStr('**********'), max_tokens=500), chat_memory=InMemoryChatMessageHistory(messages=[]), return_messages=True, memory_key='chat_history'), verbose=False, combine_docs_chain=StuffDocumentsChain(verbose=False, llm_chain=LLMChain(verbose=False, prompt=ChatPromptTemplate(input_variables=['context', 'question'], input_types={}, partial_variables={}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context'], input_types={}, partial_variables={}, template=\"Use the following pieces of context to answer the user's question.\\nIf you don't know the answer, just say that you don't know, don't try to make up an answer.\\n----------------\\n{context}\"), additional_kwargs={}), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['question'], input_types={}, partial_variables={}, template='{question}'), additional_kwargs={})]), llm=ChatGroq(client=<groq.resources.chat.completions.Completions object at 0x7cd77e9e2de0>, async_client=<groq.resources.chat.completions.AsyncCompletions object at 0x7cd7b58025d0>, model_name='llama-3.1-8b-instant', model_kwargs={}, groq_api_key=SecretStr('**********'), max_tokens=500), output_parser=StrOutputParser(), llm_kwargs={}), document_prompt=PromptTemplate(input_variables=['page_content'], input_types={}, partial_variables={}, template='{page_content}'), document_variable_name='context'), question_generator=LLMChain(verbose=False, prompt=PromptTemplate(input_variables=['chat_history', 'question'], input_types={}, partial_variables={}, template='Given the following conversation and a follow up question, rephrase the follow up question to be a standalone question, in its original language.\\n\\nChat History:\\n{chat_history}\\nFollow Up Input: {question}\\nStandalone question:'), llm=ChatGroq(client=<groq.resources.chat.completions.Completions object at 0x7cd77e9e2de0>, async_client=<groq.resources.chat.completions.AsyncCompletions object at 0x7cd7b58025d0>, model_name='llama-3.1-8b-instant', model_kwargs={}, groq_api_key=SecretStr('**********'), max_tokens=500), output_parser=StrOutputParser(), llm_kwargs={}), retriever=VectorStoreRetriever(tags=['FAISS', 'HuggingFaceEmbeddings'], vectorstore=<langchain_community.vectorstores.faiss.FAISS object at 0x7cd77fbaf380>, search_type='mmr', search_kwargs={'k': 5, 'fetch_k': 50}))"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_32548/329157500.py:2: LangChainDeprecationWarning: The method `Chain.__call__` was deprecated in langchain-classic 0.1.0 and will be removed in 1.0. Use `invoke` instead.\n",
      "  result = search(question)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'question': 'whats get_feature_extract_data',\n",
       " 'chat_history': [SystemMessage(content='', additional_kwargs={}, response_metadata={})],\n",
       " 'answer': 'Based on the provided context, `get_feature_extract_data` is a method of an object ( likely a class) that appears to be responsible for extracting data from a MongoDB database and processing it for feature extraction.\\n\\nHere\\'s a high-level overview of what this method seems to be doing:\\n\\n1. It logs an informational message to indicate that it\\'s extracting data from MongoDB.\\n2. It creates an instance of `NetworkData` and calls its `get_dataframe` method to retrieve a DataFrame from a MongoDB collection specified in `self.ingestion_config.data_ingestion_collection_path`.\\n3. It defines a path for feature data extraction in `self.ingestion_config.data_ingestion_feature_path`.\\n4. It doesn\\'t seem to be using the feature data path for anything in the provided snippet.\\n5. It defines some metrics from an artifact called `metrics_artifact` (not shown in the snippet).\\n6. It defines a split ratio `DATA_INGESTION_SPLIT_RATIO`.\\n7. It creates a preprocessor object using the `get_data_transformation` method (not shown in the snippet).\\n8. It fits and transforms the input feature data using the preprocessor, creating training and testing arrays with the target feature appended.\\n\\nHowever, the actual implementation of `get_feature_extract_data` in the provided snippet seems incomplete, as it leaves out the usage of `feature_data_path` and doesn\\'t show how the transformed data is being used.\\n\\nHere\\'s a hypothetical function definition based on the provided snippet:\\n\\n```python\\ndef get_feature_extract_data(self):\\n    \"\"\"\\n    Extracts data from MongoDB, processes it, and returns transformed data.\\n    \"\"\"\\n    logging.info(\"Extracting data from MongoDB...\")\\n    networkdata = NetworkData()\\n    dataframe = networkdata.get_dataframe(\\n        collection_name=self.ingestion_config.data_ingestion_collection_path\\n    )\\n    # ... (rest of the method implementation)\\n```\\n\\nHowever, without more context or the complete implementation, it\\'s difficult to provide a more detailed answer.'}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question = 'whats get_feature_extract_data'\n",
    "result = search(question)\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Based on the provided context, `get_feature_extract_data` is a method of an object ( likely a class) that appears to be responsible for extracting data from a MongoDB database and processing it for feature extraction.\\n\\nHere\\'s a high-level overview of what this method seems to be doing:\\n\\n1. It logs an informational message to indicate that it\\'s extracting data from MongoDB.\\n2. It creates an instance of `NetworkData` and calls its `get_dataframe` method to retrieve a DataFrame from a MongoDB collection specified in `self.ingestion_config.data_ingestion_collection_path`.\\n3. It defines a path for feature data extraction in `self.ingestion_config.data_ingestion_feature_path`.\\n4. It doesn\\'t seem to be using the feature data path for anything in the provided snippet.\\n5. It defines some metrics from an artifact called `metrics_artifact` (not shown in the snippet).\\n6. It defines a split ratio `DATA_INGESTION_SPLIT_RATIO`.\\n7. It creates a preprocessor object using the `get_data_transformation` method (not shown in the snippet).\\n8. It fits and transforms the input feature data using the preprocessor, creating training and testing arrays with the target feature appended.\\n\\nHowever, the actual implementation of `get_feature_extract_data` in the provided snippet seems incomplete, as it leaves out the usage of `feature_data_path` and doesn\\'t show how the transformed data is being used.\\n\\nHere\\'s a hypothetical function definition based on the provided snippet:\\n\\n```python\\ndef get_feature_extract_data(self):\\n    \"\"\"\\n    Extracts data from MongoDB, processes it, and returns transformed data.\\n    \"\"\"\\n    logging.info(\"Extracting data from MongoDB...\")\\n    networkdata = NetworkData()\\n    dataframe = networkdata.get_dataframe(\\n        collection_name=self.ingestion_config.data_ingestion_collection_path\\n    )\\n    # ... (rest of the method implementation)\\n```\\n\\nHowever, without more context or the complete implementation, it\\'s difficult to provide a more detailed answer.'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result['answer']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10bb7edf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "chatapp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
