{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f7a9d64",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import base64\n",
    "import streamlit as st\n",
    "from PIL import Image\n",
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "from langchain_core.messages import HumanMessage, SystemMessage\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from unstructured.partition.pdf import partition_pdf\n",
    "\n",
    "# Ensure directories exist\n",
    "if not os.path.exists(\"figures\"):\n",
    "    os.makedirs(\"figures\")\n",
    "\n",
    "class MultiModalRAG:\n",
    "    def __init__(self):\n",
    "        # We use GPT-4o for both vision and reasoning\n",
    "        self.llm = ChatOpenAI(model=\"gpt-4o\", temperature=0)\n",
    "        self.vectorstore = Chroma(\n",
    "            collection_name=\"enterprise_rag\",\n",
    "            embedding_function=OpenAIEmbeddings(),\n",
    "            persist_directory=\"./chroma_db\"\n",
    "        )\n",
    "\n",
    "    @staticmethod\n",
    "    def encode_image(image_path):\n",
    "        with open(image_path, \"rb\") as f:\n",
    "            return base64.b64encode(f.read()).decode('utf-8')\n",
    "\n",
    "    def summarize_image(self, image_path):\n",
    "        \"\"\"Generates a searchable text description for an image.\"\"\"\n",
    "        b64_image = self.encode_image(image_path)\n",
    "        response = self.llm.invoke([\n",
    "            HumanMessage(content=[\n",
    "                {\"type\": \"text\", \"text\": \"Analyze this image from a document. Provide a detailed summary of its content, charts, or data for indexing.\"},\n",
    "                {\"type\": \"image_url\", \"image_url\": {\"url\": f\"data:image/jpeg;base64,{b64_image}\"}}\n",
    "            ])\n",
    "        ])\n",
    "        return response.content\n",
    "\n",
    "    def process_pdf(self, file_path):\n",
    "        \"\"\"Extracts text and images, then stores them in the vector database.\"\"\"\n",
    "        elements = partition_pdf(\n",
    "            filename=file_path,\n",
    "            extract_images_in_pdf=True,\n",
    "            infer_table_structure=True,\n",
    "            chunking_strategy=\"by_title\",\n",
    "            max_characters=1000,\n",
    "            image_output_dir_path=\"figures\"\n",
    "        )\n",
    "        \n",
    "        for el in elements:\n",
    "            metadata = {\"source\": file_path, \"type\": \"text\"}\n",
    "            if \"Image\" in str(type(el)):\n",
    "                img_path = el.metadata.image_path\n",
    "                content = self.summarize_image(img_path)\n",
    "                metadata.update({\"type\": \"image\", \"image_path\": img_path})\n",
    "            else:\n",
    "                content = el.text\n",
    "            \n",
    "            if content:\n",
    "                self.vectorstore.add_texts(texts=[content], metadatas=[metadata])\n",
    "\n",
    "    def get_final_answer(self, query, context_docs):\n",
    "        \"\"\"Standard RAG: Asks LLM to answer based ONLY on retrieved context.\"\"\"\n",
    "        context_text = \"\\n\\n\".join([d.page_content for d in context_docs])\n",
    "        \n",
    "        prompt = [\n",
    "            SystemMessage(content=\"You are a helpful assistant. Answer the question using ONLY the provided context. If the answer isn't in the context, say you don't know.\"),\n",
    "            HumanMessage(content=f\"Context:\\n{context_text}\\n\\nQuestion: {query}\")\n",
    "        ]\n",
    "        response = self.llm.invoke(prompt)\n",
    "        return response.content\n",
    "\n",
    "    def query_system(self, user_input, is_image=False):\n",
    "        \"\"\"Handles both text and image queries, returning a text answer + images.\"\"\"\n",
    "        search_query = user_input\n",
    "        if is_image:\n",
    "            search_query = self.summarize_image(user_input)\n",
    "            st.info(\"Image search initialized...\")\n",
    "\n",
    "        # 1. Retrieve the most relevant 3 documents (text or image summaries)\n",
    "        docs = self.vectorstore.similarity_search(search_query, k=3)\n",
    "        \n",
    "        # 2. Generate a natural language answer based on those docs\n",
    "        final_answer = self.get_final_answer(search_query, docs)\n",
    "        \n",
    "        return final_answer, docs\n",
    "\n",
    "# --- STREAMLIT UI ---\n",
    "st.set_page_config(page_title=\"Multimodal RAG\", layout=\"wide\")\n",
    "st.title(\"üìë Multi-Modal Enterprise Intelligence\")\n",
    "\n",
    "if \"rag\" not in st.session_state:\n",
    "    st.session_state.rag = MultiModalRAG()\n",
    "\n",
    "with st.sidebar:\n",
    "    st.header(\"Upload Knowledge Base\")\n",
    "    uploaded_pdf = st.file_uploader(\"Upload PDF\", type=\"pdf\")\n",
    "    if uploaded_pdf and st.button(\"Index Document\"):\n",
    "        with st.spinner(\"Indexing...\"):\n",
    "            with open(\"temp.pdf\", \"wb\") as f:\n",
    "                f.write(uploaded_pdf.getbuffer())\n",
    "            st.session_state.rag.process_pdf(\"temp.pdf\")\n",
    "            st.success(\"Indexing Complete!\")\n",
    "\n",
    "st.subheader(\"Query the Document\")\n",
    "col1, col2 = st.columns([2, 1])\n",
    "with col2:\n",
    "    uploaded_query_img = st.file_uploader(\"Search using an image?\", type=[\"jpg\", \"png\", \"jpeg\"])\n",
    "with col1:\n",
    "    user_text = st.chat_input(\"Ask a question about the PDF...\")\n",
    "\n",
    "# Input Processing Logic\n",
    "query_input = None\n",
    "is_image_query = False\n",
    "\n",
    "if uploaded_query_img:\n",
    "    img_path = f\"figures/query_{uploaded_query_img.name}\"\n",
    "    with open(img_path, \"wb\") as f:\n",
    "        f.write(uploaded_query_img.getbuffer())\n",
    "    query_input, is_image_query = img_path, True\n",
    "elif user_text:\n",
    "    query_input, is_image_query = user_text, False\n",
    "\n",
    "# Display logic\n",
    "if query_input:\n",
    "    with st.spinner(\"Thinking...\"):\n",
    "        answer, matched_docs = st.session_state.rag.query_system(query_input, is_image=is_image_query)\n",
    "        \n",
    "        # Show the conversational answer\n",
    "        st.markdown(f\"### Answer:\\n{answer}\")\n",
    "        \n",
    "        # Show relevant images found during the search\n",
    "        st.markdown(\"---\")\n",
    "        st.markdown(\"#### Supporting Evidence from PDF:\")\n",
    "        \n",
    "        cols = st.columns(len(matched_docs))\n",
    "        for idx, doc in enumerate(matched_docs):\n",
    "            with cols[idx]:\n",
    "                if doc.metadata[\"type\"] == \"image\":\n",
    "                    st.image(doc.metadata[\"image_path\"], caption=\"Found Image Context\")\n",
    "                else:\n",
    "                    st.caption(\"Text Context Found\")\n",
    "                with st.expander(\"View Source Content\"):\n",
    "                    st.write(doc.page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # üìë Multimodal Enterprise Intelligence RAG\n",
    "# > **An end-to-end RAG pipeline capable of querying unstructured data (PDFs, Images, and Tables) using Cross-Modal Retrieval.**\n",
    "\n",
    "# ## üöÄ Overview\n",
    "# Traditional RAG systems are limited to text. This project implements a **Multi-Vector Retrieval** architecture that allows users to query a document using either **text** or **images**. The system \"sees\" the charts, tables, and images within a PDF, indexes them semantically, and retrieves them contextually.\n",
    "\n",
    "\n",
    "\n",
    "# ## ‚ú® Key Features\n",
    "# * **Multimodal Ingestion:** Uses `Unstructured.io` to partition complex PDFs into text chunks, structured tables, and high-resolution images.\n",
    "# * **Vision-Language Indexing:** Leverages **GPT-4o** to generate searchable text summaries of visual data, enabling images to be retrieved via standard vector similarity.\n",
    "# * **Cross-Modal Querying:** Users can upload a separate image (e.g., a photo of a graph) to find matching or semantically related content within the indexed PDF.\n",
    "# * **Unified Context:** The system provides \"Ground Truth\" by displaying the original source image alongside the AI-generated text answer.\n",
    "# * **Industry-Ready UI:** Clean, interactive **Streamlit** interface for easy document indexing and real-time chat.\n",
    "\n",
    "# ## üèóÔ∏è Technical Architecture\n",
    "# The system follows a 4-stage pipeline to ensure accuracy and explainability:\n",
    "\n",
    "# 1.  **Partitioning:** PDF elements are separated. Images are saved to a local store while tables are preserved in HTML/Markdown format.\n",
    "# 2.  **Summarization:** Each image is sent to the **GPT-4o Vision API** to generate a \"description proxy\" that captures data trends and visual context.\n",
    "# 3.  **Vector Store:** All content (text, tables, and image summaries) is converted into embeddings using `OpenAIEmbeddings` and stored in **ChromaDB**.\n",
    "# 4.  **Retrieval Logic:**\n",
    "#     * **Text Query:** Uses cosine similarity to find relevant chunks.\n",
    "#     * **Image Query:** The user's uploaded image is first \"translated\" into a textual description by the LLM, which is then used to perform a similarity search against the index.\n",
    "\n",
    "\n",
    "\n",
    "# ## üõ†Ô∏è Tech Stack\n",
    "# * **LLM:** OpenAI GPT-4o (Vision & Text capabilities)\n",
    "# * **Orchestration:** LangChain\n",
    "# * **Vector Database:** ChromaDB (Persistent)\n",
    "# * **Data Extraction:** Unstructured.io\n",
    "# * **Frontend:** Streamlit\n",
    "# * **Language:** Python 3.10+\n",
    "\n",
    "# ## üì• Installation & Setup\n",
    "\n",
    "# ### 1. Prerequisites\n",
    "# Ensure you have an OpenAI API Key and `Poppler` (required for PDF image extraction) installed on your system.\n",
    "\n",
    "# ### 2. Clone & Install\n",
    "# ```bash\n",
    "# git clone [https://github.com/yourusername/multimodal-rag-enterprise.git](https://github.com/yourusername/multimodal-rag-enterprise.git)\n",
    "# cd multimodal-rag-enterprise\n",
    "# pip install langchain langchain-openai unstructured[pdf] chromadb pillow streamlit rapidocr-onnxruntime\n",
    "\n",
    "\n",
    "# 3. Set Environment Variables\n",
    "\n",
    "# export OPENAI_API_KEY='your_api_key_here'\n",
    "\n",
    "# 4. Run the App\n",
    "\n",
    "# streamlit run app.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a87c872e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "264eae32",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import base64\n",
    "import shutil\n",
    "import streamlit as st\n",
    "from PIL import Image\n",
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "from langchain_core.messages import HumanMessage, SystemMessage\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from unstructured.partition.pdf import partition_pdf\n",
    "\n",
    "# --- CONFIGURATION & DIRECTORIES ---\n",
    "CHROMA_PATH = \"./chroma_db\"\n",
    "IMAGE_DIR = \"figures\"\n",
    "\n",
    "for folder in [IMAGE_DIR, CHROMA_PATH]:\n",
    "    if not os.path.exists(folder):\n",
    "        os.makedirs(folder)\n",
    "\n",
    "class MultiModalRAG:\n",
    "    def __init__(self):\n",
    "        # High-reasoning model (Cheap & Fast)\n",
    "        self.llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)\n",
    "        self.vectorstore = Chroma(\n",
    "            collection_name=\"enterprise_rag\",\n",
    "            embedding_function=OpenAIEmbeddings(model=\"text-embedding-3-small\"),\n",
    "            persist_directory=CHROMA_PATH\n",
    "        )\n",
    "\n",
    "    @staticmethod\n",
    "    def encode_image(image_path):\n",
    "        with open(image_path, \"rb\") as f:\n",
    "            return base64.b64encode(f.read()).decode('utf-8')\n",
    "\n",
    "    def summarize_image(self, image_path):\n",
    "        \"\"\"Standardizes visual data into searchable text.\"\"\"\n",
    "        b64_image = self.encode_image(image_path)\n",
    "        response = self.llm.invoke([\n",
    "            HumanMessage(content=[\n",
    "                {\"type\": \"text\", \"text\": \"Analyze this image from a document. Describe charts, data points, or visual content for a search index.\"},\n",
    "                {\"type\": \"image_url\", \"image_url\": {\"url\": f\"data:image/jpeg;base64,{b64_image}\"}}\n",
    "            ])\n",
    "        ])\n",
    "        return response.content\n",
    "\n",
    "    def process_pdf(self, file_path):\n",
    "        \"\"\"Industry-standard partitioning for PDF extraction.\"\"\"\n",
    "        elements = partition_pdf(\n",
    "            filename=file_path,\n",
    "            extract_images_in_pdf=True,\n",
    "            infer_table_structure=True,\n",
    "            chunking_strategy=\"by_title\",\n",
    "            max_characters=1000,\n",
    "            image_output_dir_path=IMAGE_DIR\n",
    "        )\n",
    "        \n",
    "        for el in elements:\n",
    "            metadata = {\"source\": file_path, \"type\": \"text\"}\n",
    "            if \"Image\" in str(type(el)):\n",
    "                img_path = el.metadata.image_path\n",
    "                content = self.summarize_image(img_path)\n",
    "                metadata.update({\"type\": \"image\", \"image_path\": img_path})\n",
    "            else:\n",
    "                content = el.text\n",
    "            \n",
    "            if content:\n",
    "                self.vectorstore.add_texts(texts=[content], metadatas=[metadata])\n",
    "\n",
    "    def query_system(self, user_input, is_image=False):\n",
    "        \"\"\"Cross-modal retrieval logic.\"\"\"\n",
    "        search_query = user_input\n",
    "        if is_image:\n",
    "            search_query = self.summarize_image(user_input)\n",
    "        \n",
    "        # Retrieve top matches\n",
    "        docs = self.vectorstore.similarity_search(search_query, k=3)\n",
    "        \n",
    "        # Formulate answer based on context\n",
    "        context_text = \"\\n\\n\".join([d.page_content for d in docs])\n",
    "        prompt = [\n",
    "            # SystemMessage(content=\"Answer the question using ONLY the provided context. Show expertise and clarity.\"),\n",
    "            # SystemMessage(content=\"Answer based ONLY on the provided context. If an image is relevant, mention it.\"),\n",
    "            SystemMessage(content=\"You are a helpful assistant. Answer the question using ONLY the provided context and if an image is relevant, mention it. If the answer isn't in the context, say you don't know.\"),\n",
    "\n",
    "            HumanMessage(content=f\"Context:\\n{context_text}\\n\\nQuestion: {search_query}\")\n",
    "        ]\n",
    "        answer = self.llm.invoke(prompt).content\n",
    "        return answer, docs\n",
    "\n",
    "# --- STREAMLIT DASHBOARD ---\n",
    "st.set_page_config(page_title=\"Multimodal RAG\", layout=\"wide\")\n",
    "st.title(\"üìë Multi-Modal Enterprise Intelligence\")\n",
    "\n",
    "if \"rag\" not in st.session_state:\n",
    "    st.session_state.rag = MultiModalRAG()\n",
    "\n",
    "with st.sidebar:\n",
    "    st.header(\"Admin Controls\")\n",
    "    uploaded_pdf = st.file_uploader(\"Upload Knowledge Base\", type=\"pdf\")\n",
    "    \n",
    "    if uploaded_pdf and st.button(\"Index Document\"):\n",
    "        with st.spinner(\"Indexing...\"):\n",
    "            with open(\"temp.pdf\", \"wb\") as f:\n",
    "                f.write(uploaded_pdf.getbuffer())\n",
    "            st.session_state.rag.process_pdf(\"temp.pdf\")\n",
    "            st.success(\"PDF Content Embedded!\")\n",
    "\n",
    "    if st.button(\"üóëÔ∏è Reset System\", help=\"Clears the Vector DB and Images\"):\n",
    "        shutil.rmtree(CHROMA_PATH, ignore_errors=True)\n",
    "        shutil.rmtree(IMAGE_DIR, ignore_errors=True)\n",
    "        st.cache_resource.clear()\n",
    "        st.rerun()\n",
    "\n",
    "# --- CHAT & INTERACTION ---\n",
    "st.subheader(\"Interactive Knowledge Retrieval\")\n",
    "user_text = st.chat_input(\"Ask a question about the document...\")\n",
    "uploaded_query_img = st.file_uploader(\"Or upload an image to search by visual context\", type=[\"jpg\", \"png\", \"jpeg\"])\n",
    "\n",
    "# Retrieval Logic\n",
    "query_val = None\n",
    "is_img = False\n",
    "\n",
    "if uploaded_query_img:\n",
    "    img_save_path = os.path.join(IMAGE_DIR, f\"query_{uploaded_query_img.name}\")\n",
    "    with open(img_save_path, \"wb\") as f:\n",
    "        f.write(uploaded_query_img.getbuffer())\n",
    "    query_val, is_img = img_save_path, True\n",
    "elif user_text:\n",
    "    query_val, is_img = user_text, False\n",
    "\n",
    "if query_val:\n",
    "    with st.spinner(\"Analyzing context...\"):\n",
    "        ans, results = st.session_state.rag.query_system(query_val, is_image=is_img)\n",
    "        \n",
    "        st.markdown(f\"### ü§ñ Response\\n{ans}\")\n",
    "        \n",
    "        st.markdown(\"---\")\n",
    "        st.markdown(\"#### üñºÔ∏è Evidence & Sources\")\n",
    "        cols = st.columns(3)\n",
    "        for i, doc in enumerate(results):\n",
    "            with cols[i % 3]:\n",
    "                if doc.metadata[\"type\"] == \"image\":\n",
    "                    st.image(doc.metadata[\"image_path\"], use_container_width=True)\n",
    "                st.info(f\"Source Type: {doc.metadata['type'].upper()}\")\n",
    "                with st.expander(\"Show Raw Context\"):\n",
    "                    st.write(doc.page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b121040d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "776bb399",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import base64\n",
    "import shutil\n",
    "import streamlit as st\n",
    "from PIL import Image\n",
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_core.messages import HumanMessage, SystemMessage\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from unstructured.partition.pdf import partition_pdf\n",
    "\n",
    "# --- 1. SETTINGS & DIRECTORIES ---\n",
    "CHROMA_PATH = \"./chroma_db\"\n",
    "IMAGE_DIR = \"extracted_figures\"\n",
    "os.makedirs(IMAGE_DIR, exist_ok=True)\n",
    "\n",
    "class HybridMultimodalRAG:\n",
    "    def __init__(self):\n",
    "        # High-reasoning model (Cheap & Fast)\n",
    "        self.llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)\n",
    "        \n",
    "        # LOW COST: Using free local HuggingFace embeddings instead of OpenAI\n",
    "        # This reduces embedding costs to $0.\n",
    "        self.embeddings = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
    "        \n",
    "        self.vectorstore = Chroma(\n",
    "            collection_name=\"enterprise_inventory\",\n",
    "            embedding_function=self.embeddings,\n",
    "            persist_directory=CHROMA_PATH\n",
    "        )\n",
    "\n",
    "    @staticmethod\n",
    "    def encode_image(image_path):\n",
    "        \"\"\"Convert image to base64 for LLM vision processing.\"\"\"\n",
    "        with open(image_path, \"rb\") as f:\n",
    "            return base64.b64encode(f.read()).decode('utf-8')\n",
    "\n",
    "    def describe_image(self, image_path):\n",
    "        \"\"\"Uses Vision LLM to turn an image into searchable text.\"\"\"\n",
    "        b64_image = self.encode_image(image_path)\n",
    "        response = self.llm.invoke([\n",
    "            HumanMessage(content=[\n",
    "                {\"type\": \"text\", \"text\": \"Describe this document image/chart in detail for indexing. Focus on data, labels, and trends.\"},\n",
    "                {\"type\": \"image_url\", \"image_url\": {\"url\": f\"data:image/jpeg;base64,{b64_image}\"}}\n",
    "            ])\n",
    "        ])\n",
    "        return response.content\n",
    "\n",
    "    def ingest_pdf(self, pdf_path):\n",
    "        \"\"\"Extracts text and images, creates summaries, and saves to Vector DB.\"\"\"\n",
    "        # Partitioning PDF with image extraction\n",
    "        elements = partition_pdf(\n",
    "            filename=pdf_path,\n",
    "            extract_images_in_pdf=True,\n",
    "            infer_table_structure=True,\n",
    "            chunking_strategy=\"by_title\",\n",
    "            max_characters=1000,\n",
    "            image_output_dir_path=IMAGE_DIR\n",
    "        )\n",
    "\n",
    "        for el in elements:\n",
    "            metadata = {\"source\": pdf_path}\n",
    "            \n",
    "            if \"Image\" in str(type(el)):\n",
    "                # Handle Image\n",
    "                img_path = el.metadata.image_path\n",
    "                summary = self.describe_image(img_path)\n",
    "                metadata.update({\"type\": \"image\", \"image_path\": img_path})\n",
    "                self.vectorstore.add_texts(texts=[summary], metadatas=[metadata])\n",
    "            else:\n",
    "                # Handle Text\n",
    "                metadata.update({\"type\": \"text\"})\n",
    "                self.vectorstore.add_texts(texts=[el.text], metadatas=[metadata])\n",
    "\n",
    "    def query(self, user_input, is_image=False):\n",
    "        \"\"\"Multimodal query logic.\"\"\"\n",
    "        search_text = user_input\n",
    "        \n",
    "        # If user uploads an image to ask a question\n",
    "        if is_image:\n",
    "            search_text = self.describe_image(user_input)\n",
    "            st.toast(f\"Searching for visual context: {search_text[:50]}...\")\n",
    "\n",
    "        # Search top 3 relevant chunks\n",
    "        docs = self.vectorstore.similarity_search(search_text, k=3)\n",
    "        \n",
    "        # Generate final answer based on context\n",
    "        context = \"\\n\\n\".join([d.page_content for d in docs])\n",
    "        prompt = [\n",
    "            SystemMessage(content=\"Answer based ONLY on the provided context. If an image is relevant, mention it.\"),\n",
    "            HumanMessage(content=f\"Context: {context}\\n\\nQuestion: {search_text}\")\n",
    "        ]\n",
    "        answer = self.llm.invoke(prompt).content\n",
    "        return answer, docs\n",
    "\n",
    "# --- 2. STREAMLIT INTERFACE ---\n",
    "st.set_page_config(page_title=\"MultiModal RAG\", layout=\"wide\")\n",
    "st.title(\"üìë Smart Multimodal Enterprise RAG\")\n",
    "st.markdown(\"---\")\n",
    "\n",
    "# Initialize Session\n",
    "if \"rag_engine\" not in st.session_state:\n",
    "    st.session_state.rag_engine = HybridMultimodalRAG()\n",
    "\n",
    "# Sidebar Setup\n",
    "with st.sidebar:\n",
    "    st.header(\"Admin Panel\")\n",
    "    openai_key = st.text_input(\"OpenAI API Key\", type=\"password\")\n",
    "    if openai_key:\n",
    "        os.environ[\"OPENAI_API_KEY\"] = openai_key\n",
    "    \n",
    "    uploaded_file = st.file_uploader(\"Upload PDF Knowledge Base\", type=\"pdf\")\n",
    "    if uploaded_file and st.button(\"üî• Start Indexing\"):\n",
    "        with st.spinner(\"Processing PDF (Extraction & Vision Summary)...\"):\n",
    "            with open(\"temp_doc.pdf\", \"wb\") as f:\n",
    "                f.write(uploaded_file.getbuffer())\n",
    "            st.session_state.rag_engine.ingest_pdf(\"temp_doc.pdf\")\n",
    "            st.success(\"Indexing Complete!\")\n",
    "\n",
    "    if st.button(\"üóëÔ∏è Reset All Data\"):\n",
    "        shutil.rmtree(CHROMA_PATH, ignore_errors=True)\n",
    "        shutil.rmtree(IMAGE_DIR, ignore_errors=True)\n",
    "        st.rerun()\n",
    "\n",
    "# Main Search Area\n",
    "col1, col2 = st.columns([3, 1])\n",
    "with col1:\n",
    "    user_query = st.chat_input(\"Ask about your document...\")\n",
    "with col2:\n",
    "    query_img = st.file_uploader(\"Upload image query\", type=['png', 'jpg', 'jpeg'])\n",
    "\n",
    "# Execution Logic\n",
    "if user_query or query_img:\n",
    "    input_val = user_query\n",
    "    is_img_mode = False\n",
    "    \n",
    "    if query_img:\n",
    "        input_val = \"query_image.jpg\"\n",
    "        with open(input_val, \"wb\") as f:\n",
    "            f.write(query_img.getbuffer())\n",
    "        is_img_mode = True\n",
    "\n",
    "    with st.spinner(\"Generating Answer...\"):\n",
    "        ans, sources = st.session_state.rag_engine.query(input_val, is_image=is_img_mode)\n",
    "        \n",
    "        st.markdown(\"### ü§ñ Assistant Answer\")\n",
    "        st.write(ans)\n",
    "        \n",
    "        st.markdown(\"### üîç Source Evidence\")\n",
    "        grid = st.columns(3)\n",
    "        for i, doc in enumerate(sources):\n",
    "            with grid[i]:\n",
    "                if doc.metadata.get(\"type\") == \"image\":\n",
    "                    st.image(doc.metadata[\"image_path\"], caption=\"Visual Context\")\n",
    "                st.caption(f\"Match Type: {doc.metadata.get('type')}\")\n",
    "                with st.expander(\"View Context Text\"):\n",
    "                    st.write(doc.page_content)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
