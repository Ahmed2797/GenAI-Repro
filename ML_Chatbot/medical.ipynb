{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b3c761eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from langchain_groq import ChatGroq\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "import os\n",
    "\n",
    "load_dotenv()\n",
    "groq_api_key = os.getenv('GROQ_API_KEY')\n",
    "\n",
    "llm = ChatGroq(\n",
    "    model=\"llama-3.1-8b-instant\",\n",
    "    api_key=groq_api_key,\n",
    "    max_tokens=500\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a102e572",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"LangChain is an open-source framework for building conversational AI systems. It's built on top of the Python programming language, and it allows developers to create more sophisticated conversational models by leveraging LLMs (Large Language Models) like the ones provided by models like Llama, LLaMA, and more.\\n\\nLangChain is designed to work seamlessly with LLMs and offers a variety of tools and features that make it easier to integrate these powerful models into real-world applications. Some of the key features of LangChain include:\\n\\n1. **Conversational APIs**: LangChain provides a simple way to create conversational APIs that can be used to interact with LLMs. This allows developers to build conversational interfaces that can understand and respond to user input in a more natural way.\\n2. **Task Management**: LangChain includes a task management system that allows developers to break down complex conversational tasks into smaller, more manageable pieces. This makes it easier to build and train conversational models that can perform complex tasks.\\n3. **Memory-Augmentation**: LangChain includes memory-augmentation capabilities, which allow developers to store and retrieve context information from previous conversations. This helps to improve the accuracy and coherence of conversational responses.\\n4. **Modular Architecture**: LangChain is designed with a modular architecture, which makes it easy to swap out different LLMs or add new functionality as needed.\\n\\nSome of the use cases for LangChain include:\\n\\n1. **Chatbots**: LangChain can be used to build advanced chatbots that can understand and respond to user input in a more natural way.\\n2. **Virtual Assistants**: LangChain can be used to build virtual assistants that can perform complex tasks and answer user questions in a more accurate and helpful way.\\n3. **Customer Support**: LangChain can be used to build customer support systems that can understand and respond to user queries in a more natural way.\\n\\nOverall, LangChain is a powerful tool for building conversational AI systems, and it has a lot of potential for innovation and growth in the field of conversational AI.\", additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 419, 'prompt_tokens': 40, 'total_tokens': 459, 'completion_time': 0.574129947, 'prompt_time': 0.001853957, 'queue_time': 0.055428133, 'total_time': 0.575983904}, 'model_name': 'llama-3.1-8b-instant', 'system_fingerprint': 'fp_f757f4b0bf', 'service_tier': 'on_demand', 'finish_reason': 'stop', 'logprobs': None, 'model_provider': 'groq'}, id='lc_run--019b3e8e-724a-7d32-b2a3-d7f248fa32d9-0', usage_metadata={'input_tokens': 40, 'output_tokens': 419, 'total_tokens': 459})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm.invoke([{\"role\": \"user\", \"content\": \"What is langchain?\"}])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "27b381b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_14841/1519327487.py:6: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the `langchain-huggingface package and should be used instead. To use it run `pip install -U `langchain-huggingface` and import as `from `langchain_huggingface import HuggingFaceEmbeddings``.\n",
      "  embeddings = HuggingFaceEmbeddings(\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "\n",
    "# The model name is loaded from the sentence-transformers library\n",
    "model_name = \"sentence-transformers/all-MiniLM-l6-v2\"\n",
    "\n",
    "embeddings = HuggingFaceEmbeddings(\n",
    "    model_name=model_name,\n",
    ")\n",
    "\n",
    "text = \"This is a test document.\"\n",
    "doc_embeddings = embeddings.embed_documents([text, \"Another document\"])\n",
    "query_embedding = embeddings.embed_query(text)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[-0.038338541984558105,\n",
       " 0.12346471846103668,\n",
       " -0.02864297851920128,\n",
       " 0.05365270376205444,\n",
       " 0.008845366537570953,\n",
       " -0.03983934596180916,\n",
       " -0.07300589233636856,\n",
       " 0.04777132719755173,\n",
       " -0.030462471768260002,\n",
       " 0.05497974902391434,\n",
       " 0.08505292981863022,\n",
       " 0.03665666654706001,\n",
       " -0.005319973453879356,\n",
       " -0.002233141800388694,\n",
       " -0.06071099638938904,\n",
       " -0.027237920090556145,\n",
       " -0.01135166734457016,\n",
       " -0.042437683790922165,\n",
       " 0.00912993960082531,\n",
       " 0.10081552714109421,\n",
       " 0.07578728348016739,\n",
       " 0.06911715865135193,\n",
       " 0.009857431054115295,\n",
       " -0.0018377641681581736,\n",
       " 0.02624903991818428,\n",
       " 0.03290243074297905,\n",
       " -0.07177437096834183,\n",
       " 0.028384247794747353,\n",
       " 0.06170954555273056,\n",
       " -0.052529532462358475,\n",
       " 0.033661652356386185,\n",
       " 0.07446812838315964,\n",
       " 0.07536034286022186,\n",
       " 0.03538404777646065,\n",
       " 0.06713404506444931,\n",
       " 0.010798045434057713,\n",
       " 0.08167017996311188,\n",
       " 0.016562897711992264,\n",
       " 0.03283063694834709,\n",
       " 0.036325663328170776,\n",
       " 0.0021727988496422768,\n",
       " -0.09895738214254379,\n",
       " 0.0050467848777771,\n",
       " 0.05089650675654411,\n",
       " 0.009287580847740173,\n",
       " 0.024507684633135796,\n",
       " -0.06440776586532593,\n",
       " 0.0019363233586773276,\n",
       " -0.07910346984863281,\n",
       " 0.02085036039352417,\n",
       " -0.019228283315896988,\n",
       " -0.028054701164364815,\n",
       " -0.07059799134731293,\n",
       " -0.007083642762154341,\n",
       " 0.01040566898882389,\n",
       " 0.03883412480354309,\n",
       " 0.017656050622463226,\n",
       " -0.0196060948073864,\n",
       " -0.0200584065169096,\n",
       " 0.018083831295371056,\n",
       " -0.00017212280363310128,\n",
       " 0.01304331049323082,\n",
       " -0.09337245672941208,\n",
       " 0.08453577011823654,\n",
       " 0.11705497652292252,\n",
       " 0.05741341412067413,\n",
       " -0.022439060732722282,\n",
       " -0.03677631914615631,\n",
       " -0.03434623405337334,\n",
       " -0.06383824348449707,\n",
       " -0.06846098601818085,\n",
       " -0.005553087685257196,\n",
       " 0.04437841475009918,\n",
       " 0.01666928641498089,\n",
       " 0.03091181442141533,\n",
       " -0.01975969783961773,\n",
       " -0.02485509030520916,\n",
       " -0.059043921530246735,\n",
       " 0.0945875346660614,\n",
       " -0.06530515104532242,\n",
       " -0.05597259849309921,\n",
       " -0.03284722939133644,\n",
       " 0.008115164935588837,\n",
       " -0.0022346905898302794,\n",
       " 0.00202333927154541,\n",
       " 0.07942130416631699,\n",
       " 0.08518772572278976,\n",
       " 0.007815279997885227,\n",
       " -0.013745564967393875,\n",
       " 0.03110421635210514,\n",
       " 0.010080911219120026,\n",
       " -0.0327555313706398,\n",
       " 0.007714756764471531,\n",
       " -0.006191883236169815,\n",
       " -0.05613410472869873,\n",
       " 0.004364895634353161,\n",
       " -0.01403754111379385,\n",
       " -0.0393047034740448,\n",
       " 0.07822344452142715,\n",
       " 0.07393720000982285,\n",
       " 0.05619140714406967,\n",
       " 0.0033013394568115473,\n",
       " 0.04155803844332695,\n",
       " -0.010387564077973366,\n",
       " -0.1327269971370697,\n",
       " -0.10473109781742096,\n",
       " 0.018451867625117302,\n",
       " -0.07520627975463867,\n",
       " 0.04954081401228905,\n",
       " -0.02853083796799183,\n",
       " -0.013584112748503685,\n",
       " -0.03711264207959175,\n",
       " -0.06756577640771866,\n",
       " -0.019552480429410934,\n",
       " -0.01021183468401432,\n",
       " -0.05193488672375679,\n",
       " -0.059412311762571335,\n",
       " 0.016754014417529106,\n",
       " 0.04098019376397133,\n",
       " 0.0015223517548292875,\n",
       " 0.08095286786556244,\n",
       " 0.002651096787303686,\n",
       " -0.03870721161365509,\n",
       " -0.047030311077833176,\n",
       " -0.05854424834251404,\n",
       " -0.029478447511792183,\n",
       " 0.03882650285959244,\n",
       " -8.102626724236364e-33,\n",
       " -0.012914232909679413,\n",
       " -0.014458484947681427,\n",
       " -0.022368736565113068,\n",
       " 0.1056450605392456,\n",
       " 0.003727440256625414,\n",
       " 0.005939539987593889,\n",
       " -0.023657293990254402,\n",
       " 0.041163962334394455,\n",
       " -0.07411692291498184,\n",
       " 0.00707696657627821,\n",
       " 0.001834972994402051,\n",
       " -0.03314222767949104,\n",
       " 0.0068188318982720375,\n",
       " 0.04693516343832016,\n",
       " -0.03836112096905708,\n",
       " 0.05861292779445648,\n",
       " -0.08403787761926651,\n",
       " 0.11954139918088913,\n",
       " -0.02520415000617504,\n",
       " 0.027611656114459038,\n",
       " 0.02447570115327835,\n",
       " 0.014137313701212406,\n",
       " 0.012866539880633354,\n",
       " -0.0577956922352314,\n",
       " -0.03169172629714012,\n",
       " -0.002900663996115327,\n",
       " -0.02725416235625744,\n",
       " -0.027451207861304283,\n",
       " -0.03404247388243675,\n",
       " 0.02013683319091797,\n",
       " 0.022654535248875618,\n",
       " 0.030933387577533722,\n",
       " -0.045505911111831665,\n",
       " -0.002516300417482853,\n",
       " 0.015102322213351727,\n",
       " 0.09668107330799103,\n",
       " 0.0018094475381076336,\n",
       " -0.05403869226574898,\n",
       " 0.0025403713807463646,\n",
       " 0.006050989963114262,\n",
       " -0.056302234530448914,\n",
       " -0.028254201635718346,\n",
       " 0.06966650485992432,\n",
       " 0.04410799220204353,\n",
       " 0.03983236104249954,\n",
       " -0.04194303974509239,\n",
       " -0.003809897229075432,\n",
       " -0.04156694933772087,\n",
       " 0.09482310712337494,\n",
       " 0.019028935581445694,\n",
       " -0.040117047727108,\n",
       " 0.03242229297757149,\n",
       " 0.01256583258509636,\n",
       " -0.056325897574424744,\n",
       " 0.044611856341362,\n",
       " 0.04928920418024063,\n",
       " 0.01744263619184494,\n",
       " 0.05323150008916855,\n",
       " -0.020876476541161537,\n",
       " 0.061462536454200745,\n",
       " -0.01483733020722866,\n",
       " 0.07423632591962814,\n",
       " -0.05769442766904831,\n",
       " 0.049852155148983,\n",
       " -0.0589040145277977,\n",
       " -0.0006539408932439983,\n",
       " -0.10970553010702133,\n",
       " -0.06829901784658432,\n",
       " 0.13056597113609314,\n",
       " -0.011906684376299381,\n",
       " -0.015998493880033493,\n",
       " -0.021104130893945694,\n",
       " -0.0071442145854234695,\n",
       " -0.016443882137537003,\n",
       " -0.016906289383769035,\n",
       " -0.048137117177248,\n",
       " 0.015731701627373695,\n",
       " 0.030654842033982277,\n",
       " -0.004599845502525568,\n",
       " -0.03823971375823021,\n",
       " -0.047186821699142456,\n",
       " -0.08068913966417313,\n",
       " -0.011494748294353485,\n",
       " -0.051907800137996674,\n",
       " -0.04332379251718521,\n",
       " -0.019110003486275673,\n",
       " 0.03634184971451759,\n",
       " -0.06575318425893784,\n",
       " -0.014969329349696636,\n",
       " -0.09113643318414688,\n",
       " 0.03512788191437721,\n",
       " 0.01990416832268238,\n",
       " -0.05599287152290344,\n",
       " -0.04273850843310356,\n",
       " 0.11667021363973618,\n",
       " 4.7537233992963164e-33,\n",
       " -0.04277690500020981,\n",
       " 0.010693302378058434,\n",
       " -0.08699914813041687,\n",
       " 0.11428380757570267,\n",
       " 0.02619420923292637,\n",
       " 0.008768027648329735,\n",
       " 0.08940352499485016,\n",
       " -0.0019060684135183692,\n",
       " -0.0455072782933712,\n",
       " 0.08432012796401978,\n",
       " 0.011060487478971481,\n",
       " 0.000260280619841069,\n",
       " -0.00023179147683549672,\n",
       " -0.00159421656280756,\n",
       " 0.0015580819454044104,\n",
       " -0.025324102491140366,\n",
       " -0.03786807507276535,\n",
       " -0.05463133379817009,\n",
       " 0.004270824138075113,\n",
       " 0.0162220261991024,\n",
       " -0.04763112962245941,\n",
       " 0.11077607423067093,\n",
       " 0.04578297957777977,\n",
       " 0.07989459484815598,\n",
       " -0.006792624946683645,\n",
       " -0.010313671082258224,\n",
       " 0.006975388620048761,\n",
       " -0.09530744701623917,\n",
       " -0.01435692049562931,\n",
       " -0.013479163870215416,\n",
       " -0.009381196461617947,\n",
       " -0.0026152825448662043,\n",
       " -0.12162388116121292,\n",
       " 0.07765251398086548,\n",
       " 0.009094384498894215,\n",
       " -0.1018347442150116,\n",
       " 0.13146238029003143,\n",
       " -0.045870713889598846,\n",
       " -0.009605017490684986,\n",
       " 0.024302739650011063,\n",
       " 0.04592135548591614,\n",
       " 0.08771276473999023,\n",
       " 0.05515914037823677,\n",
       " 0.047116730362176895,\n",
       " -0.022800585255026817,\n",
       " 0.05540425330400467,\n",
       " 0.03942396491765976,\n",
       " -0.06854797154664993,\n",
       " 0.07696891576051712,\n",
       " 0.026480773463845253,\n",
       " 0.013421695679426193,\n",
       " -0.03159027174115181,\n",
       " 0.021223224699497223,\n",
       " -0.02458377368748188,\n",
       " -0.09490033239126205,\n",
       " 0.05001785606145859,\n",
       " -0.0788566991686821,\n",
       " -0.046926114708185196,\n",
       " -0.009405363351106644,\n",
       " 0.06844944506883621,\n",
       " -0.01953277923166752,\n",
       " 0.08325397223234177,\n",
       " -0.0020212598610669374,\n",
       " 0.07861408591270447,\n",
       " 0.009706995449960232,\n",
       " -0.08329331129789352,\n",
       " -0.08883731812238693,\n",
       " 0.026159735396504402,\n",
       " -0.0036121411249041557,\n",
       " 0.002121205907315016,\n",
       " 0.06756484508514404,\n",
       " -0.043519094586372375,\n",
       " -0.031103327870368958,\n",
       " -0.10554482787847519,\n",
       " 0.08162882924079895,\n",
       " -0.11693757772445679,\n",
       " 0.0012153844581916928,\n",
       " -0.04222622141242027,\n",
       " -0.025040671229362488,\n",
       " -0.053820766508579254,\n",
       " 0.04668883606791496,\n",
       " -0.004659494385123253,\n",
       " -0.049144286662340164,\n",
       " 0.05339542403817177,\n",
       " -0.01682465709745884,\n",
       " -0.018910998478531837,\n",
       " 0.0021526627242565155,\n",
       " 0.010545733384788036,\n",
       " -0.028433626517653465,\n",
       " 0.06319321691989899,\n",
       " -0.041760917752981186,\n",
       " 0.0364876464009285,\n",
       " -0.028613664209842682,\n",
       " 0.012441804632544518,\n",
       " -0.030993381515145302,\n",
       " -1.8279422420164337e-08,\n",
       " -0.033647507429122925,\n",
       " -0.010457259602844715,\n",
       " 0.0063261836767196655,\n",
       " -0.03394525498151779,\n",
       " -0.0343707911670208,\n",
       " 0.04372534900903702,\n",
       " 0.07607866823673248,\n",
       " -0.05076979845762253,\n",
       " -0.06551554054021835,\n",
       " -0.023710867390036583,\n",
       " 0.05217288061976433,\n",
       " 0.008229427970945835,\n",
       " -0.05053587630391121,\n",
       " -0.004634376615285873,\n",
       " 0.0459633506834507,\n",
       " -0.048263609409332275,\n",
       " -0.007646562997251749,\n",
       " -0.024670135229825974,\n",
       " -0.058992475271224976,\n",
       " 0.021795788779854774,\n",
       " -0.03319757431745529,\n",
       " 0.02626708894968033,\n",
       " 0.01956520788371563,\n",
       " 0.022036511451005936,\n",
       " -0.0270788986235857,\n",
       " 0.07815379649400711,\n",
       " 0.032591838389635086,\n",
       " 0.10126294940710068,\n",
       " 0.007166700437664986,\n",
       " -0.031028300523757935,\n",
       " 0.04080120101571083,\n",
       " 0.10805944353342056,\n",
       " -0.009413832798600197,\n",
       " -0.010281199589371681,\n",
       " 0.037279725074768066,\n",
       " 0.11904409527778625,\n",
       " 0.049820657819509506,\n",
       " 0.052095089107751846,\n",
       " 0.02024613320827484,\n",
       " 0.055519070476293564,\n",
       " -0.10270130634307861,\n",
       " -0.009933293797075748,\n",
       " -0.022510290145874023,\n",
       " 0.03311147540807724,\n",
       " 0.052272163331508636,\n",
       " -0.029383281245827675,\n",
       " -0.1383359283208847,\n",
       " -0.014143837615847588,\n",
       " -0.03765949234366417,\n",
       " -0.08339185267686844,\n",
       " -0.003486987203359604,\n",
       " -0.041542865335941315,\n",
       " 0.04902827739715576,\n",
       " 0.021551135927438736,\n",
       " -0.040210552513599396,\n",
       " 0.008557643741369247,\n",
       " 0.0466168075799942,\n",
       " -0.00411416869610548,\n",
       " -0.038159534335136414,\n",
       " -0.015223576687276363,\n",
       " 0.12486445158720016,\n",
       " 0.08800437301397324,\n",
       " 0.08585748076438904,\n",
       " -0.01533892098814249]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query_embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "70ebee77",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "\n",
    "loader = PyPDFLoader(\"Ng_MLY07 2.pdf\")\n",
    "docs = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'producer': 'Skia/PDF m68', 'creator': 'PyPDF', 'creationdate': '', 'source': 'Ng_MLY07 2.pdf', 'total_pages': 7, 'page': 0, 'page_label': '1'}, page_content=''),\n",
       " Document(metadata={'producer': 'Skia/PDF m68', 'creator': 'PyPDF', 'creationdate': '', 'source': 'Ng_MLY07 2.pdf', 'total_pages': 7, 'page': 1, 'page_label': '2'}, page_content='Machine Learning Yearning is a   \\n deeplearning.ai project.  \\n  \\n  \\n  \\n  \\n  \\n  \\n  \\n © 2018 Andrew Ng. All Rights Reserved.  \\n  \\n \\xa0 \\n \\xa0 \\n Page 2 Machine Learning Yearning-Draft Andrew Ng'),\n",
       " Document(metadata={'producer': 'Skia/PDF m68', 'creator': 'PyPDF', 'creationdate': '', 'source': 'Ng_MLY07 2.pdf', 'total_pages': 7, 'page': 2, 'page_label': '3'}, page_content='Comparing to  \\n human-level  \\n performance  \\n \\xa0 \\n \\xa0 \\n   \\n Page 3 Machine Learning Yearning-Draft Andrew Ng'),\n",
       " Document(metadata={'producer': 'Skia/PDF m68', 'creator': 'PyPDF', 'creationdate': '', 'source': 'Ng_MLY07 2.pdf', 'total_pages': 7, 'page': 3, 'page_label': '4'}, page_content='33 Why we compare to human-level  \\n performance   \\n \\xa0 \\n Many machine learning systems aim to automate things that humans do well. Examples  \\n include image recognition, speech recognition, and email spam classification. Learning  \\n algorithms have also improved so much that we are now surpassing human-level  \\n performance on more and more of these tasks.   \\n Further, there are several reasons building an ML system is easier if you are trying to do a  \\n task that people can do well:   \\n 1. Ease of obtaining data from human labelers. \\u200b  For example, since people recognize  \\n cat images well, it is straightforward for people to provide high accuracy labels for your  \\n learning algorithm.  \\n 2. Error analysis can draw on human intuition. \\u200b  Suppose a speech recognition  \\n algorithm is doing worse than human-level recognition. Say it incorrectly transcribes an  \\n audio clip as “This recipe calls for a \\u200b pear \\u200b  of apples,” mistaking “pair” for “pear.” You can  \\n draw on human intuition and try to understand what information a person uses to get the  \\n correct transcription, and use this knowledge to modify the learning algorithm.   \\n 3. Use human-level performance to estimate the optimal error rate and also set  \\n a “desired error rate.” \\u200b  Suppose your algorithm achieves 10% error on a task, but a person  \\n achieves 2% error. Then we know that the optimal error rate is 2% or lower and the  \\n avoidable bias is at least 8%. Thus, you should try bias-reducing techniques.   \\n Even though item #3 might not sound important, I find that having a reasonable and  \\n achievable target error rate helps accelerate a team’s progress. Knowing your algorithm has  \\n high avoidable bias is incredibly valuable and opens up a menu of options to try.   \\n There are some tasks that even humans aren’t good at. For example, picking a book to  \\n recommend to you; or picking an ad to show a user on a website; or predicting the stock  \\n market. Computers already surpass the performance of most people on these tasks. With  \\n these applications, we run into the following problems:   \\n • It is harder to obtain labels. \\u200b  For example, it’s hard for human labelers to annotate a  \\n database of users with the “optimal” book recommendation. If you operate a website or  \\n app that sells books, you can obtain data by showing books to users and seeing what they  \\n buy. If you do not operate such a site, you need to find more creative ways to get data.  \\n Page 4 Machine Learning Yearning-Draft Andrew Ng'),\n",
       " Document(metadata={'producer': 'Skia/PDF m68', 'creator': 'PyPDF', 'creationdate': '', 'source': 'Ng_MLY07 2.pdf', 'total_pages': 7, 'page': 4, 'page_label': '5'}, page_content='• Human intuition is harder to count on. \\u200b  For example, pretty much no one can  \\n predict the stock market. So if our stock prediction algorithm does no better than random  \\n guessing, it is hard to figure out how to improve it.   \\n • It is hard to know what the optimal error rate and reasonable desired error  \\n rate is. \\u200b Suppose you already have a book recommendation system that is doing quite  \\n well. How do you know how much more it can improve without a human baseline?   \\n  \\n  \\n  \\n  \\n  \\xa0 \\n Page 5 Machine Learning Yearning-Draft Andrew Ng'),\n",
       " Document(metadata={'producer': 'Skia/PDF m68', 'creator': 'PyPDF', 'creationdate': '', 'source': 'Ng_MLY07 2.pdf', 'total_pages': 7, 'page': 5, 'page_label': '6'}, page_content='34 How to define human-level performance   \\n \\xa0 \\n Suppose you are working on a medical imaging application that automatically makes  \\n diagnoses from x-ray images. A typical person with no previous medical background besides  \\n some basic training achieves 15% error on this task. A junior doctor achieves 10% error. An  \\n experienced doctor achieves 5% error. And a small team of doctors that discuss and debate  \\n each image achieves 2% error. Which one of these error rates defines “human-level  \\n performance”?   \\n In this case, I would use 2% as the human-level performance proxy for our optimal error  \\n rate. You can also set 2% as the desired performance level because all three reasons from the  \\n previous chapter for comparing to human-level performance apply:   \\n • Ease of obtaining labeled data from human labelers. \\u200b  You can get a team of doctors  \\n to provide labels to you with a 2% error rate.  \\n • Error analysis can draw on human intuition. \\u200b By discussing images with a team of  \\n doctors, you can draw on their intuitions.  \\n • Use human-level performance to estimate the optimal error rate and also set  \\n achievable “desired error rate.” \\u200b  It is reasonable to use 2% error as our estimate of the  \\n optimal error rate. The optimal error rate could be even lower than 2%, but it cannot be  \\n higher, since it is possible for a team of doctors to achieve 2% error. In contrast, it is not  \\n reasonable to use 5% or 10% as an estimate of the optimal error rate, since we know these  \\n estimates are necessarily too high.  \\n When it comes to obtaining labeled data, you might not want to discuss every image with an  \\n entire team of doctors since their time is expensive. Perhaps you can have a single junior  \\n doctor label the vast majority of cases and bring only the harder cases to more experienced  \\n doctors or to the team of doctors.   \\n If your system is currently at 40% error, then it doesn’t matter much whether you use a  \\n junior doctor (10% error) or an experienced doctor (5% error) to label your data and provide  \\n intuitions. But if your system is already at 10% error, then defining the human-level  \\n reference as 2% gives you better tools to keep improving your system.   \\n  \\xa0 \\n Page 6 Machine Learning Yearning-Draft Andrew Ng'),\n",
       " Document(metadata={'producer': 'Skia/PDF m68', 'creator': 'PyPDF', 'creationdate': '', 'source': 'Ng_MLY07 2.pdf', 'total_pages': 7, 'page': 6, 'page_label': '7'}, page_content='35 Surpassing human-level performance   \\n \\xa0 \\n You are working on speech recognition and have a dataset of audio clips. Suppose your  \\n dataset has many noisy audio clips so that even humans have 10% error. Suppose your  \\n system already achieves 8% error. Can you use any of the three techniques described in  \\n Chapter 33 to continue making rapid progress?  \\n If you can identify a subset of data in which humans significantly surpass your system, then  \\n you can still use those techniques to drive rapid progress. For example, suppose your system  \\n is much better than people at recognizing speech in noisy audio, but humans are still better  \\n at transcribing very rapidly spoken speech.  \\n For the subset of data with rapidly spoken speech:  \\n 1. You can still obtain transcripts from humans that are higher quality than your algorithm’s  \\n output.  \\n 2. You can draw on human intuition to understand why they correctly heard a rapidly  \\n spoken utterance when your system didn’t.  \\n 3. You can use human-level performance on rapidly spoken speech as a desired performance  \\n target.  \\n More generally, so long as there are dev set examples where humans are right and your  \\n algorithm is wrong, then many of the techniques described earlier will apply. This is true  \\n even if, averaged over the entire dev/test set, your performance is already surpassing  \\n human-level performance.   \\n There are many important machine learning applications where machines surpass human  \\n level performance. For example, machines are better at predicting movie ratings, how long it  \\n takes for a delivery car to drive somewhere, or whether to approve loan applications. Only a  \\n subset of techniques apply once humans have a hard time identifying examples that the  \\n algorithm is clearly getting wrong. Consequently, progress is usually slower on problems  \\n where machines already surpass human-level performance, while progress is faster when  \\n machines are still trying to catch up to humans.   \\n  \\n  \\n Page 7 Machine Learning Yearning-Draft Andrew Ng')]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "extracted_text = [\n",
    "    {\n",
    "        \"source\": doc.metadata[\"source\"],\n",
    "        \"page_content\": doc.page_content\n",
    "        \n",
    "    }\n",
    "    for doc in docs\n",
    "]\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'source': 'Ng_MLY07 2.pdf', 'page_content': ''},\n",
       " {'source': 'Ng_MLY07 2.pdf',\n",
       "  'page_content': 'Machine Learning Yearning is a   \\n deeplearning.ai project.  \\n  \\n  \\n  \\n  \\n  \\n  \\n  \\n © 2018 Andrew Ng. All Rights Reserved.  \\n  \\n \\xa0 \\n \\xa0 \\n Page 2 Machine Learning Yearning-Draft Andrew Ng'},\n",
       " {'source': 'Ng_MLY07 2.pdf',\n",
       "  'page_content': 'Comparing to  \\n human-level  \\n performance  \\n \\xa0 \\n \\xa0 \\n   \\n Page 3 Machine Learning Yearning-Draft Andrew Ng'},\n",
       " {'source': 'Ng_MLY07 2.pdf',\n",
       "  'page_content': '33 Why we compare to human-level  \\n performance   \\n \\xa0 \\n Many machine learning systems aim to automate things that humans do well. Examples  \\n include image recognition, speech recognition, and email spam classification. Learning  \\n algorithms have also improved so much that we are now surpassing human-level  \\n performance on more and more of these tasks.   \\n Further, there are several reasons building an ML system is easier if you are trying to do a  \\n task that people can do well:   \\n 1. Ease of obtaining data from human labelers. \\u200b  For example, since people recognize  \\n cat images well, it is straightforward for people to provide high accuracy labels for your  \\n learning algorithm.  \\n 2. Error analysis can draw on human intuition. \\u200b  Suppose a speech recognition  \\n algorithm is doing worse than human-level recognition. Say it incorrectly transcribes an  \\n audio clip as “This recipe calls for a \\u200b pear \\u200b  of apples,” mistaking “pair” for “pear.” You can  \\n draw on human intuition and try to understand what information a person uses to get the  \\n correct transcription, and use this knowledge to modify the learning algorithm.   \\n 3. Use human-level performance to estimate the optimal error rate and also set  \\n a “desired error rate.” \\u200b  Suppose your algorithm achieves 10% error on a task, but a person  \\n achieves 2% error. Then we know that the optimal error rate is 2% or lower and the  \\n avoidable bias is at least 8%. Thus, you should try bias-reducing techniques.   \\n Even though item #3 might not sound important, I find that having a reasonable and  \\n achievable target error rate helps accelerate a team’s progress. Knowing your algorithm has  \\n high avoidable bias is incredibly valuable and opens up a menu of options to try.   \\n There are some tasks that even humans aren’t good at. For example, picking a book to  \\n recommend to you; or picking an ad to show a user on a website; or predicting the stock  \\n market. Computers already surpass the performance of most people on these tasks. With  \\n these applications, we run into the following problems:   \\n • It is harder to obtain labels. \\u200b  For example, it’s hard for human labelers to annotate a  \\n database of users with the “optimal” book recommendation. If you operate a website or  \\n app that sells books, you can obtain data by showing books to users and seeing what they  \\n buy. If you do not operate such a site, you need to find more creative ways to get data.  \\n Page 4 Machine Learning Yearning-Draft Andrew Ng'},\n",
       " {'source': 'Ng_MLY07 2.pdf',\n",
       "  'page_content': '• Human intuition is harder to count on. \\u200b  For example, pretty much no one can  \\n predict the stock market. So if our stock prediction algorithm does no better than random  \\n guessing, it is hard to figure out how to improve it.   \\n • It is hard to know what the optimal error rate and reasonable desired error  \\n rate is. \\u200b Suppose you already have a book recommendation system that is doing quite  \\n well. How do you know how much more it can improve without a human baseline?   \\n  \\n  \\n  \\n  \\n  \\xa0 \\n Page 5 Machine Learning Yearning-Draft Andrew Ng'},\n",
       " {'source': 'Ng_MLY07 2.pdf',\n",
       "  'page_content': '34 How to define human-level performance   \\n \\xa0 \\n Suppose you are working on a medical imaging application that automatically makes  \\n diagnoses from x-ray images. A typical person with no previous medical background besides  \\n some basic training achieves 15% error on this task. A junior doctor achieves 10% error. An  \\n experienced doctor achieves 5% error. And a small team of doctors that discuss and debate  \\n each image achieves 2% error. Which one of these error rates defines “human-level  \\n performance”?   \\n In this case, I would use 2% as the human-level performance proxy for our optimal error  \\n rate. You can also set 2% as the desired performance level because all three reasons from the  \\n previous chapter for comparing to human-level performance apply:   \\n • Ease of obtaining labeled data from human labelers. \\u200b  You can get a team of doctors  \\n to provide labels to you with a 2% error rate.  \\n • Error analysis can draw on human intuition. \\u200b By discussing images with a team of  \\n doctors, you can draw on their intuitions.  \\n • Use human-level performance to estimate the optimal error rate and also set  \\n achievable “desired error rate.” \\u200b  It is reasonable to use 2% error as our estimate of the  \\n optimal error rate. The optimal error rate could be even lower than 2%, but it cannot be  \\n higher, since it is possible for a team of doctors to achieve 2% error. In contrast, it is not  \\n reasonable to use 5% or 10% as an estimate of the optimal error rate, since we know these  \\n estimates are necessarily too high.  \\n When it comes to obtaining labeled data, you might not want to discuss every image with an  \\n entire team of doctors since their time is expensive. Perhaps you can have a single junior  \\n doctor label the vast majority of cases and bring only the harder cases to more experienced  \\n doctors or to the team of doctors.   \\n If your system is currently at 40% error, then it doesn’t matter much whether you use a  \\n junior doctor (10% error) or an experienced doctor (5% error) to label your data and provide  \\n intuitions. But if your system is already at 10% error, then defining the human-level  \\n reference as 2% gives you better tools to keep improving your system.   \\n  \\xa0 \\n Page 6 Machine Learning Yearning-Draft Andrew Ng'},\n",
       " {'source': 'Ng_MLY07 2.pdf',\n",
       "  'page_content': '35 Surpassing human-level performance   \\n \\xa0 \\n You are working on speech recognition and have a dataset of audio clips. Suppose your  \\n dataset has many noisy audio clips so that even humans have 10% error. Suppose your  \\n system already achieves 8% error. Can you use any of the three techniques described in  \\n Chapter 33 to continue making rapid progress?  \\n If you can identify a subset of data in which humans significantly surpass your system, then  \\n you can still use those techniques to drive rapid progress. For example, suppose your system  \\n is much better than people at recognizing speech in noisy audio, but humans are still better  \\n at transcribing very rapidly spoken speech.  \\n For the subset of data with rapidly spoken speech:  \\n 1. You can still obtain transcripts from humans that are higher quality than your algorithm’s  \\n output.  \\n 2. You can draw on human intuition to understand why they correctly heard a rapidly  \\n spoken utterance when your system didn’t.  \\n 3. You can use human-level performance on rapidly spoken speech as a desired performance  \\n target.  \\n More generally, so long as there are dev set examples where humans are right and your  \\n algorithm is wrong, then many of the techniques described earlier will apply. This is true  \\n even if, averaged over the entire dev/test set, your performance is already surpassing  \\n human-level performance.   \\n There are many important machine learning applications where machines surpass human  \\n level performance. For example, machines are better at predicting movie ratings, how long it  \\n takes for a delivery car to drive somewhere, or whether to approve loan applications. Only a  \\n subset of techniques apply once humans have a hard time identifying examples that the  \\n algorithm is clearly getting wrong. Consequently, progress is usually slower on problems  \\n where machines already surpass human-level performance, while progress is faster when  \\n machines are still trying to catch up to humans.   \\n  \\n  \\n Page 7 Machine Learning Yearning-Draft Andrew Ng'}]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extracted_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "681b9d19",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langchain_text_splitters.character.RecursiveCharacterTextSplitter at 0x73d88038d760>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter \n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=100,\n",
    "    chunk_overlap=50\n",
    ")\n",
    "\n",
    "text_splitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "24891e12",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.documents import Document\n",
    "\n",
    "all_chunks = []\n",
    "for doc in docs:\n",
    "    chunks = text_splitter.split_text(doc.page_content)\n",
    "    #print(f\"Document has been split into {len(chunks)} chunks.\")\n",
    "    for i,chunk in enumerate(chunks):\n",
    "        chunk_doc = Document(\n",
    "            page_content=chunk,\n",
    "            metadata={\"source\": doc.metadata[\"source\"], \"chunk\": i}\n",
    "        )\n",
    "        all_chunks.append(chunk_doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "9f214120",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'Ng_MLY07 2.pdf', 'chunk': 0}, page_content='Machine Learning Yearning is a   \\n deeplearning.ai project.'),\n",
       " Document(metadata={'source': 'Ng_MLY07 2.pdf', 'chunk': 1}, page_content='deeplearning.ai project.  \\n  \\n  \\n  \\n  \\n  \\n  \\n  \\n © 2018 Andrew Ng. All Rights Reserved.'),\n",
       " Document(metadata={'source': 'Ng_MLY07 2.pdf', 'chunk': 2}, page_content='© 2018 Andrew Ng. All Rights Reserved.'),\n",
       " Document(metadata={'source': 'Ng_MLY07 2.pdf', 'chunk': 3}, page_content='Page 2 Machine Learning Yearning-Draft Andrew Ng'),\n",
       " Document(metadata={'source': 'Ng_MLY07 2.pdf', 'chunk': 0}, page_content='Comparing to  \\n human-level  \\n performance'),\n",
       " Document(metadata={'source': 'Ng_MLY07 2.pdf', 'chunk': 1}, page_content='human-level  \\n performance  \\n \\xa0 \\n \\xa0 \\n   \\n Page 3 Machine Learning Yearning-Draft Andrew Ng'),\n",
       " Document(metadata={'source': 'Ng_MLY07 2.pdf', 'chunk': 0}, page_content='33 Why we compare to human-level  \\n performance'),\n",
       " Document(metadata={'source': 'Ng_MLY07 2.pdf', 'chunk': 1}, page_content='Many machine learning systems aim to automate things that humans do well. Examples'),\n",
       " Document(metadata={'source': 'Ng_MLY07 2.pdf', 'chunk': 2}, page_content='include image recognition, speech recognition, and email spam classification. Learning'),\n",
       " Document(metadata={'source': 'Ng_MLY07 2.pdf', 'chunk': 3}, page_content='algorithms have also improved so much that we are now surpassing human-level'),\n",
       " Document(metadata={'source': 'Ng_MLY07 2.pdf', 'chunk': 4}, page_content='performance on more and more of these tasks.'),\n",
       " Document(metadata={'source': 'Ng_MLY07 2.pdf', 'chunk': 5}, page_content='Further, there are several reasons building an ML system is easier if you are trying to do a'),\n",
       " Document(metadata={'source': 'Ng_MLY07 2.pdf', 'chunk': 6}, page_content='task that people can do well:'),\n",
       " Document(metadata={'source': 'Ng_MLY07 2.pdf', 'chunk': 7}, page_content='1. Ease of obtaining data from human labelers. \\u200b  For example, since people recognize'),\n",
       " Document(metadata={'source': 'Ng_MLY07 2.pdf', 'chunk': 8}, page_content='cat images well, it is straightforward for people to provide high accuracy labels for your'),\n",
       " Document(metadata={'source': 'Ng_MLY07 2.pdf', 'chunk': 9}, page_content='learning algorithm.'),\n",
       " Document(metadata={'source': 'Ng_MLY07 2.pdf', 'chunk': 10}, page_content='2. Error analysis can draw on human intuition. \\u200b  Suppose a speech recognition'),\n",
       " Document(metadata={'source': 'Ng_MLY07 2.pdf', 'chunk': 11}, page_content='algorithm is doing worse than human-level recognition. Say it incorrectly transcribes an'),\n",
       " Document(metadata={'source': 'Ng_MLY07 2.pdf', 'chunk': 12}, page_content='audio clip as “This recipe calls for a \\u200b pear \\u200b  of apples,” mistaking “pair” for “pear.” You can'),\n",
       " Document(metadata={'source': 'Ng_MLY07 2.pdf', 'chunk': 13}, page_content='of apples,” mistaking “pair” for “pear.” You can'),\n",
       " Document(metadata={'source': 'Ng_MLY07 2.pdf', 'chunk': 14}, page_content='draw on human intuition and try to understand what information a person uses to get the'),\n",
       " Document(metadata={'source': 'Ng_MLY07 2.pdf', 'chunk': 15}, page_content='correct transcription, and use this knowledge to modify the learning algorithm.'),\n",
       " Document(metadata={'source': 'Ng_MLY07 2.pdf', 'chunk': 16}, page_content='3. Use human-level performance to estimate the optimal error rate and also set'),\n",
       " Document(metadata={'source': 'Ng_MLY07 2.pdf', 'chunk': 17}, page_content='a “desired error rate.” \\u200b  Suppose your algorithm achieves 10% error on a task, but a person'),\n",
       " Document(metadata={'source': 'Ng_MLY07 2.pdf', 'chunk': 18}, page_content='achieves 2% error. Then we know that the optimal error rate is 2% or lower and the'),\n",
       " Document(metadata={'source': 'Ng_MLY07 2.pdf', 'chunk': 19}, page_content='avoidable bias is at least 8%. Thus, you should try bias-reducing techniques.'),\n",
       " Document(metadata={'source': 'Ng_MLY07 2.pdf', 'chunk': 20}, page_content='Even though item #3 might not sound important, I find that having a reasonable and'),\n",
       " Document(metadata={'source': 'Ng_MLY07 2.pdf', 'chunk': 21}, page_content='achievable target error rate helps accelerate a team’s progress. Knowing your algorithm has'),\n",
       " Document(metadata={'source': 'Ng_MLY07 2.pdf', 'chunk': 22}, page_content='high avoidable bias is incredibly valuable and opens up a menu of options to try.'),\n",
       " Document(metadata={'source': 'Ng_MLY07 2.pdf', 'chunk': 23}, page_content='There are some tasks that even humans aren’t good at. For example, picking a book to'),\n",
       " Document(metadata={'source': 'Ng_MLY07 2.pdf', 'chunk': 24}, page_content='recommend to you; or picking an ad to show a user on a website; or predicting the stock'),\n",
       " Document(metadata={'source': 'Ng_MLY07 2.pdf', 'chunk': 25}, page_content='market. Computers already surpass the performance of most people on these tasks. With'),\n",
       " Document(metadata={'source': 'Ng_MLY07 2.pdf', 'chunk': 26}, page_content='these applications, we run into the following problems:'),\n",
       " Document(metadata={'source': 'Ng_MLY07 2.pdf', 'chunk': 27}, page_content='• It is harder to obtain labels. \\u200b  For example, it’s hard for human labelers to annotate a'),\n",
       " Document(metadata={'source': 'Ng_MLY07 2.pdf', 'chunk': 28}, page_content='database of users with the “optimal” book recommendation. If you operate a website or'),\n",
       " Document(metadata={'source': 'Ng_MLY07 2.pdf', 'chunk': 29}, page_content='app that sells books, you can obtain data by showing books to users and seeing what they'),\n",
       " Document(metadata={'source': 'Ng_MLY07 2.pdf', 'chunk': 30}, page_content='buy. If you do not operate such a site, you need to find more creative ways to get data.'),\n",
       " Document(metadata={'source': 'Ng_MLY07 2.pdf', 'chunk': 31}, page_content='Page 4 Machine Learning Yearning-Draft Andrew Ng'),\n",
       " Document(metadata={'source': 'Ng_MLY07 2.pdf', 'chunk': 0}, page_content='• Human intuition is harder to count on. \\u200b  For example, pretty much no one can'),\n",
       " Document(metadata={'source': 'Ng_MLY07 2.pdf', 'chunk': 1}, page_content='predict the stock market. So if our stock prediction algorithm does no better than random'),\n",
       " Document(metadata={'source': 'Ng_MLY07 2.pdf', 'chunk': 2}, page_content='guessing, it is hard to figure out how to improve it.'),\n",
       " Document(metadata={'source': 'Ng_MLY07 2.pdf', 'chunk': 3}, page_content='• It is hard to know what the optimal error rate and reasonable desired error'),\n",
       " Document(metadata={'source': 'Ng_MLY07 2.pdf', 'chunk': 4}, page_content='rate is. \\u200b Suppose you already have a book recommendation system that is doing quite'),\n",
       " Document(metadata={'source': 'Ng_MLY07 2.pdf', 'chunk': 5}, page_content='well. How do you know how much more it can improve without a human baseline?'),\n",
       " Document(metadata={'source': 'Ng_MLY07 2.pdf', 'chunk': 6}, page_content='Page 5 Machine Learning Yearning-Draft Andrew Ng'),\n",
       " Document(metadata={'source': 'Ng_MLY07 2.pdf', 'chunk': 0}, page_content='34 How to define human-level performance'),\n",
       " Document(metadata={'source': 'Ng_MLY07 2.pdf', 'chunk': 1}, page_content='Suppose you are working on a medical imaging application that automatically makes'),\n",
       " Document(metadata={'source': 'Ng_MLY07 2.pdf', 'chunk': 2}, page_content='diagnoses from x-ray images. A typical person with no previous medical background besides'),\n",
       " Document(metadata={'source': 'Ng_MLY07 2.pdf', 'chunk': 3}, page_content='some basic training achieves 15% error on this task. A junior doctor achieves 10% error. An'),\n",
       " Document(metadata={'source': 'Ng_MLY07 2.pdf', 'chunk': 4}, page_content='experienced doctor achieves 5% error. And a small team of doctors that discuss and debate'),\n",
       " Document(metadata={'source': 'Ng_MLY07 2.pdf', 'chunk': 5}, page_content='each image achieves 2% error. Which one of these error rates defines “human-level'),\n",
       " Document(metadata={'source': 'Ng_MLY07 2.pdf', 'chunk': 6}, page_content='performance”?'),\n",
       " Document(metadata={'source': 'Ng_MLY07 2.pdf', 'chunk': 7}, page_content='In this case, I would use 2% as the human-level performance proxy for our optimal error'),\n",
       " Document(metadata={'source': 'Ng_MLY07 2.pdf', 'chunk': 8}, page_content='rate. You can also set 2% as the desired performance level because all three reasons from the'),\n",
       " Document(metadata={'source': 'Ng_MLY07 2.pdf', 'chunk': 9}, page_content='previous chapter for comparing to human-level performance apply:'),\n",
       " Document(metadata={'source': 'Ng_MLY07 2.pdf', 'chunk': 10}, page_content='• Ease of obtaining labeled data from human labelers. \\u200b  You can get a team of doctors'),\n",
       " Document(metadata={'source': 'Ng_MLY07 2.pdf', 'chunk': 11}, page_content='to provide labels to you with a 2% error rate.'),\n",
       " Document(metadata={'source': 'Ng_MLY07 2.pdf', 'chunk': 12}, page_content='• Error analysis can draw on human intuition. \\u200b By discussing images with a team of'),\n",
       " Document(metadata={'source': 'Ng_MLY07 2.pdf', 'chunk': 13}, page_content='doctors, you can draw on their intuitions.'),\n",
       " Document(metadata={'source': 'Ng_MLY07 2.pdf', 'chunk': 14}, page_content='• Use human-level performance to estimate the optimal error rate and also set'),\n",
       " Document(metadata={'source': 'Ng_MLY07 2.pdf', 'chunk': 15}, page_content='achievable “desired error rate.” \\u200b  It is reasonable to use 2% error as our estimate of the'),\n",
       " Document(metadata={'source': 'Ng_MLY07 2.pdf', 'chunk': 16}, page_content='optimal error rate. The optimal error rate could be even lower than 2%, but it cannot be'),\n",
       " Document(metadata={'source': 'Ng_MLY07 2.pdf', 'chunk': 17}, page_content='higher, since it is possible for a team of doctors to achieve 2% error. In contrast, it is not'),\n",
       " Document(metadata={'source': 'Ng_MLY07 2.pdf', 'chunk': 18}, page_content='reasonable to use 5% or 10% as an estimate of the optimal error rate, since we know these'),\n",
       " Document(metadata={'source': 'Ng_MLY07 2.pdf', 'chunk': 19}, page_content='estimates are necessarily too high.'),\n",
       " Document(metadata={'source': 'Ng_MLY07 2.pdf', 'chunk': 20}, page_content='When it comes to obtaining labeled data, you might not want to discuss every image with an'),\n",
       " Document(metadata={'source': 'Ng_MLY07 2.pdf', 'chunk': 21}, page_content='entire team of doctors since their time is expensive. Perhaps you can have a single junior'),\n",
       " Document(metadata={'source': 'Ng_MLY07 2.pdf', 'chunk': 22}, page_content='doctor label the vast majority of cases and bring only the harder cases to more experienced'),\n",
       " Document(metadata={'source': 'Ng_MLY07 2.pdf', 'chunk': 23}, page_content='doctors or to the team of doctors.'),\n",
       " Document(metadata={'source': 'Ng_MLY07 2.pdf', 'chunk': 24}, page_content='If your system is currently at 40% error, then it doesn’t matter much whether you use a'),\n",
       " Document(metadata={'source': 'Ng_MLY07 2.pdf', 'chunk': 25}, page_content='junior doctor (10% error) or an experienced doctor (5% error) to label your data and provide'),\n",
       " Document(metadata={'source': 'Ng_MLY07 2.pdf', 'chunk': 26}, page_content='intuitions. But if your system is already at 10% error, then defining the human-level'),\n",
       " Document(metadata={'source': 'Ng_MLY07 2.pdf', 'chunk': 27}, page_content='reference as 2% gives you better tools to keep improving your system.'),\n",
       " Document(metadata={'source': 'Ng_MLY07 2.pdf', 'chunk': 28}, page_content='Page 6 Machine Learning Yearning-Draft Andrew Ng'),\n",
       " Document(metadata={'source': 'Ng_MLY07 2.pdf', 'chunk': 0}, page_content='35 Surpassing human-level performance'),\n",
       " Document(metadata={'source': 'Ng_MLY07 2.pdf', 'chunk': 1}, page_content='You are working on speech recognition and have a dataset of audio clips. Suppose your'),\n",
       " Document(metadata={'source': 'Ng_MLY07 2.pdf', 'chunk': 2}, page_content='dataset has many noisy audio clips so that even humans have 10% error. Suppose your'),\n",
       " Document(metadata={'source': 'Ng_MLY07 2.pdf', 'chunk': 3}, page_content='system already achieves 8% error. Can you use any of the three techniques described in'),\n",
       " Document(metadata={'source': 'Ng_MLY07 2.pdf', 'chunk': 4}, page_content='Chapter 33 to continue making rapid progress?'),\n",
       " Document(metadata={'source': 'Ng_MLY07 2.pdf', 'chunk': 5}, page_content='If you can identify a subset of data in which humans significantly surpass your system, then'),\n",
       " Document(metadata={'source': 'Ng_MLY07 2.pdf', 'chunk': 6}, page_content='you can still use those techniques to drive rapid progress. For example, suppose your system'),\n",
       " Document(metadata={'source': 'Ng_MLY07 2.pdf', 'chunk': 7}, page_content='is much better than people at recognizing speech in noisy audio, but humans are still better'),\n",
       " Document(metadata={'source': 'Ng_MLY07 2.pdf', 'chunk': 8}, page_content='at transcribing very rapidly spoken speech.'),\n",
       " Document(metadata={'source': 'Ng_MLY07 2.pdf', 'chunk': 9}, page_content='For the subset of data with rapidly spoken speech:'),\n",
       " Document(metadata={'source': 'Ng_MLY07 2.pdf', 'chunk': 10}, page_content='1. You can still obtain transcripts from humans that are higher quality than your algorithm’s'),\n",
       " Document(metadata={'source': 'Ng_MLY07 2.pdf', 'chunk': 11}, page_content='output.  \\n 2. You can draw on human intuition to understand why they correctly heard a rapidly'),\n",
       " Document(metadata={'source': 'Ng_MLY07 2.pdf', 'chunk': 12}, page_content='spoken utterance when your system didn’t.'),\n",
       " Document(metadata={'source': 'Ng_MLY07 2.pdf', 'chunk': 13}, page_content='3. You can use human-level performance on rapidly spoken speech as a desired performance'),\n",
       " Document(metadata={'source': 'Ng_MLY07 2.pdf', 'chunk': 14}, page_content='target.  \\n More generally, so long as there are dev set examples where humans are right and your'),\n",
       " Document(metadata={'source': 'Ng_MLY07 2.pdf', 'chunk': 15}, page_content='algorithm is wrong, then many of the techniques described earlier will apply. This is true'),\n",
       " Document(metadata={'source': 'Ng_MLY07 2.pdf', 'chunk': 16}, page_content='even if, averaged over the entire dev/test set, your performance is already surpassing'),\n",
       " Document(metadata={'source': 'Ng_MLY07 2.pdf', 'chunk': 17}, page_content='human-level performance.'),\n",
       " Document(metadata={'source': 'Ng_MLY07 2.pdf', 'chunk': 18}, page_content='There are many important machine learning applications where machines surpass human'),\n",
       " Document(metadata={'source': 'Ng_MLY07 2.pdf', 'chunk': 19}, page_content='level performance. For example, machines are better at predicting movie ratings, how long it'),\n",
       " Document(metadata={'source': 'Ng_MLY07 2.pdf', 'chunk': 20}, page_content='takes for a delivery car to drive somewhere, or whether to approve loan applications. Only a'),\n",
       " Document(metadata={'source': 'Ng_MLY07 2.pdf', 'chunk': 21}, page_content='subset of techniques apply once humans have a hard time identifying examples that the'),\n",
       " Document(metadata={'source': 'Ng_MLY07 2.pdf', 'chunk': 22}, page_content='algorithm is clearly getting wrong. Consequently, progress is usually slower on problems'),\n",
       " Document(metadata={'source': 'Ng_MLY07 2.pdf', 'chunk': 23}, page_content='where machines already surpass human-level performance, while progress is faster when'),\n",
       " Document(metadata={'source': 'Ng_MLY07 2.pdf', 'chunk': 24}, page_content='machines are still trying to catch up to humans.'),\n",
       " Document(metadata={'source': 'Ng_MLY07 2.pdf', 'chunk': 25}, page_content='Page 7 Machine Learning Yearning-Draft Andrew Ng')]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created 100 chunks\n"
     ]
    }
   ],
   "source": [
    "print(f\"Created {len(all_chunks)} chunks\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "5a2e48a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "\n",
    "# The model name is loaded from the sentence-transformers library\n",
    "model_name = \"sentence-transformers/all-MiniLM-l6-v2\"\n",
    "\n",
    "embeddings = HuggingFaceEmbeddings(\n",
    "    model_name=model_name,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.vectorstores import FAISS\n",
    "\n",
    "victor_store = FAISS.from_documents(all_chunks, embeddings)\n",
    "\n",
    "victor_store.save_local(\"faiss_medical\")\n",
    "# ret = FAISS.load_local(\"faiss_medical\", embeddings)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "32d3f2ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriver = victor_store.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 2})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "ab53edc4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'34 How to define human-level performance'"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = retriver.invoke('whats human-level performance')\n",
    "response[1].page_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "e6e9f8ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------\n",
      "Score: 0.117998704\n",
      "Metadata: {'source': 'Ng_MLY07 2.pdf', 'chunk': 17}\n",
      "human-level performance.\n",
      "--------\n",
      "Score: 0.25098738\n",
      "Metadata: {'source': 'Ng_MLY07 2.pdf', 'chunk': 0}\n",
      "Comparing to  \n",
      " human-level  \n",
      " performance\n",
      "--------\n",
      "Score: 0.25807592\n",
      "Metadata: {'source': 'Ng_MLY07 2.pdf', 'chunk': 0}\n",
      "34 How to define human-level performance\n",
      "--------\n",
      "Score: 0.2812174\n",
      "Metadata: {'source': 'Ng_MLY07 2.pdf', 'chunk': 9}\n",
      "previous chapter for comparing to human-level performance apply:\n"
     ]
    }
   ],
   "source": [
    "query = \"whats human-level performance?\"\n",
    "\n",
    "docs_and_scores = retriver.vectorstore.similarity_search_with_score(query, k=4)\n",
    "\n",
    "for doc, score in docs_and_scores:\n",
    "    print(\"--------\")\n",
    "    print(\"Score:\", score)\n",
    "    print(\"Metadata:\", doc.metadata)\n",
    "    print(doc.page_content[:400])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_variables=['context', 'input'] input_types={} partial_variables={} template=\"You are an assistant for question-answering tasks.\\nUse the following pieces of retrieved context to answer the question.\\nIf you don't know the answer, say that you don't know.\\nUse three sentences maximum and keep the answer concise.\\n\\nQuestion: {input}\\nContext: {context}\\nAnswer:\\n\"\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "prompt_template = \"\"\"You are an assistant for question-answering tasks.\n",
    "Use the following pieces of retrieved context to answer the question.\n",
    "If you don't know the answer, say that you don't know.\n",
    "Use three sentences maximum and keep the answer concise.\n",
    "\n",
    "Question: {question}\n",
    "Context: {context}\n",
    "Answer:\n",
    "\"\"\"\n",
    "\n",
    "prompt = PromptTemplate.from_template(prompt_template)\n",
    "\n",
    "print(prompt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'• It is hard to know what the optimal error rate and reasonable desired error\\n\\n• Error analysis can draw on human intuition. \\u200b By discussing images with a team of'"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_context(question):\n",
    "    docs = retriver.invoke(question)\n",
    "\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "context = get_context(\"whats Error analysis?\")\n",
    "context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "0c774061",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Human-level performance refers to a level of performance that matches or exceeds that of a typical human in a particular task or domain. It is often used as a benchmark for artificial intelligence and machine learning systems, indicating that they have achieved a level of competence similar to that of a human. Achieving human-level performance is considered a significant milestone in AI development.'"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.runnables import RunnableParallel,RunnablePassthrough\n",
    "\n",
    "runable = (RunnableParallel(\n",
    "    {'context':get_context,\n",
    "    'question':RunnablePassthrough()\n",
    "    }\n",
    "))\n",
    "\n",
    "chain = runable | prompt | llm\n",
    "\n",
    "response = chain.invoke('whats human-level performance?')\n",
    "response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RunnableBinding(bound=RunnableAssign(mapper={\n",
       "  context: RunnableBinding(bound=RunnableLambda(lambda x: x['input'])\n",
       "           | VectorStoreRetriever(tags=['FAISS', 'HuggingFaceEmbeddings'], vectorstore=<langchain_community.vectorstores.faiss.FAISS object at 0x73d735ffb260>, search_kwargs={'k': 2}), kwargs={}, config={'run_name': 'retrieve_documents'}, config_factories=[])\n",
       "})\n",
       "| RunnableAssign(mapper={\n",
       "    answer: RunnableBinding(bound=RunnableBinding(bound=RunnableAssign(mapper={\n",
       "              context: RunnableLambda(format_docs)\n",
       "            }), kwargs={}, config={'run_name': 'format_inputs'}, config_factories=[])\n",
       "            | PromptTemplate(input_variables=['context', 'input'], input_types={}, partial_variables={}, template=\"You are an assistant for question-answering tasks.\\nUse the following pieces of retrieved context to answer the question.\\nIf you don't know the answer, say that you don't know.\\nUse three sentences maximum and keep the answer concise.\\n\\nQuestion: {input}\\nContext: {context}\\nAnswer:\\n\")\n",
       "            | ChatGroq(client=<groq.resources.chat.completions.Completions object at 0x73d6ff0ce7e0>, async_client=<groq.resources.chat.completions.AsyncCompletions object at 0x73d6fef92d80>, model_name='llama-3.1-8b-instant', model_kwargs={}, groq_api_key=SecretStr('**********'), max_tokens=500)\n",
       "            | StrOutputParser(), kwargs={}, config={'run_name': 'stuff_documents_chain'}, config_factories=[])\n",
       "  }), kwargs={}, config={'run_name': 'retrieval_chain'}, config_factories=[])"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_classic.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain_classic.chains.retrieval import create_retrieval_chain\n",
    "\n",
    "combine_docs_chain = create_stuff_documents_chain(llm, prompt)\n",
    "\n",
    "response = create_retrieval_chain(retriver, combine_docs_chain)\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input': 'What is Machine Learning Yearning?',\n",
       " 'context': [Document(id='c2aa4ddc-8298-44bb-9184-2e17f675396e', metadata={'source': 'Ng_MLY07 2.pdf', 'chunk': 0}, page_content='Machine Learning Yearning is a   \\n deeplearning.ai project.'),\n",
       "  Document(id='bb8ac637-ddcd-450a-a964-53f91be4adeb', metadata={'source': 'Ng_MLY07 2.pdf', 'chunk': 1}, page_content='Many machine learning systems aim to automate things that humans do well. Examples')],\n",
       " 'answer': 'Machine Learning Yearning is a project by deeplearning.ai that helps learners understand how to approach machine learning problems. It aims to provide a framework for tackling complex machine learning challenges. The project focuses on the process of machine learning, not just the techniques.'}"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.invoke({\n",
    "    \"input\": \"What is Machine Learning Yearning?\",\n",
    "    \"context\": get_context(\"What is Machine Learning Yearning?\")\n",
    "})\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "chatapp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
